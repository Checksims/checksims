\section{Results}
\label{sec:results}
The experiments described in Section~\ref{sec:methods} were performed on the
anonymized student data described in Section~\ref{sec:studentcode}. This
section summarizes their results, and attempts to draw conclusions about the
state of \textit{Checksims} and its algorithms from them.

\subsection{Algorithm Comparison}
Our algorithm comparison tests, described in Section~\ref{sec:algcompare},
were split into three overall tests. The first compared the overall number of
results detected by each algorithm across all our test data. The second
compared the running times of both algorithms across all test data. The third
and final test compared the results returned by the two algorithms for a subset
of all the assignments, to determine what matches were identified by one
algorithm but not another.


\subsubsection{Overall Results Detected}
Figure~\ref{fig:algocompare} shows all results from applying both algorithms
to every assignment in the test data. Results under 70\% are not shown, as they
are less interesting (a far lower proportion will be cases of unauthorized
copying) and they are orders of magnitude more than the significant results. It
is clear that \textit{Smith-Waterman} is a far more sensitive algorithm than
\textit{Line Comparison}, detecting a great many more instances in every range
save 100\% similarity (which both algorithms should be able to detect easily).
This sensitivity is not necessarily an indicator of accuracy, however; it is
possible that all of the results reported by \textit{Smith-Waterman} are simply
false positives.

Because of the obvious limitations of the \textit{Line Comparison} algorithm 
(detailed in Section~\ref{sec:linecompare}), we do not believe this to be the
case. A large number of the additional results found by
\textit{Smith-Waterman} are almost certainly real instances of unauthorized
copying (or, at the least, cases that should have been brought to the
attention of course staff for manual review). This matches our hypothesis that
\textit{Smith-Waterman} is the more accurate of the two algorithms (though,
again, we cannot prove this with only these results).

All known occurrences of common code were removed from our test data. In
several cases, we were unable to obtain the common code from the instructors
who gave the assignment. In each of these cases, we identified one student who
had 100\% similarity to all others. On further examination, such students
typically had submitted no code save the common starter code. Consequently, we
were able to use their submissions as common code for these assignments.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{algorithmcompare.pdf}
\caption{Results detected by \textit{Line Comparison} and \textit{Smith Waterman} across all assignments}
\label{fig:algocompare}
\end{figure}


\paragraph{Previous Undiscovered Academic Dishonesty}
As Figure~\ref{fig:algocompare} shows, the \textit{Smith-Waterman} algorithm
identified 17 cases of 100\% similarity, and a further 35 over 90\%. As was
mentioned in Section~\ref{sec:anon}, all our test data is taken from the actual
courses --- previous offerings of sophomore-level Computer Science classes.
Given this, we can assume that the results in Figure~\ref{fig:algocompare}
represent a large number of cases of academic dishonesty, though we cannot say
how many of the reported results are actually cases of academic dishonesty. We
can, however, draw reasonable inferences.

The 100\% results are almost certainly cases of academic dishonesty. The notion
that two students could have typed in 100\% identical code independently is
completely implausible. We investigated several of these pairs, and all we
found were what we
consider to be academic dishonesty. Results from the 90\% to 99\% range are
also nearly-certain cases of academic dishonesty. We reviewed 6 of these pairs
of assignments, and found that all of them match our definition of academic
dishonesty.

The 70\% to 89\% range, however, contains a number of similarities that may
not be caused by academic dishonesty. In some cases, students implemented the
same algorithm in very similar ways. Given that there are not many ways to
write a simple algorithm (for example, Bubblesort), especially if typical loop
counter conventions (\texttt{i} for outermost loop, \texttt{j} for next
innermost, etc) are followed. Six of the nine courses we obtained code from
were offerings of CS2301, an intro-level course targeted at non-Computer
Science majors. Most assignments for CS2301 are very simple, requiring only
trivial algorithms (without much variation in algorithm choice or
implementation). Furthermore, Professor Lauer permits the copying of algorithms
out of the textbook, so students may end up with completely identical versions
of simple functions (like binary tree insertion). Given this, many similarities
in the 70\% to 89\% range may have produced their code independently, but using
very similar algorithms and pseudocode --- acceptable behavior, according to
Professor Lauer.

The potential difference between the two assignments that form our test data
is emphasized by the graph in Figure~\ref{fig:classcompare}. Almost all
similarities identified are from CS2301, despite it only containing
approximately 70\% of our test code. Given this, the smaller similarities seen
may well be due to the reasons identified in the previous paragraph.

Even discounting a large number of the similarities present as potentially not
cases of academic dishonesty, our results do paint a concerning picture of
undetected past incidences of academic dishonesty within CS2301.
Professor Lauer has confermed that to his knowledge, almost none of the near
certain cases we identified (90\% and higher) were caught. We hope that the
deployment of \textit{Checksims} may be able to reduce these numbers as word of
its use spreads.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{classcompare.pdf}
\caption{Results in Figure~\ref{fig:algocompare} split by course}
\label{fig:classcompare}
\end{figure}


\subsubsection{Runtime Comparison}
\label{sec:runtime}
Figure~\ref{fig:smithwatermanruntime} shows the runtime of the
\textit{Smith-Waterman} algorithm for every assignment in our dataset. No
assignment took over 2400 seconds (40 minutes) to complete, and the vast
majority finished in under 360 seconds (6 minutes). We originally intended to
produce a comparative graph also showing results from \textit{Line Comparison},
but the results from that algorithm were sufficiently similar that we did not
deem it necessary to graph them. No assignment took longer than 1 second to
complete using \textit{Line Comparison}; the graph, compared to the results
from \textit{Smith-Waterman}, would be a flat line slightly above 0. This
supports our hypothesis that \textit{Line Comparison} is the faster of the two
algorithms, and confirms its usefulness as a fast initial pass to identify
highly similar submissions.

It is noteworthy that, while no assignment shown completed in over 2400 seconds
(40 minutes), we were forced to manually intervene on one occasion. The
algorithm hung for 4 hours performing similarity detection on one pair of
students on one assignment in our sample data (and likely would have run much
longer, had we not ended it prematurely). We investigated the assignment more
closely, and found that two students had submitted an unorthodox solution using
large sets of lookup tables. The two assignments were sufficiently different to
remove unauthorized copying as a factor; we believe they both came upon the
solution independently. The pair of assignments using these lookup tables were
approximately 3000 and 3500 lines each, and were approximately 13000 and 15000
tokens after being run through the default tokenizer of the
\textit{Smith-Waterman} algorithm. In comparison, typical student submissions
for this assignment were perhaps 100 lines of code, and 300 to 400 tokens in
size. By removing these two assignments from the comparison, we were able to
reduce the runtime of Smith-Waterman to around one minute.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{smithwatermanruntime.pdf}
\caption{Runtime of the Smith-Waterman algorithm on our sample data. Each data point is one anonymized assignment.}
\label{fig:smithwatermanruntime}
\end{figure}


\subsubsection{Algorithm Comparison}
Figure~\ref{fig:resultsbyalgo} shows the results from 6 randomly-selected
assignments from the overall dataset. The results are grouped by assignment,
and show how many results were detected by each algorithm for each of the
assignments. It is immediately clear that, for two of the six assignments, the
results for \textit{Line Comparison} are strict subsets of those from
\textit{Smith-Waterman}. However, the trend is somewhat reversed for
assignments three and five, where a majority of results were detected by
\textit{Line Comparison} and not by \textit{Smith-Waterman}. Manual review of
these results indicates that most of these results are cases of one student
submitting very little to no code (typically in the dozens of lines, with many
being trivially simple --- for example, \} or \texttt{return;}). Many of these
lines are also contained in larger assignments, so the trivial assignment
appears to be very similar to larger assignments to \textit{Line Comparison}
(the larger assignments typically display inverse similarities that are very
small --- 5 to 20\% being common). \textit{Smith-Waterman} has awareness of the
ordering of tokens within a document, and consequently will ignore trivial
sequences like \texttt{\{ return; \}} if they are not present in the submission
being compared to in an almost-identical form. Through this, we can conclude
that almost all results detected by \textit{Line Comparison} but not
\textit{Smith-Waterman} are false positives, providing further evidence for the
accuracy of the \textit{Smith-Waterman} algorithm.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{resultsbyalgo.pdf}
\caption{Detection Algorithm Results for 6 Selected Assignments}
\label{fig:resultsbyalgo}
\end{figure}


\subsection{MOSS Comparison}
We identified six assignments, containing over 120 individual student
submissions, from our overall dataset to run through MOSS using the
methodology explained in Section~\ref{sec:mosscompare}. These submissions were
run through MOSS, and the results were compared to those obtained using
\textit{Checksims} using the Smith-Waterman algorithm. 

For the purposes of this comparison, we must clarify our definition of
``significant results.'' In \textit{Checksims} and throughout this paper, we
have generally defined this as any result with a similarity percentage of over
70\%. MOSS, however, does not have such a definition, and instead presents
results ordered by number of similar tokens, with nothing to signify which
results are worthy of being inspected and which are not. For each comparison,
we were able to identify a point in the results where results after had many
fewer similar tokens than results before; we used these points to define
significant results in MOSS, with any results coming before being considered
significant. We acknowledge that this is not an optimal method of selecting
significant results, but could not find a better solution. Given the ease of
selecting relevant results in \textit{Checksims}, we suggest that the overall
usability of our results are better than MOSS, are no further research was
performed to validate our suspicion.

\textbf{Overall, we found that every significant similarity identified by MOSS
was also identified by \textit{Checksims}.} The reverse, however, was not true.
Several small submissions (of 75 lines and under) were identified as being
very similar to other submissions by \textit{Checksims}, but were not found
similar by MOSS. We found eight such submissions, and visually examined each to
make a final determination of which piece of similarity detection software was
correct. We sided with \textit{Checksims} in all cases, as the assignments were
very similar on visual examination. This may indicate a weakness of MOSS with
very small submissions, but we have insufficient data to draw conclusions.

It is worth noting that the definition of similar matches differs between
\textit{Checksims} and MOSS. \textit{Checksims} ranks similarities based on
the percent of tokens in an assignment that match another assignment. MOSS,
on the other hand, ranks similarities based on the overall number of tokens
matching another assignment. Because of this, some matches considered
significant by \textit{Checksims} are not considered significant to MOSS, which
we observed several times in the results. Small submissions containing a high
percentage of matched tokens were ranked far higher by \textit{Checksims} than
MOSS, though MOSS did identify the similar tokens (and high percentage
similarity). We consider such submissions to be identified by both
\textit{Checksims} and MOSS, even though they were ranked highly by one and not
the other. We did not observe any matches ranked highly by MOSS but not
\textit{Checksims}, though this could theoretically occur in very large
submissions with a large number of matching tokens, but a low percentage of
matching tokens.

Our comparison also does not test one of the potential strengths of MOSS over
\textit{Checksims}. MOSS performs syntax-aware comparisons, which lets it
perform much more powerful normalizations than \textit{Checksims} is capable of
at present. These normalizations are described in
Section~\ref{sec:syntaxaware}, and should allow MOSS to defeat deliberate
attempts to obfuscate similar code that would otherwise go unnoticed.
Comparing data including such submissions might highlight the advantages of
MOSS over \textit{Checksims}, but we could not find any submissions in our
dataset that contained any significant attempt to obfuscate similarities.
Because of this, our comparisons are more favorable to \textit{Checksims} than
the real world may be. It is possible that the prevalence of deliberately
obfuscated similarities may rise if knowledge that a similarity detection
system is in use becomes widespread, which would make the performance of 
\textit{Checksims} in such cases more important, and we would like to perform
further experiments in this direction.


\section{Real-World Usage}
During this project, \textit{Checksims} was used in real-world situations on two
occasions. Course staff (including one of the authors) made use of development
versions of the program to attempt to identify academic dishonesty in
student submissions in ongoing courses. Though results from these real-world
uses cannot be published for privacy reasons, they provide valuable insight
into how \textit{Checksims} would be used in typical class.


\subsection{Embedded Computing in Engineering Design}
The first usage of \textit{Checksims} was in an Electrical and Computer
Engineering department course, \textit{Embedded Computing in Engineering
Design}. A teaching assistant in that course, Nicholas DeMarinis, became
concerned that some of his students may have been collaborating on their
microcontroller code beyond was was allowed by course rules. He was provided an
early version of \textit{Checksims} to verify his suspicions. Though he was
not permitted to share his results, he provided valuable feedback on improving
the usability of the program. He requested the ability to remove common code
from a submission and an output format suitable for import into a spreadsheet
program for performing statistics calculations, both of which are present in
\textit{Checksims} as of the time of this writing.


\subsection{Machine Organization and Assembly Language}
A nearly-complete version of \textit{Checksims} was applied to the first
assignment in a course for which one of the authors, Matthew Heon, was a
teaching assistant. This assignment was an excercise in optimization and
bitwise operations and was composed of a relatively small C project with a
great deal of common code. In a class of 68, \textit{Checksims} detected three
students with extremely similar submissions. The grading
system for that assignment was highly automated and involved very little human
interaction. Without \textit{Checksims}, It is unlikely that the similarity
would have been detected.
