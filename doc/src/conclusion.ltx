\section{Future Work}
Checksims was built to be extensible and easily support new features, as
described in Section~\ref{sec:approach}. While this is generally considered to
be a good design philosophy, it was especially important because we did not
have time to add so many features that we felt might be valuable. There is
much work that can be done in the future to add these missing features.

We feel that one of the most important features that can be added to
\textit{Checksims} is the ability to use fingerprinting algorithms with a
persistent database of student submissions; such an improvement would allow
fast checking not only within a single assignment (as \textit{Checksims} does
today), but also against every assignment previously run through the detector
--- detecting students who, for example, used another student's assignment
from a previous term. Furthermore, advanced fingerprinting algorithms offer
most of the accuracy of Vector Distance approaches (like
\textit{Smith-Waterman}) while being substantially faster; for details, see
Section~\ref{sec:moss}). The addition of a fast comparison algorithm based on
MOSS would alleviate the speed disadvantages of \textit{Smith-Waterman} on
large classes and assignments.

To add some of the benefits of fingerprinting algorithms to the
\textit{Smith-Waterman} algorithm, it would be useful to add the ability to
specify an ``archive'' directory, which would contain a number of student
submissions from previous years. The archived submissions would be compared
against all student submissions from the current year, but not against each
other, greatly reducing the number of comparisons that must be made and
removing extraneous results about previous years. Adding such a feature would
be a relatively small set of changes that would make the
\textit{Smith-Waterman} algorithm much more usable when comparing with past
assignments.

Additional output strategies would be greatly beneficial to the use of
\textit{Checksims}. While we feel that our HTML and Threshold output strategies
are adequate for everyday use, they fall short of being ideal for usability.
For example, an output strategy that would offer the option to view the
similarities detected between two assignments would potentially be very
valuable to course staff, as it could speed their investigation of suspected
cases of similarity substantially.

Usability improvements for \textit{Checksims} could also come from tools to
make it easier to apply to assignments. Professor Lauer has suggested
integration with \textit{TurnIn}, an in-house project submission platform used
at WPI, which could run a \textit{Checksims} scan automatically after
submissions for each assignment are closed. Another option would be a GUI
wrapper for \textit{Checksims} to automate common tasks (for example, placing
student submissions into separate folders, or decompressing submissions given
as .zip or .tar files).

It was suggested to the authors that the comparison of programs at runtime
could provide useful metrics for similarity detection --- for example,
Valgrind profiling of memory use and execution time. We did not find
significant investigation into such techniques in our literature review, and
they could certainly be investigated using \textit{Checksims}.


\section{Conclusion}
\textit{Checksims} provides instructors with a simple, cross-platform interface
that allows programming instructors to rapidly check large numbers of
assignments for academic dishonesty. It has been very successful in tests on
old assignments, and shows accuracy equivelant to the industry-standard
solution in initial testing. Moreover, its limited real-world deployment has
already revealed a dramatic amount of unauthorized copying --- 37 cases of
near-certain academic dishonesty were found in 43 total assignments from
9 previous offerings of courses. Additional work and expanded deployment will
likely increase its success.
