@inproceedings{ahtiainen06,
  title={Plaggie: GNU-licensed source code plagiarism detection engine for Java exercises},
  author={Ahtiainen, Aleksi and Surakka, Sami and Rahikainen, Mikko},
  booktitle={Proceedings of the 6th Baltic Sea conference on Computing education research: Koli Calling 2006},
  pages={141--142},
  year={2006},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    GNU GPL licensed Java 1.5 source code plagiarism detection software dating to
    2006. No evidence of active development after this point.}
}
@inproceedings{arwin2006plagiarism,
  title={Plagiarism detection across programming languages},
  author={Arwin, Christian and Tahaghoghi, Seyed M.M.},
  booktitle={Proceedings of the 29th Australasian Computer Science Conference-Volume 48},
  pages={277--286},
  year={2006},
  organization={Australian Computer Society, Inc.},
  annote={Introduces \textit{XPlag}, a tool designed to detect instances of
    code copied from one programming language into another. \textit{XPlag}
    functions by comparing the intermediate representation of two programs
    written in different languages but built by the same compiler suite.}
}
@inproceedings{baker95,
  title={On finding duplication and near-duplication in large software systems},
  author={Baker, Brenda S.},
  booktitle={Reverse Engineering, 1995., Proceedings of 2nd Working Conference on},
  pages={86--95},
  year={1995},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents \textit{Dup}, a tool for locating instances of duplication or
    near-duplication in a large software system.}
}
@inproceedings{belkhouche04,
  title={Plagiarism detection in software designs},
  author={Belkhouche, Boumediene and Nix, Anastasia and Hassell, Johnette},
  booktitle={Proceedings of the 42nd annual Southeast regional conference},
  pages={207--211},
  year={2004},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Focuses on very high-level comparison, analyzing design (code
    structure, data structures) of two programs for similarity. Performs
    progressively higher-level analysis on C programs at five levels of
    abstraction. Very language-specific approach.}
}
@article{beth14,
  title={A Comparison of Similarity Techniques for Detecting Source Code Plagiarism},
  author={Beth, Bradley},
  year={2014},
  annote={
    \setlength{\parskip}{1.5ex}
    Beth measures the performance of four approaches to plagiarism detection
    against a simulated corpus of plagiarized C programs using five distinct
    obfuscation techniques: comment alteration, whitespace padding, identifier
    renaming, code reordering, and refactoring algebraic expressions. The project
    measures the effectiveness of Levenshtein edit distance (in source and in the
    LLVM intermediate representation bitcode), tree edit distance in the abstract
    syntax tree, graph edit distance in the control flow graphs, and $w$-shingling,
    both in the source and in the IR bitcode. The algorithms are also checked
    against an unrelated piece of source code to check for false positives, and
    their performance was also compared with the performance of MOSS.
    \par
    The author limited the corpus to C programs, but believes the results will be
    similar for any compiler that uses the LLVM toolchain. The corpus is, however,
    very small, and perhaps not very realistic. The results are certainly not
    conclusive, but they provide a decent starting point.
    \par
    It seems that most approaches detect some attacks well, but not others. The
    author had comments and whitespace removed before running the checks, so both
    of those approaches did poorly. It seems that changes to order and nomenclature
    are best detected when checking compiler-generated structures rather than the
    source itself. $w$-shingling against the LLVM Intermediate Result performed ``the
    best.''
    \par
    ``$w$-shingling measures the proportion of $n$-gram sequences two documents have in
    common to the total number of $n$-gram sequences that occur in either document.''
    Beth speculates that MOSS uses a similar $n$-gram fingerprinting retrieval
    system called winnowing.}
}
@inproceedings{bowyer99,
  title={Experience using `MOSS' to detect cheating on programming assignments},
  author={Bowyer, Kevin W. and Hall, Lawrence O.},
  booktitle={Frontiers in Education Conference, 1999. FIE'99. 29th Annual},
  volume={3},
  pages={13B3--18},
  year={1999},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Old paper (1999) but very relevant as it directly discusses plagiarism in a
    programming class setting. Describes MOSS, a web-based plagiarism detection
    service still made available by Stanford (upwards of a decade after initial
    inception). MOSS does not provide details of algorithms used internally,
    does not provide source code, but does provide a useful benchmark for
    usability given its popularity.}
}
@inproceedings{brin95,
  title={Copy detection mechanisms for digital documents},
  author={Brin, Sergey and Davis, James and Garcia-Molina, Hector},
  booktitle={ACM SIGMOD Record},
  volume={24},
  number={2},
  pages={398--409},
  year={1995},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents COPS, a fingerprinting copy detection approach based on chunking.
    Brin, et al., note that chunking approaches cannot make use of the underlying
    structure of the document. A chunk is simply a group of units that do have
    some structural meaning (units are remarkably similar to ``tokens'' as we
    describe them in other parts of the literature).
    \par
    Brin et al. present four chunking strategies:
    \begin{enumerate}
    \item One unit equals one chunk
    \item Non-overlapping chunks of multiple units
    \item Multiple-unit chunks with maximum overrlap
    \item Using the smallest chunk whose hash is divisible by some number $p$.
    \end{enumerate}

    One unit equals one chunk was rejected for its high space requirement.
    Non-overlapping chunks were rejected because of their phase dependence.
    All-overlapping chunks were rejected for having the same space requirement
    as one-unit chunks, and because an attacker could defeat it by changing one
    unit in each chunk. Expanding chunks until the chunk hash is divisible by
    some number $p$; note the similarity to document fingerprinting approaches.
    The authors choose the last strategy.}
}
@article{burrows07,
  title={Efficient plagiarism detection for large code repositories},
  author={Burrows, Steven and Tahaghoghi, Seyed M.M. and Zobel, Justin},
  journal={Software: Practice and Experience},
  volume={37},
  number={2},
  pages={151--175},
  year={2007},
  publisher={Wiley Online Library},
  annote={
    \setlength{\parskip}{1.5ex}
    Focuses on efficiency over large data sets. Details indexing algorithms adapted
    from genomic information retrieval for maintaining a database of source code
    that new submissions can be compared against, in a very efficient manner (so
    as to scale to many thousands or tens of thousands of submissions).}
}
@article{buss94,
  title={Investigating reverse engineering technologies for the CAS program understanding project},
  author={Buss, E. and De Mori, Renato and Gentleman, W. Morven and Henshaw, John and Johnson, Howard and Kontogiannis, Kostas and Merlo, Ettore and Muller, HA and Mylopoulos, John and Paul, Santanu and others},
  journal={IBM Systems Journal},
  volume={33},
  number={3},
  pages={477--500},
  year={1994},
  publisher={IBM},
  annote={
    \setlength{\parskip}{1.5ex}
    \setlength{\parskip}{1.5ex}
    Presents various reverse engineering assistance techniques designed to assist
    with program understanding. Used IBM SQL/DS, a large tool written in the
    proprietary PL/AS language, as a reference system. The program undertook seven
    top-level goals:
    \begin{enumerate}
    \item Detect uninitialized data, pointer errors, and memory leaks
    \item Detect data type mismatches
    \item Find incomplete uses of record fields
    \item Find similar code fragments
    \item Localize algorithmic plans
    \item Recognize inefficient or high-complexity code
    \item Predict the impact of change.
    \end{enumerate}
    \par
    The heading ``Pattern Matching'' in the background was fairly useful. They
    mention a few methods:
    \begin{itemize}
    \item Text analysis: The huge advantage of text analysis, of course, is that it is
       language-independent. Interesting quote: ``For some understanding purposes,
       less analysis is better; syntactic and semantic analysis can actually
       information content in the code, such as formatting, identifier choices,
       white space, and commentary. Evidence to identify instances of cut-and-paste
       is lost as a result of syntactic analysis.'' The text analysis research
       occurred at IBM and was done by Johnson, who coauthored this paper as well
       as others in this bibliography.
    \item Syntactic analysis: This research occurred at the University of Michigan and
       did not seem to be interested in detecting cloned code. However, the results
       might be at least somewhat applicable. The authors complain about existing
       (mostly text-based and graph-based) search tools, and then present SCRUPLE,
       a pattern-based query system with a powerful pattern language that allows
       the programmer to search, for instance, for three nested loops in order to
       find a matrix multiplication.
    \item Semantic analysis: This research occurred at McGill University, and
       clone detection was just one of their goals. The McGill research focuses
       on representing code as a vector of complexity metrics; close vectors
       are more likely to be similar (this is a classic vector distance
       algorithm). The McGill research uses five metrics:
       \begin{enumerate}
       \item Number of functions called
       \item Ratio of input-output variables to fanout
       \item McCabe's cyclomatic complexity
       \item Albrecht's function-point quality metric
       \item Henry-Kafura's information flow quality metric
       \end{enumerate}
       The researchers measure distance in two ways: by Euclidean distance, and
       by clustering thresholds on each axis. Needless to say, this approach is
       language-dependent.
    \end{itemize}
    \par
    
    Text analysis found \num{727} copied lines out of a \num{51655} line sample of source code
    from SQL/DS. The processing took two hours. They say that subgroup's research
    is now focused on finding approximate matches, implying to me that their
    findings include only exact matches. Syntactic analysis was not used to detect
    program similarity, and no quantitative results were presented for semantic
    analysis. The authors present a few qualitative takeaways:
    \par
    \begin{enumerate} 
    \item Domain-specific knowledge is critical in easing the interpretation of large
    software systems
    \item Program representations for efficient queries are essential
    \item Many kinds of approaches are needed in a comprehensive reverse engineering
    approach
    \item An extensible approach is needed to consolidate these diverse approaches
    into a unified framework
    \end{enumerate}
    \par
    They closed with a discussion of how they integrated their tool into SQL/DS,
    which is not relevant to this project.}
}
@article{chen04,
  title={Shared information and program plagiarism detection},
  author={Chen, Xin and Francia, Brent and Li, Ming and Mckinnon, Brian and Seker, Amit},
  journal={Information Theory, IEEE Transactions on},
  volume={50},
  number={7},
  pages={1545--1551},
  year={2004},
  publisher={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Attempts to ``take a step back'' and develop a universal measure for the
    amount of information shared between two sequences, be they DNA, text, or source
    code, which can then be used to make a determination on plagiarism. However, to
    make use of this algorithm, the program must be parsed into tokens to remove
    whitespace issues (amongst other reasons). Solution is named SID --- Software
    Integrity Diagnosis.}
}
@inproceedings{ciesielski08,
  title={Evolving similarity functions for code plagiarism detection},
  author={Ciesielski, Vic and Wu, Nelson and Tahaghoghi, Seyed},
  booktitle={Proceedings of the 10th annual conference on Genetic and evolutionary computation},
  pages={1453--1460},
  year={2008},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Discusses the use of genetic algorithms to tune an existing algorithm for
    similarity evaluation (Okapi) for optimum accuracy, and furthermore uses
    particle swarm genetic optimization to devise novel formulas for plagiarism
    detection.}
}
@inproceedings{clough03,
  title={Old and new challenges in automatic plagiarism detection},
  author={Clough, Paul and others},
  booktitle={National Plagiarism Advisory Service, 2003; \url{http://ir.shef.ac.uk/cloughie/index.html}},
  year={2003},
  organization={Citeseer},
  annote={
    \setlength{\parskip}{1.5ex}
    TODO}
}
@article{cosma06,
  title={Source-code plagiarism: A UK academic perspective},
  author={Cosma, Georgina and Joy, M.S.},
  year={2006},
  publisher={University of Warwick},
  annote={
    \setlength{\parskip}{1.5ex}
    A survey of UK academics focused not on how to detect plagiarism, but what it
    is in the context of source code and programming classes.}
}
@article{crochemore01,
  title={A fast and practical bit-vector algorithm for the longest common subsequence problem},
  author={Crochemore, Maxime and Iliopoulos, Costas S and Pinzon, Yoan J and Reid, James F},
  journal={Information Processing Letters},
  volume={80},
  number={6},
  pages={279--285},
  year={2001},
  publisher={Elsevier},
  annote={
    \setlength{\parskip}{1.5ex}
    Efficient solution to Longest Common Subsequence problem, which has important
    implications for plagiarism detection (though it cannot cope with comments,
    whitespace, etc on its own).}
}
@inproceedings{gabel08,
  title={Scalable detection of semantic clones},
  author={Gabel, Mark and Jiang, Lingxiao and Su, Zhendong},
  booktitle={Software Engineering, 2008. ICSE'08. ACM/IEEE 30th International Conference on},
  pages={321--330},
  year={2008},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    This presents a scalable approach to identifying ``semantic codes''---
    semantically equivalent source code blocks (here presented in the context of
    the detection of dead/redundant code, but plagiarism applications are obvious).
    Reports that fingerprinting is not as good as some other methods --- the
    conclusion mentions the existing ``identity algorithm'' is more accurate in
    their testing.}
}
@article{irving04,
  title={Plagiarism and Collusion Detection using the Smith-Waterman Algorithm},
  author={Irving, Robert W.},
  journal={University of Glasgow},
  year={2004},
  annote={
    \setlength{\parskip}{1.5ex}
    A similarity detection algorithm for plaintexts intended for plagiarism
    detection. According to conclusion, very accurate, but slow --- perhaps too slow
    for anything but very small batches of files.}
}
@inproceedings{johnson94,
  title={Substring Matching for Clone Detection and Change Tracking},
  author={Johnson, J. Howard},
  booktitle={Software Maintenance, 1994. Proceedings, International Conference on},
  pages={120--126},
  year={1994},
  organization={ieee},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents a tool for locating similarities in text, including source code. The
    tool works in six steps:
    \begin{enumerate}
    \item \label{itm:txt} Perform text-to-text transformations on each file
    \item \label{itm:substrings} Break the text into potentially overlapping
      substrings
    \item \label{itm:rawmatch} Generate a database of ``raw matches'' by
      finding the substrings that match
    \item \label{itm:iterate} Iterate to describe the matches more concisely
    \item \label{itm:correct} Perform task-specific data reduction
    \item \label{itm:summarize} Summarize high-level matches
    \end{enumerate}
    \par
    Steps \ref{itm:substrings}, \ref{itm:rawmatch}, and \ref{itm:iterate} work
    only on exact matches, so any partial matching must be done via
    normalization in step \ref{itm:txt}. Common transformations include white
    space removal, comment removal, and identifier renaming. Steps
    \ref{itm:substrings}-\ref{itm:iterate} have the advantage of being
    language-agnostic. Note that this approach is reminiscent of the document
    fingerprinting approaches to plagiarism detection. Step \ref{itm:rawmatch}
    is done by karp-rabin string matching \cite{karp87}. In step
    \ref{itm:iterate}, they perform tasks such as merging consecutive matches
    and other ``lossless'' compression strategies. Step \ref{itm:correct} is
    where one might  perform ``lossy'' compression, such as eliminating certain
    text (like, for instance, a copyright notice) that is expected to be
    cloned.
    \par
    The authors tested their prototype on gcc, versions 2.5.8 and 2.3.3; both
    releases together total \num{1440} files with a combined size of \num{40} megabytes.
    They found a total of \num{988} ``clusters,'' or matched substrings. \num{315} of these
    clusters were of type ``abx'' (contained in one file from each release) or
    type ``ab='' (contained in one file from each release with the same name).
    These represent code that was not changed between releases, or that was
    moved to a different location. There were a very large number of abx
    clusters as a result of a large naming convention change done by the gcc
    team between the two releases. They identified some software cloning, and
    areas of massive changes. The authors reported very few nonsense matches.}
}
@inproceedings{jones01,
  title={Metrics Based Plagarism Monitoring},
  author={Jones, Edward L.},
  booktitle={Journal of Computing Sciences in colleges},
  volume={16},
  number={4},
  pages={253--261},
  year={2001},
  organization={consortium for computing sciences in colleges},
  annote={
    \setlength{\parskip}{1.5ex}
    A developed example of ``feature comparison'' --- creates
    and examines ``profiles'' of program features (line count, number of unique
    tokens, average line length, number of spaces, that sort of thing). No evidence
    is presented that it is actually effective, and indeed they do not test on
    real-world data (only note that they intend to use it in their own courses)}
}
@article{joy1999,
  title={Plagiarism in Programming Assignments},
  author={Joy, Mike and Luck, Michael},
  journal={Education, IEEE transactions on},
  volume={42},
  number={2},
  pages={129--133},
  year={1999},
  publisher={ieee},
  annote={
    \setlength{\parskip}{1.5ex}
    \par
    Joy and Luck provide a definition of plagiarism, ``unacknowledged copying of documents
    or programs,'' and name several potential causes of plagiarism.
    \par
    Joy and Luck describe
    two obfuscation techniques: lexical changes (comment changes, formatting,
    changing identifier names, etc.) and structural changes (loop replacement, ifs
    to cases, statement ordering, refactoring, etc.)
    \par
    They describe two pair comparison techniques: comparing attribute counts, and
    comparing structure.
    \par
    They present an algorithm called sherlock, with the following requirements:
    \begin{itemize}
    \item must be reliable
    \item must be simple to change for a new language
    \item must have an ``efficient interface''
    \item output must be clear to somene unfamiliar with the programs
    \end{itemize}
    \par
     incremental comparison compares five times: in original form, with whitespace
     removed, with comments removed, with both removed, and tokenized. looks for
     ``runs'' with a maximum allowed size and density of ``anomalies.'' looks for and
     reports maximum length runs.
    \par
     Presents a very interesting visualization with a point for each submission
     and similarities connected by lines; shorter lines correspond to closer
     matches.}
}
@article{karp87,
  title={Efficient Randomized Pattern-Matching Algorithms},
  author={Karp, Richard M. and Rabin, Michael O.},
  journal={IBM journal of research and development},
  volume={31},
  number={2},
  pages={249--260},
  year={1987},
  publisher={ibm},
  annote={
    \setlength{\parskip}{1.5ex}
    \par
    This is a seminal paper that is cited by almost every article in this
    bibliography. It seems that almost all program similarity detection tools use
    karp-rabin string matching to search for and verify matches.
    \par
    Presents a generalized string-matching algorithm that works in ``real time''
    (each bit of input can be processed as soon as it comes in and requires
    constant time) and in a constant number of registers. It requires keeping a
    substring in memory of the same length as the one you are searching for.
    Authors claim that it seems to be competitive on classical strings only
    for larger substring sizes, but it has a huge advantage in being able to
    search two-dimensional arrays, higher dimensional arrays, and even irregular shapes
    with the same mathematical background.
    \par
    ``The idea of using fingerprinting techniques for string-matching problems
    is not new. Many such techniques based on check sums and hash functions can
    be found in the literature. What is new is the particular way of choosing
    the fingerprinting functions at run time. This randomization technique
    permits us to establish very strong properties of our algorithms, even if
    the input data are chosen by an intelligent adversary who knows the nature
    of the algorithm.''
    \par
    First presents a generalization of all string matching problems, and explains
    how the simple pattern-matching problem fits that framework.
    \par
    Karp and Rabin present three algorithms for deciding whether there is a
    match given an input string $X$ of length $n$, an output string $Y$, a
    finite set of fingerprinting functions $S$, and a set of valid indices $R$.
    Brief descriptions follow:
    \begin{itemize}
      \item Computes $k$ fingerprinting functions on each substring of length
        $n$ in $Y$. If they all report a match, halts immediately. This
        algorithm can report a false match, but only if all $k$ fingerprint
        functions collide.
      \item Computes only one fingerprint function on each substring of length
        $n$ in $Y$, but goes back and verifies the string after a match; false
        matches are caught and discarded.
      \item Same as above, but changes to a different fingerprinting function
        after a false match.
    \end{itemize}
    \par
    The rest of the paper introduces and thoroughly explores the properties of
    several fingerprinting functions. The details are outside the scope of this
    project.}
}
@incollection{khanna07,
  title={A Formal Investigation of \textit{diff3}},
  author={Khanna, Sanjeev and Kunal, Keshav and Pierce, Benjamin C.},
  booktitle={FSTTCS 2007: Foundations of Software Technology and Theoretical Computer Science},
  pages={485--496},
  year={2007},
  publisher={Springer},
  annote={
    \setlength{\parskip}{1.5ex}
    A discussion of diff3, a three-way version of the conventional diff algorithm. This
    could be used for plagiarism detection (detect similarities between two files
    that are not shared by a third, given reference code shared by all students).}
}
@inproceedings{krinke10,
  title={Distinguishing Copies from Originals in Software Clones},
  author={Krinke, Jens and Gold, Nicolas and Jia, Yue and Binkley, David},
  booktitle={Proceedings of the 4th international workshop on software clones},
  pages={41--48},
  year={2010},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Another paper that doesn't really solve the plagiarism problem, and instead
    attempts to find duplicate/dead code. This one is interesting because of its
    categorization metrics, though --- it attempts to classify code as either a
    straight duplicate, close copy, or unclassifiable (some duplicated code, but
    not enough to conclusively classify).}
}
@article{lancaster05,
  title={Classifications of Plagiarism Detection Engines},
  author={Lancaster, Thomas and Culwin, Fintan},
  journal={Innovation in Teaching and Learning in Information and Computer Sciences},
  volume={4},
  number={2},
  year={2005},
  publisher={The Higher Education Academy Innovation Way, York Science Park, Heslington, York YO10 5BR},
  annote={
    \setlength{\parskip}{1.5ex}
    Lancaster provides a detailed taxonomy of plagiarism detection tools, and
    describes all (then) commonly-known examples in terms of that taxonomy.}
}
@article{murugesan10,
  title={Efficient Privacy-Preserving Similar Document Detection},
  author={Murugesan, Mummoorthy and Jiang, Wei and Clifton, Chris and Si, Luo and Vaidya, Jaideep},
  journal={The VLDB Journal; The International Journal on Very Large Data Bases},
  volume={19},
  number={4},
  pages={457--475},
  year={2010},
  publisher={Springer-Verlag New York, Inc.},
  annote={
    \setlength{\parskip}{1.5ex}
    Attempts to detect similar documents when the text of the document is not
    available, for instance, when checking for plagiarism between conferences with
    confidential systems.} 
}
@article{parker89,
  title={Computer Algorithms for Plagiarism Detection},
  author={Parker, Alan and others},
  year={1989},
  publisher={Citeseer},
  annote={
    \setlength{\parskip}{1.5ex}
    Defines a five-level plagiarism ``spectrum'' to categorize the different
    types of obfuscations that are commonly used in duplicated student
    assignments, then provides an overview of some then-advanced similarity
    detection algorithms. All seven algorithms examined are feature-extraction
    algorithms that analyze similarities in software metrics.}
}
@inproceedings{potthast10,
 author={Potthast, Martin and Stein, Benno and Barr'{o}n-Cede\~{n}o, Alberto and Rosso, Paolo},
 title={An Evaluation Framework for Plagiarism Detection},
 booktitle={Proceedings of the 23rd International Conference on Computational Linguistics: Posters},
 series={COLING '10},
 year={2010},
 location={Beijing, China},
 pages={997--1005},
 numpages={9},
 url={http://dl.acm.org/citation.cfm?id=1944566.1944681},
 acmid={1944681},
 publisher={Association for Computational Linguistics},
 address={Stroudsburg, PA, USA},
 annote={
    \setlength{\parskip}{1.5ex}
   Potthast et al. formalize a plagiarism as a 4-tuple consisting of the
   plagiarizing document, the copied document, and the plagiarized and original
   passages within each. They then explain that it is impossible to find an
   adequate source of ``true'' plagiarized material for a number of valid reasons,
   and describe three ways of generating a corpus: pay humans to plagiarize, use
   sources of legitimately copied material such as wire stories, or use an
   algorithm to mutate the document.
   \par
   They present PAN-PC-10, a plagiarism corpus created with Mechanical Turk and an
   algorithmic approach. They compare the corpus with existing corpora Clough09
   and METER, but stop short of claiming that any one database is the best.}
}
@techreport{prechelt00,
  title={JPlag: Finding plagiarism among a set of programs},
  author={Prechelt, Lutz and Malpohl, Guido and Philippsen, Michael},
  year={2000},
  institution={Technical Report 2000-1, Fakultat fur Informatik, Universitat Karlsruhe, D-76128 Karlsruhe, Germany},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents JPlag, a similarity detection tool based on greedy string tiling,
    but with optimizations to decrease the average runtime from $O(n^2)$ to
    $O(n)$. Jplag achieves results comparable to MOSS, but provides a web
    interface rather than an email-based one.}
}
@inproceedings{schleimer03,
  title={Winnowing: local algorithms for document fingerprinting},
  author={Schleimer, Saul and Wilkerson, Daniel S. and Aiken, Alex},
  booktitle={Proceedings of the 2003 ACM SIGMOD international conference on Management of data},
  pages={76--85},
  year={2003},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Schleimer et al. present a document fingerprinting algorithm called winnowing,
    and describe its use in Stanford's MOSS service. They describe the concept of
    ``$k$-gram filtering,'' where a document of n tokens is described as a sequence of
    $(n-k+1)$ overlapping $k$-grams, with a $k$-gram being a sequence of $k$ tokens. In
    $k$-gram filtering, each $k$-gram is hashed, and stored, with its document ID and
    location, in a lookup table. The $k$-grams are reduced to a smaller list using a
    filtering algorithm, and future documents can be checked against this corpus of
    document ``fingerprints.''
    \par
    Our current LineCompare code, described in \ref{sec:linecompare} is an
    implementation of $k$-gram filtering; in LineCompare, each line is a token,
    a document is represented as a sequence of 1-grams, and the 1-grams are
    filtered using the identity function.
    \par
    According to the paper, current (at publication) $k$-gram filters suffer from a
    number of disadvantages. The biggest one is that the filter used is typically a
    mod-$p$ filter; a mod-$p$ filter accepts a $k$-gram $x$ if $H(x)$ is congruent to zero
    mod $p$. Mod-$p$ filters are weak because the fingerprints selected from the
    document are uneven --- there could be huge runs of $n$-grams that do not hash to
    zero mod $p$. In principle, the maximum ``gap width'' in a document is unbounded,
    and in practice it is often longer than most web pages. Mod-$p$ especially chokes
    on low-entropy data --- a long string of zeroes, for instance, will either go
    completely unfingerprinted, or fingerprinted every single time.
    \par
    The paper's contribution is winnowing, a $k$-gram filter that guarantees an upper
    bound on the distance between fingerprints in a document. That means that a
    copy that is longer than the maximum gap width is guaranteed to be detected.
    Winnowing has achieved widespread adoption, including by MOSS, and its merit
    has caused this paper to accumulate \num{711} citations on Google Scholar.
    \par
    Schleimer, et al. introduce the concept of a ``local algorithm,'' an
    algorithm that selects a
    document fingerprint from a ``window'' of consecutive $k$-grams with length $w$. An
    algorithm is local if it meets two conditions:
    \begin{enumerate}
    \item For each possible window, the algorithm selects at least one fingerprint
       from within that window, and
    \item The choice depends only on the contents of that window, not on any other.
    \end{enumerate}
    \par
    The authors demonstrate that if two documents are compared with a $k$-gram filter
    using a local algorithm with window size w, the comparison will detect at least
    one $k$-gram from each shared substring of length $w+k-1$. The minimum density
    (asymptotic proportion of fingerprinted $k$-grams to total $k$-grams) of a local
    fingerprint selection algorithm is $1.5 (w+1)$. Winnowing has an asymptotic
    density of $2/(w+1)$, leading the authors to claim it is ``within 33\% of optimal.''
    \par
    The related work describes the Karp-Rabin algorithm, which finds occurrences of
    a substring in a larger string \cite{karp87}. SCAM \cite{shivakumar95} uses vector
    distance between documents to find copies. Baker \cite{baker95} presents a
    concept called
    ``parameterized matches,'' which can rename parameters to be equal and more
    easily detect copies that way.
    \par
    They ran winnowing ($w=100$) and mod-50, an implementation of mod-$p$ with
    $p=50$, on random data and found that both selected approximately the same
    number of fingerprints per unit of data. Against a corpus of a half million web
    pages, they found that both came close to their expected fingerprint density, but that
    mod-50 was highly non-uniform: mod-50 scanned a run of \num{29900} non-whitespace,
    non-tag characters without selecting a fingerprint. Any duplication within
    those characters would be completely undetected by mod-50.
    \par
    Winnowing ended up fingerprinting extremely densely in low-entropy data, so the
    authors presented a very minor adjustment called ``robust winnowing'' to correct.
  }
}
@article{shivakumar95,
  title={SCAM: A copy detection mechanism for digital documents},
  author={Shivakumar, Narayanan and Garcia-Molina, Hector},
  year={1995},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents SCAM, a vector distance approach to copy detection. Like previous
    work, SCAM breaks documents into chunks and then compares the chunks for
    overlap; but instead of chunking into sentences or paragraphs like previous
    work, SCAM chunks by words. Most previous work in this area simply compared
    the size of the overlap against the size of the document, but that doesn't
    work if you are chunking by words; so the authors propose a new similarity
    approach based on vector distances between word counts.}
}
@article{smith1981identification,
  title={Identification of common molecular subsequences},
  author={Smith, Temple F. and Waterman, Michael S.},
  journal={Journal of molecular biology},
  volume={147},
  number={1},
  pages={195--197},
  year={1981},
  publisher={Elsevier},
  annote={
    \setlength{\parskip}{1.5ex}
    Describes the Smith-Waterman algorithm for comparing genetic sequences.
    This algorithm produces an optimal global overlay between two strings drawn
    from any alphabet. This original paper is very focused on genetics research
    but the algorithm itself is a crucial part of Irving's paper on similarity
    detection.
  }
}
@article{whale1990identification,
  title={Identification of program similarity in large populations},
  author={Whale, Geoff},
  journal={The Computer Journal},
  volume={33},
  number={2},
  pages={140--146},
  year={1990},
  publisher={Br Computer Soc},
  annote={
    \setlength{\parskip}{1.5ex}
    Whale provides an overview of the state of similarity detection in the year
    1990. The document contains a discussion of the prevalence and motivation
    for students to copy others' assignments, common techniques for obscuring
    unauthorized copying, the various metrics to evaluate similarity detectors
    and the poor state of evaluation methodologies, and a broad overview of the
    most common approaches at the time. The paper closes with a comparison of
    the effectiveness of the most popular approaches.}
}

