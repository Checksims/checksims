@inproceedings{ahtiainen06,
  title={Plaggie: GNU-licensed source code plagiarism detection engine for Java exercises},
  author={Ahtiainen, Aleksi and Surakka, Sami and Rahikainen, Mikko},
  booktitle={Proceedings of the 6th Baltic Sea conference on Computing education research: Koli Calling 2006},
  pages={141--142},
  year={2006},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    GNU GPL licensed Java 1.5 source code plagiarism detection software dating to
    2006. No evidence of active development after this point.}
}
@article{aiken05,
  title={Moss: A system for detecting software plagiarism},
  author={Aiken, Alex and others},
  journal={University of California--Berkeley. See www. cs. berkeley. edu/aiken/moss. html},
  volume={9},
  year={2005},
  annote={TODO}
}
@inproceedings{baker95,
  title={On finding duplication and near-duplication in large software systems},
  author={Baker, Brenda S},
  booktitle={Reverse Engineering, 1995., Proceedings of 2nd Working Conference on},
  pages={86--95},
  year={1995},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents Dup, a tool for locating instances of duplication or near-duplication
    in a large software system.}
}
@inproceedings{belkhouche04,
	title={Plagiarism detection in software designs},
	author={Belkhouche, Boumediene and Nix, Anastasia and Hassell, Johnette},
	booktitle={Proceedings of the 42nd annual Southeast regional conference},
	pages={207--211},
	year={2004},
	organization={ACM},
	annote={
          \setlength{\parskip}{1.5ex}
	  Focuses on very high-level comparison, analyzing design (code
          structure, data structures) of two programs for similarity. Performs
          progressively higher-level analysis on C programs at five levels of
          abstraction. Very language-specific approach.}
}
@article{beth14,
  title={A Comparison of Similarity Techniques for Detecting Source Code Plagiarism},
  author={Beth, Bradley},
  year={2014},
  annote={
    \setlength{\parskip}{1.5ex}
    Beth measures the performance of four approaches to plagiarism detection
    against a simulated corpus of plagiarized C programs using five distinct
    obfuscation techniques: comment alteration, whitespace padding, identifier
    renaming, code reordering, and refactoring algebraic expressions. The project
    measures the effectiveness of Levenshtein edit distance (in source and in the
    LLVM intermediate representation bitcode), tree edit distance in the abstract
    syntax tree, graph edit distance in the control flow graphs, and w-shingling,
    both in the source and in the IR bitcode. The algorithms are also checked
    against an unrelated piece of source code (to check for false positives), and
    their performance was also compared with the performance of MOSS, the state of
    the art plagiarism detection service provided by Stanford).
    \par
    The author limited the corpus to C programs, but believes the results will be
    similar for any compiler which uses the LLVM toolchain. The corpus is, however,
    very small, and perhaps not too realistic. The results are certainly not
    conclusive, but they provide a decent starting point.
    \par
    It seems that most approaches detect some attacks well, but not others. The
    author had comments and whitespace removed before running the checks, so both
    of those approaches did poorly. It seems that changes to order and nomenclature
    are best detected when checking compiler-generated structures rather than the
    source itself. w-shingling against the LLVM Intermediate Result performed ``the
    best''.
    \par
    ``w-shingling measures the proportion of n-gram sequences two documents have in
    common to the total number of n-gram sequences that occur in either document''.
    Beth speculates that MOSS uses a similar n-gram fingerprinting retrieval
    system called winnowing.
    \par
    In the introduction, the author differentiated ``code clone detection'' from
    ``source code plagiarism detection'', but did not elaborate. Perhaps we should
    look more closely into the difference.}
}
@inproceedings{bowyer99,
  title={Experience using`` moss'' to detect cheating on programming assignments},
  author={Bowyer, Kevin W and Hall, Lawrence O},
  booktitle={Frontiers in Education Conference, 1999. FIE'99. 29th Annual},
  volume={3},
  pages={13B3--18},
  year={1999},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Old paper (1999) but very relevant as it directly discusses plagiarism in a
    programming class setting. Describes MOSS, a web-based plagiarism detection
    service still made available by Stanford (upwards of a decade after initial
    inception). MOSS does not provide details of algorithms used internally,
    does not provide source code, but does provide what would seem to be a useful
    benchmark for usability given its popularity.}
}
@inproceedings{brin95,
  title={Copy detection mechanisms for digital documents},
  author={Brin, Sergey and Davis, James and Garcia-Molina, Hector},
  booktitle={ACM SIGMOD Record},
  volume={24},
  number={2},
  pages={398--409},
  year={1995},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents COPS, a fingerprinting copy detection approach based on chunking.
    Brin et al. note that chunking approaches cannot make use of the underlying
    structure of the document. A chunk is simply a group of units, which do have
    some structural meaning (units are remarkably similar to ``tokens'' as we
    describe them in other parts of the literature).
    \par
    Brin et al. present four chunking strategies: one unit equals one chunk was
    rejected for its high space requirement. Non-overlapping chunks were rejected
    because of their phase dependence. All-overlapping chunks were rejected for
    having the same space requirement as one-unit chunks, and because an attacker
    could defeat it by changing one unit in each chunk. Expanding chunks until the
    chunk hash is divisible by some number $p$; note the similarity to the
    document fingerprinting approaches we have studied. The authors choose the
    last strategy.}
}
@inproceedings{brixtel10,
  title={Language-independent clone detection applied to plagiarism detection},
  author={Brixtel, Romain and Fontaine, Mathieu and Lesner, Boris and Bazin, Cyril and Robbes, Romain},
  booktitle={Source Code Analysis and Manipulation \(SCAM\), 2010 10th IEEE Working Conference on},
  pages={77--86},
  year={2010},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    TO DO; Very promising abstract}
}
@article{burrows07,
  title={Efficient plagiarism detection for large code repositories},
  author={Burrows, Steven and Tahaghoghi, Seyed MM and Zobel, Justin},
  journal={Software: Practice and Experience},
  volume={37},
  number={2},
  pages={151--175},
  year={2007},
  publisher={Wiley Online Library},
  annote={
    \setlength{\parskip}{1.5ex}
    Focused on efficiency over large data sets. Details indexing algorithms adapted
    from genomic information retrieval for maintaining a database of source code
    which new submissions can be compared against, in a very efficient manner (so
    as to scale to many thousands or tens of thousands of submissions).}
}
@article{buss94,
  title={Investigating reverse engineering technologies for the CAS program understanding project},
  author={Buss, E and De Mori, Renato and Gentleman, W. Morven and Henshaw, John and Johnson, Howard and Kontogiannis, Kostas and Merlo, Ettore and Muller, HA and Mylopoulos, John and Paul, Santanu and others},
  journal={IBM Systems Journal},
  volume={33},
  number={3},
  pages={477--500},
  year={1994},
  publisher={IBM},
  annote={
    \setlength{\parskip}{1.5ex}
    \setlength{\parskip}{1.5ex}
    Presents various reverse engineering assistance techniques designed to assist
    with program understanding. Used IBM SQL/DS, a large tool written in the
    proprietary PL/AS language, as a reference system. The program undertook seven
    top-level goals:
    \begin{enumerate}
    \item Detect uninitialized data, pointer errors, and memory leaks
    \item Detect data type mismatches
    \item Find incomplete uses of record fields
    \item Find similar code fragments
    \item Localize algorithmic plans
    \item Recognize inefficient or high-complexity code
    \item Predict the impact of change.
    \end{enumerate}
    \par
    We are only interested in goal number 4, and those headings are the only ones
    we read.
    \par
    The heading ``Pattern Matching'' in the background was fairly useful. They
    mention a few methods:
    \begin{itemize}
    \item Text analysis: The huge advantage of text analysis, of course, is that it is
       language-independent. Interesting quote: ``For some understanding purposes,
       less analysis is better; syntactic and semantic analysis can actually
       information content in the code, such as formatting, identifier choices,
       white space, and commentary. Evidence to identify instances of cut-and-paste
       is lost as a result of syntactic analysis.'' The text analysis research
       occurred at IBM and was done by Johnson, who coauthored this paper as well
       as others in this bibliography.
    \item Syntactic analysis: This research occurred at the University of Michigan and
       did not seem to be interested in detecting cloned code. However, the results
       might be at least somewhat applicable. The authors complain about existing
       (mostly text-based and graph-based) search tools, and then present SCRUPLE,
       a pattern-based query system with a powerful pattern language that allows
       the programmer to search, for instance, for three nested loops in order to
       find a matrix multiplication.
    \item Semantic analysis: This research occurred at McGill, and clone detection was
       just one of their goals. The McGill research focuses on representing code as
       a vector of complexity metrics; close vectors are more likely to be similar
       (this is a classic vector distance algorithm). The McGill research uses five
       metrics:
       \begin{enumerate}
       \item Number of functions called
       \item Ratio of input-output variables to fanout
       \item McCabe's cyclomatic complexity
       \item Albrecht's function-point quality metric
       \item Henry-Kafura's information flow quality metric
       \end{enumerate}
       The researchers measure distance in two ways: by Euclidean distance, and
       by clustering thresholds on each axis. Needless to say, this approach is
       language-dependent.
    \end{itemize}
    \par
    
    Text analysis found 727 copied lines out of a 51,655 line sample of source code
    from SQL/DS. The processing took two hours. They say that subgroup's research
    is now focused on finding approximate matches, implying to me that their
    findings include only exact matches. Syntactic analysis was not used to detect
    program similarity, and no quantitative results were presented for semantic
    analysis. The authors present a few qualitative takeaways:
    \par
    \begin{enumerate} 
    \item Domain-specific knowledge is critical in easing the interpretation of large
    software systems
    \item Program representations for efficient queries are essential
    \item Many kinds of approaches are needed in a comprehensive reverse engineering
    approach
    \item An extensible approach is needed to consolidate these diverse approaches
    into a unified framework
    \end{enumerate}
    \par
    They closed with a discussion of how they integrated their tool into SQL/DS,
    which is not very interesting to us.}
}
@article{chen04,
  title={Shared information and program plagiarism detection},
  author={Chen, Xin and Francia, Brent and Li, Ming and Mckinnon, Brian and Seker, Amit},
  journal={Information Theory, IEEE Transactions on},
  volume={50},
  number={7},
  pages={1545--1551},
  year={2004},
  publisher={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Attempts to ``take a step back'' and develop a universal measure for the
    amount of information shared between 2 sequences (be they DNA, text, or source
    code) which can then be used to make a determination on plagiarism. However, to
    make use of this algorithm, the program must be parsed into tokens to remove
    whitespace issues (amongst other reasons). Solution is named SID --- Software
    Integrity Diagnosis. I can find no information on current development (paper
    dated 2004), and the official website no longer exists. No evidence source code
    was ever released.}
}
@inproceedings{ciesielski08,
  title={Evolving similarity functions for code plagiarism detection},
  author={Ciesielski, Vic and Wu, Nelson and Tahaghoghi, Seyed},
  booktitle={Proceedings of the 10th annual conference on Genetic and evolutionary computation},
  pages={1453--1460},
  year={2008},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Discusses the use of genetic algorithms to tune an existing algorithm for
    similarity evaluation (Okapi) for optimum accuracy, and furthermore uses
    particle swarm genetic optimization to devise novel formulas for plagiarism
    detection.}
}
@inproceedings{clough03,
  title={Old and new challenges in automatic plagiarism detection},
  author={Clough, Paul and others},
  booktitle={National Plagiarism Advisory Service, 2003; http://ir. shef. ac. uk/cloughie/index. html},
  year={2003},
  organization={Citeseer},
  annote={
    \setlength{\parskip}{1.5ex}
    According to abstract, focuses on natural language.}
}
@article{cosma06,
  title={Source-code plagiarism: A UK academic perspective},
  author={Cosma, Georgina and Joy, MS},
  year={2006},
  publisher={University of Warwick},
  annote={
    \setlength{\parskip}{1.5ex}
    A survey of UK academics focused not on how to detect plagiarism, but what it
    is in the context of source code and programming classes. Useful for abstract/
    introduction, not really useful otherwise as Prof. Lauer has provided his own
    definition, and that is what we are working with for this project.}
}
@article{crochemore01,
  title={A fast and practical bit-vector algorithm for the longest common subsequence problem},
  author={Crochemore, Maxime and Iliopoulos, Costas S and Pinzon, Yoan J and Reid, James F},
  journal={Information Processing Letters},
  volume={80},
  number={6},
  pages={279--285},
  year={2001},
  publisher={Elsevier},
  annote={
    \setlength{\parskip}{1.5ex}
    Efficient solution to Longest Common Subsequence problem, which has important
    implications for plagiarism detection (though it cannot cope with comments,
    whitespace, etc on its own).}
}
@inproceedings{gabel08,
  title={Scalable detection of semantic clones},
  author={Gabel, Mark and Jiang, Lingxiao and Su, Zhendong},
  booktitle={Software Engineering, 2008. ICSE'08. ACM/IEEE 30th International Conference on},
  pages={321--330},
  year={2008},
  organization={IEEE},
  annote={
    \setlength{\parskip}{1.5ex}
    Not a proper discussion of plagiarism detection, but still very applicable.
    This presents a scalable approach to identifying ``semantic codes''---
    semantically equivalent source code blocks (here presented in the context of
    the detection of dead/redundant code, but plagiarism applications are obvious).
    Based on construction of a syntax tree in a form very similar to a function
    call graph.}
}
@inproceedings{gitchell99,
  title={sim: a utility for detecting similarity in computer programs},
  author={Gitchell, David and Tran, Nicholas},
  booktitle={acm sigcse bulletin},
  volume={31},
  number={1},
  pages={266--270},
  year={1999},
  organization={acm},
  annote={
    \setlength{\parskip}{1.5ex}
    a functional description of the previously-described sim utility. no
    significant details of the algorithm are mentioned which are not expanded on in
    the first paper, but it does mention a worthwhile statistic: sim is $o(s^2)$
    complexity, where s is the size of the parse tree of the program being
    processed. this, perhaps, places the scalability papers in a better context?}
}
@article{hoad03,
  title={methods for identifying versioned and plagiarized documents},
  author={Hoad, Timothy C and Zobel, Justin},
  journal={journal of the american society for information science and technology},
  volume={54},
  number={3},
  pages={203--215},
  year={2003},
  publisher={wiley online library},
  annote={
    \setlength{\parskip}{1.5ex}
    a textual approach to source code plagiarism detection based on a
    ``fingerprinting'' method --- in keeping with our ``line-by-line checksum'' examples,
    but more adaptable (can cross multiple lines, etc). apparently, fingerprinting
    is not as good as some other methods, though --- their conclusion mentions the
    existing ``identity algorithm'' is more accurate in their testing.}
}
@article{hordijk08,
  title={structured review of code clone literature},
  author={Hordijk, Wiebe and Ponisio, Mar{'\i}a Laura and Wieringa, Roel},
  year={2008},
  publisher={centre for telematics and information technology, university of twente},
  annote={TODO}
}
@article{irving04,
  title={plagiarism and collusion detection using the smith-waterman algorithm},
  author={Irving, Robert w},
  journal={university of glasgow},
  year={2004},
  annote={
    \setlength{\parskip}{1.5ex}
    a similarity detection algorithm for plaintexts intended for plagiarism
    detection. according to conclusion, very accurate, but slow --- perhaps too slow
    for anything but very small batches of files. this does describe our situation,
    though. worth looking into.}
}
@inproceedings{johnson94,
  title={substring matching for clone detection and change tracking},
  author={Johnson, J Howard},
  booktitle={software maintenance, 1994. proceedings., international conference on},
  pages={120--126},
  year={1994},
  organization={ieee},
  annote={
    \setlength{\parskip}{1.5ex}
    presents a tool for locating similarities in text, including source code. the
    tool works in six steps:
    \begin{enumerate}
    \item \label{itm:txt} perform text-to-text transformations on each file
    \item \label{itm:substrings} break the text into potentially overlapping
      substrings
    \item \label{itm:rawmatch} generate a database of ``raw matches'' by
      finding the substrings that match
    \item \label{itm:iterate} iterate to describe the matches more concisely
    \item \label{itm:correct} perform task-specific data reduction
    \item \label{itm:summarize} summarize high-level matches
    \end{enumerate}
    \par
    steps \ref{itm:substrings}, \ref{itm:rawmatch}, and \ref{itm:iterate} work
    only on exact matches, so any partial matching must be done via
    normalization in step \ref{itm:txt}. common transformations include white
    space removal, comment removal, and identifier renaming. steps
    \ref{itm:substrings}-\ref{itm:iterate} have the advantage of being
    language-agnostic. note that this approach is reminiscent of the document
    fingerprinting approaches to plagiarism detection. step \ref{itm:rawmatch}
    is done by karp-rabin string matching \cite{karp87}. in step
    \ref{itm:iterate}, they perform tasks such as merging consecutive matches
    and other ``lossless'' compression strategies. step \ref{itm:correct} is
    where one might  perform ``lossy'' compression, such as eliminating certain
    text (like, for instance, a copyright notice) that is expected to be
    cloned.
    \par
    the authors tested their prototype on gcc, versions 2.5.8 and 2.3.3; both
    releases together total 1440 files with a combined size of 40 megabytes.
    they found a total of 988 ``clusters'', or matched substrings. 315 of these
    clusters were of type ``abx'' (contained in one file from each release) or
    type ``ab='' (contained in one file from each release with the same name).
    these represent code that was not changed between releases, or that was
    moved to a different location. there were a very large number of abx
    clusters as a result of a large naming convention change done by the gcc
    team between the two releases. they identified some software cloning, and
    areas of massive changes. the authors reported very few nonsense matches.}
}
@inproceedings{jones01,
  title={metrics based plagarism monitoring},
  author={Jones, Edward L},
  booktitle={journal of computing sciences in colleges},
  volume={16},
  number={4},
  pages={253--261},
  year={2001},
  organization={consortium for computing sciences in colleges},
  annote={
    \setlength{\parskip}{1.5ex}
    a developed example of ``feature comparison'' --- creates
    and examines ``profiles'' of program features (line count, number of unique
    tokens, average line length, number of spaces, that sort of thing). no evidence
    is presented that it is actually effective, and indeed they do not test on
    real-world data (only note that they intend to use it in their own courses)}
}
@article{joy1999,
  title={plagiarism in programming assignments},
  author={Joy, Mike and Luck, Michael},
  journal={education, ieee transactions on},
  volume={42},
  number={2},
  pages={129--133},
  year={1999},
  publisher={ieee},
  annote={
    \setlength{\parskip}{1.5ex}
    \par
    joy provides a definition of plagiarism, ``unacknowledged copying of documents
    or programs'', and names several potential causes of plagiarism.
    \par
    joy describes
    two obfuscation techniques: lexical changes (comment changes, formatting,
    changing identifier names, etc.) and structural changes (loop replacement, ifs
    to cases, statement ordering, refactoring, etc.)
    \par
    joy describes two pair comparison techniques: comparing attribute counts, and
    comparing structure.
    \par
    presents an algorithm called sherlock, with the following requirements:
    \begin{itemize}
    \item must be reliable
    \item must be simple to change for a new language
    \item must have an ``efficient interface''
    \item output must be clear to somene unfamiliar with the programs
    \end{itemize}
    \par
     incremental comparison: compare five times: in original form, with whitespace
     removed, with comments removed, with both removed, and tokenized. looks for
     ``runs'' with a maximum allowed size and density of ``anomalies''. looks for and
     reports maximum length runs.
    \par
     presents a very interesting visualization with a point for each submission
     and similarities connected by lines; shorter lines correspond to closer
     matches.}
}
@article{karp87,
  title={efficient randomized pattern-matching algorithms},
  author={Karp, Richard M. and Rabin, Michael O.},
  journal={IBM journal of research and development},
  volume={31},
  number={2},
  pages={249--260},
  year={1987},
  publisher={ibm},
  annote={
    \setlength{\parskip}{1.5ex}
    \par
    this is a seminal paper that is cited by almost every article in this
    bibliography. it seems that almost all program similarity detection tools use
    karp-rabin string matching to search for and verify matches.
    \par
    presents a generalized string-matching algorithm that works in ``real time''
    (each bit of input can be processed as soon as it comes in and requires
    constant time) and in a constant number of registers. it requires keeping a
    substring in memory of the same length as the one you are searching for.
    authors claim that it seems to be competitive on classical strings only
    for larger substring sizes, but it has a huge advantage in being able to
    search 2d arrays, higher dimensional arrays, and even irregular shapes
    with the same mathematical background.
    \par
    "The idea of using fingerprinting techniques for string-matching problems
    is not new. Many such techniques based on check sums and hash functions can
    be found in the literature. What is new is the particular way of choosing
    the fingerprinting functions at run time. This randomization technique
    permits us to establish very strong properties of our algorithms, even if
    the input data are chosen by an intelligent adversary who knows the nature
    of the algorithm."
    \par
    first presents a generalization of all string matching problems, and explains
    how the simple pattern-matching problem we are interested in fits that
    framework.
    todo explain the framework here
    it also provides an example of how a 2d array fits the framework, but we are
    not interested in that. i have not yet finished reading the paper, but a
    description of the relevant stuff will go here.
    \par
    Karp and Rabin present three algorithms for deciding whether there is a
    match given an input string $X$ of length $n$, an output string $Y$, a
    finite set of fingerprinting functions $S$, and a set of valid indices $R$.
    Brief descriptions follow:
    \begin{itemize}
      \item Computes $k$ fingerprinting functions on each substring of length
        $n$ in $Y$. If they all report a match, halts immediately. This
        algorithm can report a false match, but only if all $k$ fingerprint
        functions collide.
      \item Computes only one fingerprint function on each substring of length
        $n$ in $Y$, but goes back and verifies the string after a match; false
        matches are caught and discarded.
      \item Same as above, but changes to a different fingerprinting function
        after a false match.
    \end{itemize}
    \par
    The rest of the paper introduces and thoroughly explores the properties of
    several fingerprinting functions. The details are outside the scope of this
    project.}
}
@incollection{khanna07,
  title={a formal investigation of diff3},
  author={Khanna, Sanjeev and Kunal, Keshav and Pierce, Benjamin C},
  booktitle={fsttcs 2007: foundations of software technology and theoretical computer science},
  pages={485--496},
  year={2007},
  publisher={springer},
  annote={
    \setlength{\parskip}{1.5ex}
    a discussion of diff3, a 3-way version of the conventional diff algorithm. this
    could be used for plagiarism detection (detect similarities between 2 files
    that are not shared by a third [given reference code shared by all students]).}
}
@inproceedings{krinke10,
  title={distinguishing copies from originals in software clones},
  author={Krinke, Jens and Gold, Nicolas and Jia, Yue and Binkley, David},
  booktitle={proceedings of the 4th international workshop on software clones},
  pages={41--48},
  year={2010},
  organization={acm},
  annote={
    \setlength{\parskip}{1.5ex}
    another paper that doesn't really solve the plagiarism prolem, and instead
    attempts to find duplicate/dead code. this one is interesting because of its
    categorization metrics, though --- it attempts to classify code as either a
    straight duplicate, close copy, or unclassifiable (some duplicated code, but
    not enough to conclusively classify). this might be worth carrying over into
    our work.}
}
@article{lancaster05,
  title={Classifications of plagiarism detection engines},
  author={Lancaster, Thomas and Culwin, Fintan},
  journal={Innovation in Teaching and Learning in Information and Computer Sciences},
  volume={4},
  number={2},
  year={2005},
  publisher={The Higher Education Academy Innovation Way, York Science Park, Heslington, York YO10 5BR},
  annote={
    \setlength{\parskip}{1.5ex}
    Lancaster provides a detailed taxonomy of plagiarism detection tools, and
    describes all (then) commonly-known plagiarism detection tools in terms of
    that taxonomy.}
}
@inproceedings{lukashenko07,
  title={Computer-based plagiarism detection methods and tools: an overview},
  author={Lukashenko, Romans and Graudina, Vita and Grundspenkis, Janis},
  booktitle={Proceedings of the 2007 international conference on Computer systems and technologies},
  pages={40},
  year={2007},
  organization={ACM},
  annote={TODO}
}
@article{murugesan10,
  title={Efficient privacy-preserving similar document detection},
  author={Murugesan, Mummoorthy and Jiang, Wei and Clifton, Chris and Si, Luo and Vaidya, Jaideep},
  journal={The VLDB Journal; The International Journal on Very Large Data Bases},
  volume={19},
  number={4},
  pages={457--475},
  year={2010},
  publisher={Springer-Verlag New York, Inc.},
  annote={
    \setlength{\parskip}{1.5ex}
    Attempts to detect similar documents when the text of the document is not
    available, for instance, when checking for plagiarism between conferences with
    confidential systems. Not relevant enough, since we should have access to our
    full corpus.}
}
@article{parker89,
  title={Computer algorithms for plagiarism detection},
  author={Parker, Alan and others},
  year={1989},
  publisher={Citeseer},
  annote={TODO}
}
@inproceedings{potthast10,
 author={Potthast, Martin and Stein, Benno and Barr'{o}n-Cede\~{n}o, Alberto and Rosso, Paolo},
 title={An Evaluation Framework for Plagiarism Detection},
 booktitle={Proceedings of the 23rd International Conference on Computational Linguistics: Posters},
 series={COLING '10},
 year={2010},
 location={Beijing, China},
 pages={997--1005},
 numpages={9},
 url={http://dl.acm.org/citation.cfm?id=1944566.1944681},
 acmid={1944681},
 publisher={Association for Computational Linguistics},
 address={Stroudsburg, PA, USA},
 annote={
    \setlength{\parskip}{1.5ex}
   Potthast et al. formalize a plagiarism as a 4-tuple consisting of the
   plagiarizing document, the copied document, and the plagiarized and original
   passages within each. They then explain that it is impossible to find an
   adequate source of ``true'' plagiarized material for a number of valid reasons,
   and describe three ways of generating a corpus: pay humans to plagiarize, use
   sources of legitimately copied material such as wire stories, or use an
   algorithm to mutate the document.
    \par
   They present PAN-PC-10, a plagiarism corpus created with Mechanical Turk and an
   algorithmic approach. They compare the corpus with existing corpori Clough09
   and METER, but stop short of claiming that any one database is the best.}
}
@inproceedings{roy08mutation,
  title={Towards a mutation-based automatic framework for evaluating code clone detection tools},
  author={Roy, Chanchal K and Cordy, James R},
  booktitle={Proceedings of the 2008 C 3 S 2 E conference},
  pages={137--140},
  year={2008},
  organization={ACM},
  annote={TODO}
}
@inproceedings{schleimer03,
  title={Winnowing: local algorithms for document fingerprinting},
  author={Schleimer, Saul and Wilkerson, Daniel S and Aiken, Alex},
  booktitle={Proceedings of the 2003 ACM SIGMOD international conference on Management of data},
  pages={76--85},
  year={2003},
  organization={ACM},
  annote={
    \setlength{\parskip}{1.5ex}
    Schleimer et al. present a document fingerprinting algorithm called winnowing,
    and describe its use in Stanford's MOSS service. They describe the concept of
    ``k-gram filtering'', where a document of n tokens is described as a sequence of
    (n-k+1) overlapping k-grams, with a k-gram being a sequence of k tokens. In
    k-gram filtering, each k-gram is hashed, and stored, with its document ID and
    location, in a lookup table. The k-grams are reduced to a smaller list using a
    filtering algorithm, and future documents can be checked against this corpus of
    document ``fingerprints''.
    \par
    Note that our current LineCompare code is an implementation of k-gram
    filtering; in LineCompare, each line is a token, a document is represented as a
    sequence of 1-grams, and the 1-grams are filtered using the identity function.
    Since k-gram filtering (referred to by most other literature as ``n-gram''
    filtering) seems to be very prevalent in the literature, we should consider
    refactoring our implementation to be a more general framework.
    \par
    According to the paper, current (at publication) k-gram filters suffer from a
    number of disadvantages. The biggest one is that the filter used is typically a
    mod-p filter; a mod-p filter accepts a k-gram x if H(x) is congruent to zero
    mod p. Mod-p filters are weak because the fingerprints selected from the
    document are uneven --- there could be huge runs of n-grams that do not hash to
    zero mod p. In principle, the maximum ``gap width'' in a document is unbounded,
    and in practice it is often longer than most web pages. Mod-p especially chokes
    on low-entropy data --- a long string of zeroes, for instance, will either go
    completely unfingerprinted, or fingerprinted every single time.
    \par
    The paper's contribution is winnowing, a k-gram filter that guarantees an upper
    bound on the distance between fingerprints in a document. That means that a
    copy which is longer than the maximum gap width is guaranteed to be detected.
    Winnowing has achieved widespread adoption, including by MOSS, and its merit
    has caused this paper to rack up 711 citations on Google Scholar.
    \par
    Schleimer, et al. introduce the concept of a ``local algorithm'', which selects a
    document fingerprint from a ``window'' of consecutive k-grams with length w. An
    algorithm is local if it meets two conditions:
    \begin{enumerate}
    \item For each possible window, the algorithm selects at least one fingerprint
       from within that window, and
    \item The choice depends only on the contents of that window, not on any other.
    \end{enumerate}
    \par
    The authors demonstrate that if two documents are compared with a k-gram filter
    using a local algorithm with window size w, the comparison will detect at least
    one k-gram from each shared substring of length w+k-1. The minimum density
    (asymptotic proportion of fingerprinted k-grams to total k-grams) of a local
    fingerprint selection algorithm is 1.5/(w+1). Winnowing has an asymptotic
    density of 2/(w+1), leading the authors to claim it is ``within 33\% of optimal''.
    \par
    The related work describes the Karp-Rabin algorithm, which finds occurrences of
    a substring in a larger string; it seems pretty fundamental. SCAM uses vector
    distance between documents to find copies. Baker presents a concept called
    ``parameterized matches'', which can rename parameters to be equal and more
    easily detect copies that way.
    \par
    They ran winnowing (w=100) and mod-50 on random data and found that both came
    very close to their expected density. Against a corpus of a half million web
    pages, they found that both came close to their expected density, but that
    mod-50 was highly non-uniform. There was a run of 29,900 non-whitespace,
    non-tag characters without a fingerprint from mod-50. The probability of that
    happening in a random terabyte of data is 10e-220.
    \par
    Winnowing ended up fingerprinting extremely densely in low-entropy data, so the
    authors presented a very minor adjustment called ``robust winnowing'' to correct
    it. Robust winnowing is not a local algorithm, but performed better than
    winnowing.
    \par
    The authors concluded with some linguistic analysis of the web which we don't
    care about, and several extremely useful implementation suggestions.}
}
@article{shivakumar95,
  title={SCAM: A copy detection mechanism for digital documents},
  author={Shivakumar, Narayanan and Garcia-Molina, Hector},
  year={1995},
  annote={
    \setlength{\parskip}{1.5ex}
    Presents SCAM, a vector distance approach to copy detection. Like previous
    work, SCAM breaks documents into chunks and then compares the chunks for
    overlap; but instead of chunking into sentences or paragraphs like previous
    work, SCAM chunks by words. Most previous work in this area simply compared
    the size of the overlap against the size of the document, but that doesn't
    work if you are chunking by words; so the authors propose a new similarity
    approach based on vector distances between word counts.
    \par
    The authors reject the Vector Space Model and find a significant weakness with
    the Cosine Similarity Measure; namely, that it performs poorly when word
    frequency magnitudes differ significantly. They make an adjustment based on
    word frequency and call their measure the Relative Frequency Model (RFM). A
    full description can be found in the paper, but qualitatively, the RFM only
    considers words whose frequencies are ``similar'' according to a tunable
    parameter $\epsilon$.} 
}
@article{smith1981identification,
  title={Identification of common molecular subsequences},
  author={Smith, Temple F and Waterman, Michael S},
  journal={Journal of molecular biology},
  volume={147},
  number={1},
  pages={195--197},
  year={1981},
  publisher={Elsevier},
  annote={
    \setlength{\parskip}{1.5ex}
    Describes the Smith-Waterman algorithm for comparing genetic sequences.
    TODO EXPAND.
  }
}
@inproceedings{arwin2006plagiarism,
  title={Plagiarism detection across programming languages},
  author={Arwin, Christian and Tahaghoghi, Seyed MM},
  booktitle={Proceedings of the 29th Australasian Computer Science Conference-Volume 48},
  pages={277--286},
  year={2006},
  organization={Australian Computer Society, Inc.},
  annote={
    \setlength{\parskip}{1.5ex}
    TODO
  }
}
@article{whale1990identification,
  title={Identification of program similarity in large populations},
  author={Whale, Geoff},
  journal={The Computer Journal},
  volume={33},
  number={2},
  pages={140--146},
  year={1990},
  publisher={Br Computer Soc},
  annote={
    \setlength{\parskip}{1.5ex}
    Gives a list of 13 obfuscation measures which can be used to disguise
    unauthorized copying. TODO EXPAND.    
  }
}


