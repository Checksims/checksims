\section{Need for Evaluation}
Once \textit{Checksims} had been written, the next logical step was to test it
to ensure functionality. The program could not be considered complete unless it
met the requirements identified in Section~\ref{sec:requirements} - for
example, a low false negative rate. Given this, we sought to obtain suitable
test data for use in verifying our implementation.


\subsection{Data Sources}
Obtaining source code with known similarities is not an easy task, however.
Given the intended use of our program, the most relevant source code would be
from student assignments in computer science courses at WPI, and in particular
from intro-level courses. Using this as test data does, however, prove
problematic for several reasons, detailed below.

Existing similarity detectors for textual works often use sets of
procedurally generated works, with known degrees of similarity, to assess the
functioning of their programs. This presents another potential source of data,
though again not without issues.


\subsubsection{Student Code}
Source code from students in Computer Science classes is an attractive test
data option at first glance. This most closely matches the intended use case
for Checksims --- the identification of similar code submissions for
programming projects. A set of test data built from submissions from previous
offerings of the same classes that Checksims may be deployed in will most
closely mirror its use in the real world. 

Using student code does, however, provide a number of strong disadvantages.
The first of these is a substantial privacy concern. Student submissions are
typically only available to the course staff (a professor and typically several
teaching assistants). The students, when submitting, were not notified that
their programs might be used in an academic study. Furthermore, it might be
possible to obtain an approximation of a student's grade based on their
submission, compromising their academic privacy.

% Consider moving if there's a good place for it
\iffalse
These concerns do not apply to a member of the course staff, who already have
access to student source code and grades. Fortuitously, one of the authors was
an undergraduate teaching assistant for the WPI Computer Science department
while this project was completed. Unfortunately, the other author was not,
which meant that any results obtained by this method could not be shared even
within the team --- and even if both team members were able to share this
information, it would not be able to be published.
\fi

In addition to privacy concerns, student code presents another obstacle.
Prior to this project, there was no similarity detection system in common use
in the WPI Computer Science department. Consequently, aside from occasional
submissions which were overly similar identified by course staff, there is no
concrete record of which submissions within a group are similar. Any results on
such code will have no baseline to compare against.

Despite these obstacles, student code remained the most desirable source of
test data. The head of WPI's Computer Science department was contacted for an
opinion on the use of anonymized student code, and stated that it could be used
if all personally identifiable information was removed prior to it being given
to the authors for use in testing. Given this, an anonymization script was
written to remove such information and generate usable test code.


\paragraph{Anonymization Script}
\label{sec:anon}
Student code has the potential to compromise the privacy of the submitting
student if it can be identified who submitted the assignment. However, this
concern is alleviated if all personally identifiable information can be
removed. The head of WPI's Computer Science department gave permission for
student code to be used if this could be done, which prompted the authors to
construct a script to strip such information.

An examination was made of student code that one of the authors (an
undergraduate teaching assistant for WPI's CS department) had access to. From
this, several common forms of personally identifiable information were located.
The first, and easiest to remove, was the filenames of the students' submission
directories, which contained the username of the submitter. Simply renaming the
directories was enough to remove this as a concern. Comments were the next
concern, containing the vast majority of remaining personally identifiable
information. Identifying information like names and usernames almost never
occurred outside of comments. After consulting with several professors, it was
decided that stripping comments and submission names was sufficient to satisfy
the requirement that personally identifying information was removed, with the
caveat that a professor manually review anonymized code to ensure that no
obvious identifying information remained. Professor Lauer graciously
offered to perform this final vetting.

% Cite interview with Fisler?

A script was constructed to accomplish this goal in a largely-automated fashion
to make it feasible to obtain large bodies of test code. Written in Bash, it
leveraged a preexisting script that leverages regular expressions to remove
C and C++ style comments.

% TODO add citation for remcomms sed script

Given the nature of the script, its use is limited to submissions using one of
the C family of languages (C, C++, and Java being the most typically used).

Using this script, a large volume of test code was obtained from several past
offerings of CS2301 (Systems Programming for Non-Majors, taught in C) and
CS2303 (Systems Programming Concepts, taught in C and C++). All programming
assignments for 9 previous offerings of these courses were obtained, totaling
to over 357 thousand lines of code (as computed by David A. Wheeler's
``SLOCCount'' program). Six of the nine courses were taught by Professor Lauer,
the rest by other instructors. TODO average submission size, average students
per class.

(Note to Prof. Lauer: the above is the preferred form of citation for data
generated by SLOCCount, as given in the program's output. Should we use our
own citation format or bow to the author's wishes here?)


\subparagraph{Accuracy Without Comments}
Given that anonymized student code tests were performed on submissions stripped
of comments, the question is raised as to whether this is a reasonable set of
data to compare against. Actual source code will, in all likelihood, have a
sizable number of comments. The absence of comments has the potential to
substantially alter results --- for example, two submissions which differed
only by comments would appear as 100\% similar when, with comments, they might
only be 70\% to 80\%. Indeed, there is the potential that removing comments
might increase the accuracy of a similarity detection program (though it could
also decrease accuracy under other circumstances) . Work on this matter
remains outside the scope of this paper; our use of source code without
comments was forced by previously-mentioned circumstances, and we use it on
the assumption that it represents code encountered in the real world with a
reasonable degree of accuracy.


\paragraph{Baseline Output}
A baseline to compare against was necessary to truly use student code as test
data for \textit{Checksims}. While the output of the program can be
investigated to verify that any reported similarities do exist and weed out
false positives, it is impossible to identify false negatives (submissions
which contain unusual similarities but are not flagged by our software)
without baseline output identifying all similar submissions. Given that we
characterize false negatives as a very undesirable characteristic, it is thus
necessary that we possess the means to verify they are not present.

A manual investigation of an assignment would certainly prove the most precise
manner of identifying similarities. However, as previously mentioned, our
obtained test data exceeds 357,000 lines of code, rendering a manual audit of
all test data impossible in a reasonable timeframe. Even auditing a single
nontrivial assignment would prove extremely time consuming, given class sizes
of 40 to 60 students for most of the test data. It would be possible to audit
only a subset, but this greatly reduces the utility of having such a large
volume of test data.

An alternative to manual auditing would be to use an existing piece of
similarity detection software to provide baseline results. However, no existing
piece of software truly matches the definition of academic dishonest and
unauthorized copying used when building \textit{Checksims}, as was described in
Section~\ref{sec:lauerdishonesty}. This could lead to a great number of false
positives (for a less permissive definition) or negatives (for a more
permissive definition).

Given that it is possible to manually review results to identify false
positives (and doing so is far less time intensive than a manual audit), we
chose to use a similarity detection program with a less stringent definition
of similarity to obtain baseline results, then manually remove false positives.
In doing so, we would also obtain a figure for the number of false positives
such a system would produce if used in place of Checksims, providing insight as
to how necessary the construction of a new system was.

We chose to use MOSS to generate our baseline results. MOSS is, as previously
mentioned in Section~\ref{sec:moss}, is a freely available similarity detection
service intended for identifying academic dishonesty, and is anecdotally known
to have a stricter definition of similarity and academic dishonesty than the
one used to construct Checksims (though previous use of MOSS by Professor
Lauer's course staff). Furthermore, MOSS is considered a benchmark in
similarity detection by many. Numerous papers on new algorithms in similarity
detection will compare their results against MOSS. By doing the same, we can
obtain a measure of our output quality compared against what is often said to
be the golden standard in academic dishonesty detection for source code
\cite{beth14}.
% TODO cite papers which compare against MOSS

It is noteworthy that using MOSS for baseline results will not completely
remove false negatives as a factor. If both MOSS and Checksims do not
identify any unusual similarities that might be indicative of academic
dishonesty, this will constitute a false negative which has been completely
missed. We judge this not particularly likely, given the quality and maturity
of the algorithms used by MOSS (described in the literature review at
\ref{sec:moss}). It is unlikely that MOSS will miss similarities indicate of
academic dishonesty, and if it should, this is not a significant loss, as we
are not aiming to produce a perfect solution, only one that is ``good enough''
--- we do not expect \textit{Checksims} to catch cases which have eluded the
industry-benchmark solution.


\subsubsection{Simulated Similarity}
Most textual similarity detectors are tested and tuned with
procedurally generated sets of work containing deliberately-inserted
similarities. A number of such corpora are freely available online. These
offer the advantage of easy verifiability --- unlike student submissions, there
is a well-defined set of similarities in the submission, so output can easily
be verified. There are no privacy concerns, because the test data was
``written'' procedurally by a program.

These simulated corpora only exist for plaintext, however, where our search is
for similarity in source code. Consequently, we do not feel that these corpora
are especially helpful for testing the overall functionality of Checksims.
They have been incorporated into the project as unit tests for individual
algorithms, providing a verifiable set of results we can test for; they were
not used in full-scale functional testing.


\section{Experimental Verification}
\label{sec:methods}
Having obtained a large volume of test data, several experiments were then
planned to provide full-scale functional testing of \textit{Checksims}.
Comparisons between the two shipped algorithms, \textit{Line Comparison} and
\textit{Smith-Waterman}, could provide the benefits and disadvantages of both
algorithms. Comparisons against MOSS could provide proof of a low
false-negative rate and a benchmark against a proven similarity detection
program. Together, these experiments provided data to allow us to draw
conclusions on whether \textit{Checksims} was ready for its intended job ---
deployment for use by WPI course staff.


\subsection{Algorithm Comparisons}
\label{sec:algcompare}
\textit{Checksims} presently provides two algorithms for similarity detection,
\textit{Line Comparison} and \textit{Smith-Waterman} (detailed in 
Section~\ref{sec:linecompare} and Section~\ref{sec:smithwaterman}). The first
set of tests on \textit{Checksims} focused on comparing these two algorithms to
measure their sensitivity and runtime.

From our research, we could form hypotheses on the behavior of both algorithms.
\textit{Smith-Waterman}, guaranteed to find the most efficient local alignment
of strings, should be more accurate by far than \textit{Line Comparison},
which will fail to identify lines that differ by even one character. However,
\textit{Smith-Waterman}'s exponential runtime and large computational
requirements should mean that it is slower by far than 
\textit{Line Comparison}.

We hope to verify our use cases for these algorithms with this experiment.
\textit{Smith-Waterman} is hypothesized to trade speed for accuracy, and 
\textit{Line Comparison} accuracy for speed; if both hypotheses are correct,
then each algorithm has a use case and should be included in the final release.
If, however, one of the algorithms does not provide its claimed advantage,
there is no reason for its inclusion, and it can be removed from the final
release.

To perform this experiment, \textit{Checksims} was run twice on every
assignment in our test data, once using each algorithm. The default
tokenization strategy was used for both algorithms. Output was saved to a
unique file for each assignment, and program runtime saved to another file.
From these, results were computed. Results with similarities under 70\% were
immediately discarded, to limit the amount of checks which would have to be
done. Results over 70\% were reported for both algorithms over all assignments.
Subsequently, we identified four assignments with a large number of
similarities, and compared the results for the \textit{Smith-Waterman} and
\textit{Line Comparison} algorithms - how many results were shared between
them, and how many were caught by only one algorithm. No false positive testing
was performed.


\subsection{MOSS Comparison}
We aimed to glean several things by comparing the output of \textit{Checksims}
and MOSS on several assignments. By comparing the number of false positives
produced by both programs, we aimed to determine whether MOSS (and its more
strict definition of what constitutes academic dishonesty) produced an
inordinate number of false positives (as compared to \textit{Checksims}).
Furthermore, we aimed to verify that \textit{Checksims} contained few (or no)
false negatives.

To perform this experiment, we first identified a number of assignments from
our test data which had a very large number, or almost no, similarities
(according to data from the previous experiment). Two of each were chosen and
given to MOSS to perform detection on (given that MOSS is a free service, and
not particularly fast, we did not wish to place undue demands on it by running
our entire set of test data through).

The MOSS results for the chosen assignments were then investigated to identify
false positives --- similarities identified by MOSS which did not match our
definition of similarity. The number of false positives (and the original
number of results) were reported, and the false positives thrown out. The
\textit{Checksims} results from the previous experiment for both algorithms
were then run through the same process, with false positives being identified
and thrown out, and the number of overall matches and false positives being
recorded. Based on this, the number of false positives in the results of MOSS
and \textit{Checksims} (both algorithms) can be compared, both in absolute
terms and as a percentage of results recorded. Finally, the results were
overlaid to see which matches are missing from either program --- what MOSS
found but \textit{Checksims} did not, or \textit{Checksims} found but MOSS did
not.
