\section{Need for Evaluation}
Once \textit{Checksims} had been written, the next logical step was to test it
to ensure functionality. The program could not be considered complete unless it
met the requirements identified in Section~\ref{sec:requirements} - for
example, a low false negative rate. Given this, we sought to obtain suitable
test data for use in verifying our implementation.


\subsection{Data Sources}
Obtaining source code with known similarities is not an easy task, however.
Given the intended use of our program, the most relevant source code would be
from student assignments in computer science courses at WPI, and in particular
from intro-level courses. Using this as test data does, however, prove
problematic for several reasons, detailed below.

Existing similarity detectors for textual works often use sets of
procedurally generated works, with known degrees of similarity, to assess the
functioning of their programs. This presents another potential source of data,
though again not without issues.


\subsubsection{Student Code}
\label{sec:studentcode}
Source code from students in Computer Science classes is an attractive test
data option at first glance. This most closely matches the intended use case
for \textit{Checksims} --- the identification of similar code submissions for
programming projects. A set of test data built from submissions from previous
offerings of the same classes that \textit{Checksims} may be deployed in will
most closely mirror its use in the real world. 

Using student code does, however, raise a number of important concerns. The
first of these is a substantial privacy concern. Student submissions are
typically only available to the course staff (a professor and typically
several teaching assistants). The students, when submitting, were not
notified that their programs might be used in an academic study. Furthermore,
it might be possible to obtain an approximation of a particular student's grade
based on that student's submission, compromising his or her academic privacy.

% Consider moving if there's a good place for it
\iffalse
These concerns do not apply to a member of the course staff, who already have
access to student source code and grades. Fortuitously, one of the authors was
an undergraduate teaching assistant for the WPI Computer Science department
while this project was completed. Unfortunately, the other author was not,
which meant that any results obtained by this method could not be shared even
within the team --- and even if both team members were able to share this
information, it would not be able to be published.
\fi

In addition to privacy concerns, student code presents another obstacle.
Prior to this project, there was no similarity detection system in common use
in the WPI Computer Science department. Consequently, aside from occasional
submissions identified by course staff as overly similar, there is no
concrete record of which submissions within a group are similar. Any results on
such code will have no baseline to compare against.

Despite these obstacles, student code remained the most desirable source of
test data. The head of WPI's Computer Science department was contacted for an
opinion on the use of anonymized student code, and stated that it could be used
if all personally identifiable information was removed prior to it being given
to the authors for use in testing. Given this, an anonymization script was
written to remove such information and generate usable test code.


\paragraph{Anonymization Script}
\label{sec:anon}
Student code has the potential to compromise the privacy of the submitting
student, but such concerns can be alleviated if all personally identifiable
information can be removed. The head of WPI's Computer Science department
gave permission for student code to be used if this could be done, which
prompted the authors to construct a script to strip such information.

An examination was made of student code that one of the authors (an
undergraduate teaching assistant for WPI's CS department) had access to. From
this, several common forms of personally identifiable information were located.
The first, and easiest to remove, were the filenames of the students' submission
directories, which contained the usernames of the submitters. Simply renaming
the directories was enough to remove this as a concern. Comments were the
next concern, containing the vast majority of remaining personally
identifiable information. Identifying information like names and usernames
almost never occurred outside of comments. After consulting with the Computer
Science department head, it was decided that stripping comments and submission
names was sufficient to satisfy the requirement that personally identifying
information was removed, with the caveat that a professor manually review
anonymized code to ensure that no obvious identifying information remained.
Professor Lauer graciously offered to perform this final vetting.

A script was constructed in Bash to accomplish this goal in a largely-automated
fashion to make it feasible to obtain large bodies of test code. The script's
use is limited to submissions written in languages that use comments delineated
with the ``\texttt{//}'' and ``\texttt{/* */}'' symbols. The anonymization
script uses a verbatim copy of the \textit{remcomms} Sed script by Brian Hiles
for removing C comments
\footnote{available at http://sed.sourceforge.net/grabbag/scripts/remccoms3.sed}.
The full text of this script is included as Appendix~\ref{sec:anonfull}

Using this script, a large volume of test code was obtained from several past
offerings of CS2301 (Systems Programming for Non-Majors, taught in C) and
CS2303 (Systems Programming Concepts, taught in C and C++). All programming
assignments for 9 previous offerings of these courses were obtained, totaling
to over 357 thousand lines of code \footnote{as computed by David A. Wheeler's
``SLOCCount'' program, a free program for computing the number of lines in a
body of source code}. Six of the nine courses were taught by Professor Lauer,
the rest by other instructors. Between the 9 courses, there were a total of
43 student assignments, with an average of 73 students per course and 147 lines
of code per student submission.


\subparagraph{Accuracy Without Comments}
Given that anonymized student code tests were performed on submissions stripped
of comments, the question is raised as to whether this is a reasonable set of
data to test our program. Actual source code will, in all likelihood, have a
sizable number of comments. The absence of comments has the potential to
substantially alter results --- for example, two submissions that differed
only by comments would appear as 100\% similar when, with comments, they might
only be 70\% to 80\%. Indeed, there is the potential that removing comments
might increase the accuracy of a similarity detection program (though it could
also decrease accuracy under other circumstances). Because it is trivial to
strip comments from files before running \textit{Checksims}, we believe these
results represent a lower bound on the tool's effectiveness. The question of
whether \textit{Checksims} might be even more effective on source code with
comments is beyond the scope of this analysis.


\paragraph{Baseline Output}
A baseline for comparison was necessary to truly use student code as test
data for \textit{Checksims}. While the output of the program can be
investigated to verify that any reported similarities do exist and to eliminate
false positives, it is impossible to identify false negatives (submissions
that contain unusual similarities but are not flagged by our software)
without baseline output identifying all similar submissions. We consider
consider false negatives a very undesirable characteristic, so we would prefer
to verify that they are not present. 

A manual investigation of an assignment would certainly prove the most precise
manner of identifying similarities. However, since our obtained test data
exceeds 357,000 lines of code, a manual audit of all test data is impossible
to complete in a reasonable timeframe. Even auditing a single nontrivial
assignment would prove extremely time consuming, given class sizes of 40 to
60 students for most of the test data. It would be possible to audit only a
subset, but this greatly reduces the utility of having such a large volume of
test data.

An alternative to manual auditing is to use an existing piece of similarity
detection software to provide baseline results. However, no existing piece of
software truly matches the definition of academic dishonesty and unauthorized
copying used when building \textit{Checksims}, as was described in
Section~\ref{sec:lauerdishonesty}. This could lead to a great number of false
positives (for a less permissive definition) or negatives (for a more
permissive definition).

Given that it is possible to manually review results to identify false
positives (and doing so is far less time intensive than a manual audit), we
chose to use a similarity detection program with a less stringent definition
of similarity to obtain baseline results, then manually remove false positives.
In doing so, we would also obtain a figure for the number of false positives
such a system would produce if used in place of \textit{Checksims}, providing
insight as to how necessary the construction of a new system was.

We chose to use MOSS to generate our baseline results. MOSS, described in
Section~\ref{sec:moss}, is a freely available similarity detection service
intended for identifying academic dishonesty. Based on previous use, our
advisor believes it uses a more sensitive definition of academic dishonesty
than that used to construct \textit{Checksims}. Furthermore, MOSS is considered
\textit{the} benchmark in similarity detection by most researchers. many papers
on new algorithms in similarity detection compare their results against MOSS
\cite{belkhouche04, beth14}. By doing the same, we can obtain a measure of our
output quality compared against the gold standard in academic dishonesty
detection for source code \cite{beth14}.

It is noteworthy that using MOSS for baseline results do not completely
eliminate false negatives. If both MOSS and \textit{Checksims} fail to detect
an unusual similarity, our experiment will fail to record a false negative.
We believe this event will be uncommon because of the quality and maturity of
the algorithms used by MOSS (described in Section~\ref{sec:moss}). It
is unlikely that MOSS will miss similarities indicative of academic
dishonesty \cite{schleimer03}. Similarities that manage to escape both MOSS
and \textit{Checksims} will continue to be a challenge, but we are not aiming
to produce a perfect solution, only one that is ``good enough'' --- we do not
expect \textit{Checksims} to catch cases that have eluded the industry
benchmark solution.


\subsubsection{Simulated Similarity}
Most natural language similarity detectors are tested and tuned with
procedurally generated sets of work containing deliberately-inserted
similarities. A number of such corpora are freely available online. They
offer the advantage of easy verifiability --- unlike student submissions,
there is a well-defined set of similarities in the corpus, so output can
easily be verified. There are no privacy concerns, because the test data was
``written'' procedurally by a program.

These simulated corpora only exist for natural language, however, where our
search is for similarity in source code. We have no reason to be confident that
results obtained by testing \textit{Checksims} against natural language will
correspond to our tool's performance against source code. We did use them in
the project as functional tests for individual algorithms, providing a
reproducible set of results to test for, but we do not infer anything about
the tool's quality from their results. 

\section{Experimental Verification}
\label{sec:methods}
After obtaining a large volume of test data, we executed several experiments
to provide full-scale functional testing of \textit{Checksims}. Comparisons
between the two shipped algorithms, \textit{Line Comparison} and
\textit{Smith-Waterman}, illustrate the benefits and disadvantages of both
algorithms. Comparisons against MOSS demonstrate a comparable false-negative
rate and a benchmark against a proven similarity detection program. Together,
the experiments provided data to allow us to draw conclusions on whether 
\textit{Checksims} was ready for its intended job --- deployment for use by
WPI course staff.


\subsection{Algorithm Comparisons}
\label{sec:algcompare}
\textit{Checksims} presently provides two algorithms for similarity detection,
\textit{Line Comparison} and \textit{Smith-Waterman} (detailed in 
Section~\ref{sec:linecompare} and Section~\ref{sec:smithwaterman}). The first
set of tests on \textit{Checksims} focused on comparing these two algorithms to
measure their sensitivity and runtime.

From our research, we could form hypotheses on the behavior of both algorithms.
\textit{Smith-Waterman}, guaranteed to find the most efficient local alignment
of strings, should be more accurate by far than \textit{Line Comparison},
which will fail to identify lines that differ by even one character. However,
\textit{Smith-Waterman}'s quadratic runtime and large memory  requirements
should mean that it is slower by far than \textit{Line Comparison}.

\textit{Smith-Waterman} is hypothesized to trade speed for accuracy, and 
\textit{Line Comparison} accuracy for speed; if both hypotheses were correct,
then each algorithm would have a use case and should be included in the final
release. If, however, one of the algorithms does not provide its claimed
advantage, there is no reason for its inclusion, and it can be removed from
the final release.

To perform this experiment, \textit{Checksims} was run twice on every
assignment in our test data, once using each algorithm. The default
tokenization strategy was used for both algorithms. Output was saved to a
unique file for each assignment, and program runtime saved to another file.
From these, results were computed. Results with similarities under 70\% were
immediately discarded, to limit the number of manual checks that would have to
be done. Results over 70\% were reported for both algorithms over all
assignments. Subsequently, we identified four assignments with a significant
number of similarities, and compared the results for the
\textit{Smith-Waterman} and \textit{Line Comparison} algorithms --- how many
results were shared between them, and how many were caught by only one
algorithm. No false positive testing was performed.


\subsection{MOSS Comparison}
\label{sec:mosscompare}
We aimed to learn several things by comparing the output of \textit{Checksims}
and MOSS on several assignments. By comparing the number of false positives
produced by both programs, we aimed to determine whether MOSS, with its more
sensitive definition of what constitutes academic dishonesty, produced an
inordinate number of false positives compared to \textit{Checksims}.
Furthermore, we aimed to estimate the false negative rate of
\textit{Checksims}.

To perform this experiment, we first identified groups of assignments from
our test data that had a few, some, and many similarities according to data
from the previous experiment. Two of each were chosen and checked with MOSS.

The MOSS results for the chosen assignments were examined by hand to identify
false positives --- similarities identified by MOSS that did not match our
definition of academic dishonesty. The number of false positives (and the
original number of results) were recorded, and the false positives thrown
out. The \textit{Checksims} results from the previous experiment for both  
algorithms were then examined by the same process, with false positives being
identified and thrown out, and the number of overall matches and false
positives being recorded. The false positive rates of MOSS and
\textit{Checksims} (both Line Comparison and \textit{Smith-Waterman}) can be
compared, both in absolute terms and as a percentage of results recorded.
Finally, the results were overlaid to see which matches are missing from
either program --- what MOSS found but \textit{Checksims} did not, or
\textit{Checksims} found but MOSS did not.
