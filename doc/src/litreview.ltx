\section{Definitions}
Several terms are fundamental to the field of similarity detection, but are
not in common use. For clarity, we provide definitions for them here.

\begin{itemize}
\item The terms \textit{academic dishonesty} and \textit{unauthorized copying}
	are used throughout this paper to refer to the use of another's work
	without permission or attribution. In this context, it refers to students
	who use another person's work without attribution and/or without the
	permission of the instructor to do so. This is often referred to as
	``cheating'' or ``plagiarism'' in general use, but we deliberately avoid
	these terms where possible, as they can be perceived to be controversial
	or judgmental.
\item A \textit{corpus} (plural \textit{corpora}) is a body of work. A corpus
	may contain many documents written by many people. A typical corpus in the
	world of academic dishonesty detection might be all the student
	submissions for a specific assignment.
\iffalse
% Commented out as these terms are never used
\item Similarities between works contained within a single corpus are referred
	to as \textit{intra-corpal similarities}. Similarities between a work in a
	corpus and works not contained within a corpus are referred to as
	\textit{extra-corpal similarities}.
\fi
\item A \textit{Token} is a piece of a larger input. Tokens are usually
	generated by an algorithm (typically called a \textit{Tokenizer}) that
	breaks an input string up in a consistent manner (for example, at each
	newline). \textit{Tokens} are formed from the broken-up chunks of the
	input.
\item A \textit{False Positive} for given tool is a result that is matched by
	the tool, but should not be included. In the context of a similarity
	detection system, it would be two or more submissions that are flagged
	as unusually similar, but are not considered to be academic dishonesty.
\item A \textit{False Negative} for a given tool is a result that is not
	matched by the tool, but should have been included. In the context of a
	similarity detection system, it would be two results that are not
	flagged as unusually similar in spite of having been created by
	unauthorized copying.
\end{itemize}


\section{Literature Review}
This section summarizes our review of existing literature in the area of
similarity detection. It is focused on three areas: what is academic
dishonesty (Section~\ref{sec:dishonesty}), what algorithms exist for detecting
academic dishonesty (Section~\ref{sec:algo}), and what preexisting solutions
are there that might also solve the specific problems that were the impetus
for our solution (Section~\ref{sec:existing}).

\subsection{Academic Dishonesty}
\label{sec:dishonesty}
There have been a number of scholarly attempts to provide a definition for the
term ``academic dishonesty'' (and the associated term ``plagiarism'', which we
avoid). Thomas Lancaster, in a 2005 survey of similarity detection systems,
defines academic dishonesty as ``The process by which students submit work for
academic credit which contains other people's unacknowledged words or ideas''
\cite{lancaster05}. Lancaster provides a solid foundation, but there is still a
question of what specific acts constitute academic dishonesty.

The question of academic dishonesty has been further complicated by the
emergence of code-hosting websites such as \textit{Github} and
\textit{Bitbucket}, and Q\&A sites such as \textit{StackOverflow}. Students
may host code they wrote for a class on \textit{Github}, where it may be
copied without permission or attribution by their classmates (or students in
another class who were given the same assignment). In the case of
\textit{StackOverflow}, students may ask questions on how to complete an 
assignment. The answers they receive might include example source code, and 
most would agree that copying this example code (again, without attribution 
and likely without permission) constitutes academic dishonesty. However, 
some professors feel that the use of any information (even hints as to 
algorithms, or pseudocode versions of a solution) obtained from such a 
source constitutes academic dishonesty. In the case of copying from a site like
\textit{GitHub} without the knowledge of the creator, almost anyone familiar
with academia would agree that the student who copied the given code was guilty
of unauthorized copying. However, many go further, arguing that the student who
originally hosted the code is guilty of academic dishonesty for enabling
copying to take place \cite{cosma06}. In all cases, different instructors
have different definitions of which offenses constitute unauthorized copying,
complicating the creation of tools to assist in detecting the practice.

There is no consensus among academics as to what degree of copying constitutes
academic dishonesty, but most professors insist that they ``know it when they
see it''. Mike Joy and Michael Luck provide some example behaviors that they
would consider dishonest \cite{joy1999}:
\begin{itemize}
\item ``A weak student produces work in close collaboration with a
 colleage\textit{[sic.]} in the belief that it is acceptable.''\\
\item ``A weak student copies, then edits, a colleage's program, with or
 without the colleage's permission, hoping that this will go unnoticed.''\\
\item ``A poorly motivated (but not necessarily weak) student copies, and then
 edits, a colleague's program, with the intention of minimizing the work
 needed.''\\
\end{itemize}

A 2006 survey of professors in the UK on academic dishonesty produced a broad
spectrum of results for what professors perceive is (and is not) academic
dishonesty \cite{cosma06}. The sharing of source code, comments, overall
design, documentation, and user interface were all near-universally perceived
to be unauthorized copying amongst respondents to the survey. Specifically
for source code, use of code from other sources without acknowledgment (even
if the source code was adapted to the student's specific application, or
rewritten from another language) was again near-universally considered to be
academic dishonesty. However, many respondents noted that the degree of
adaptation was an important factor, noting that while 100\% similarity was
almost certainly indicative of unauthorized copying, there was a point at
which the changes to the code would cause it to no longer be a case of
unauthorized copying; respondents disagreed as to where, exactly, this point
was. A majority of respondents indicated that almost every offense involving
the unauthorized duplication of source code could, indeed, be considered
academic dishonesty given the right circumstances, save cases where the code
copied was written by the submitting student. In such cases (for example, the
submission of an assignment written previously for a different class, with
slight modifications), most respondents answered that it was a violation of
course policy, but not a matter of academic dishonesty.

\subsubsection{Detection of Academic Dishonesty}
There has been a great deal of research focused on the detection of academic 
dishonesty (both in plaintext and source code); Thomas Lancaster presents a 
thorough overview of existing work in his 2005 paper, including a comparison 
of all existing academic dishonesty detection solutions available at time of 
publication \cite{lancaster05}. The detection of unauthorized copying draws 
heavily on the fields of Natural Language Processing and Information 
Retrieval to process student submissions and efficiently store and retrieve 
information about similarities \cite{beth14}.

Academic dishonesty detection is generally broken into two broad fields. The
first is the detection of unauthorized copying in source code, and the second
the detection of unauthorized copying in natural language. The source code
detection problem is generally considered the easier of the two problems, given
that programming languages follow a fixed grammar. The need for advanced 
natural language processing is mostly eliminated, given the constraints on 
what is valid source code \cite{clough03}. Given the problem we are 
attempting to solve, we are naturally more interested in source code 
similarity detection.
% TODO do we cite lancaster05?


\paragraph{Obfuscation}
\label{sec:obfuscation}
In the case of academic dishonesty, violators have an interest in preventing
the detection of similarities in their programs. Consequently, they often
take steps to obfuscate code in an attempt to hide or remove similarities.
Programs and algorithms that attempt to detect malware or academic dishonesty
must be resistant to common obfuscation techniques to be successful.

Geoffrey Whale listed 12 methods of defeating similarity detection in a 1990
paper \cite{whale1990identification}. Whale's list, widely cited, is reproduced
below, and is considered a definitive enumeration of methods of disguising
unauthorized copying. It is presented in order of sophistication, least to
greatest.
\begin{enumerate}
\item Changing comments or formatting (for example, adding whitespace)
\item Changing identifiers
\item Changing the order of operands in expressions (for example, $1 + 2$
	into $2 + 1$)
\item Changing data types (substituting floats for integers, or exploding
	structures into separate variables)
\item Replacing expressions with semantically identical equivalents (for
	example, \code{!x} with \code{x == false})
\item Adding redundant statements or variables
\item Changing the order of independent statements
\item Changing the structure of iteration statements
\item Changing the structure of conditional statements
\item Replacing procedure calls with procedure bodies
\item Introducing non-structured statements such as \texttt{GOTO}s
\item Combining original and copied program fragments
\end{enumerate}

Similarities are not only useful as indicators of academic dishonesty; the next
section describes similarity detection techniques that are designed for other
applications.

\subsection{Other Applications of Similarity Detection}
There are a number of fields with an interest in detecting similarities between
input documents, most for purposes completely different than ours. These
inputs are not necessarily source code, but the fields in question may have
developed general-purpose algorithms or techniques which could be useful in the
construction of our application.


\subsubsection{Code Clone Detection}
Most professional software engineers agree that source code should not be
duplicated in a large programming project, yet it often is. Naturally, a
number of tools have been developed to search for \textit{code clones}, or
instances of duplicated source code. An early code clone detector,
\textit{dup}, was developed at AT\&T. \textit{Dup} searches C source files
line-by-line and can be used to detect parameterized matches - files that
match when eliminating certain differences such as variable names. In 1993, 
textit{dup} found parameterized matches for 15\% of the X Window System, and
23\% of a large proprietary AT\&T system \cite{baker95}.


\subsubsection{Bioinformatics}
The comparison of similar sequences is common practice in Bioinformatics.
Strands of DNA, RNA, or proteins are often sequenced and compared for a
variety of reasons. A well-known example is the use of DNA testing in
the criminal justice system, where DNA comparisons are used to identify the
perpetrators of crimes from trace evidence. A number of commonly-used
similarity detection algorithms were originally developed for bioinformatics
use --- for example, the Smith-Waterman algorithm described in Section~
\ref{sec:smithwatermanlit} \cite{smith1981identification}.
% TODO Could use a general citation for similarity checking in bioinformatics


\subsubsection{Copyright Infringement Detection}
Some people have tried to use similarity detection techniques to detect
copyright infringement. The earliest copyright infringement detection tool we
found was \textit{COPS} (COpyright Protection System), by Sergey Brin, James
Davis, and Hector Garcia-Molina, in 1995 \cite{brin95}. Copyright infringement
detection is a very similar problem to academic dishonesty detection, but there
is much more research explicitly focused on academic dishonesty than on
copyright infringement.


\subsection{Algorithms}
\label{sec:algo}
Previous research into similarity detection largely falls into two categories:
feature comparisons and structural comparisons. Feature comparison algorithms 
build a profile of various attributes of the input documents (for example, 
number of distinct tokens, overall word count, average number of characters 
per line) and compares the profiles of submissions to determine if they are 
unusually similar. Structural comparisons compare the content of submissions 
--- for example, comparing the specific tokens that form the inputs
\cite{arwin2006plagiarism}. Within structural comparisons, there are two
broad subcategories: Vector-Distance algorithms and Fingerprinting algorithms.
% TODO Lancaster05 good citation for here


\subsubsection{Syntax Awareness in Comparison}
Similarity detection on source code offers a number of advantages over working
with natural language. Every programming language can be tokenized and parsed
according to a grammar defined by the language. Through this grammar, we can
identify the purpose of all input tokens --- variable name, language keyword,
function name, etc. This permits the use of powerful normalization techniques
that are not available when dealing with natural languages.

The abstract syntax tree representation of a program highlights similarities
that may be harder to spot in plaintext. By parsing input submissions
into such syntax trees, similarity detection can be performed in a less
ambiguous manner. As compilers and interpreters normally perform this task
when preparing to compile or execute code, performing this parsing is not an
undue burden when tokenizing submissions. By parsing input submissions into an
abstract syntax tree (as a compiler or interpreter normally would do to
compile or execute the code), similarity detection can be performed on a
representation of the program where similarities that might be ambiguous in
plaintext become clear. For example, the operations $1 + 2$ and $2 + 1$ are
identical in purpose, but plaintext comparison would typically not be able to
identify their similarity beyond the shared $+$ character. However, parsing
into a syntax tree would create identical $+$ nodes with children $1$ and $2$
for both, identifying that they are identical (as additional is commutative).
Furthermore, it becomes possible to apply a consistent normalization scheme to
things like identifiers. By renaming identifiers in a consistent manner, it
becomes possible to remove the effectiveness of some approaches to obfuscating
similarities (namely, renaming functions and variables)
\cite{arwin2006plagiarism}.

The use of parsing and related normalizations can be a powerful tool to detect
obfuscated similarities. All 13 of the obfuscation techniques discussed in
Section~\ref{sec:obfuscation} can be identified and defeated by using
normalized abstract syntax trees produced by a parser
\cite{arwin2006plagiarism}. The obvious disadvantage of this approach is that
the parsing phase is language-sensitive. This limits a similarity detection
system to a few languages and imposes a significant burden to support
additional languages. In addition, language-specific parsing entails
information loss; some features that may be used indicate similarity, such as
comments or identifier misspellings, are often lost during parsing and by
similar normalizations. Despite its drawbacks, almost every similarity
detection system targeted at source code uses a syntax-aware parsing phase
to catch similarities that might otherwise be undetectable 
\cite{whale1990identification} \cite{arwin2006plagiarism} \cite{lancaster05}.


\subsubsection{Vector-Distance Algorithms}
Vector-distance algorithms compare two or more inputs and identify the
number and sequence of edits that must be made to transform one of the inputs
into the other(s). The complement of this sequence of edits is the set of
things that did not change --- the similarities between the two documents.
Vector-Distance algorithms are typically the slowest of all  similarity
detection algorithms, but fingerprinting and feature comparison may not
identify the best possible match.


\paragraph{Smith-Waterman Algorithm}
\label{sec:smithwatermanlit}
The \textit{Smith-Waterman} algorithm was developed by Temple Smith and
Michael Waterman in 1981 for the comparison of DNA sequences
\cite{smith1981identification}. It is a dynamic programming algorithm that
seeks to find the optimal alignment of two strings. Unlike algorithms that
solve the traditional longest common substring problem, the
\textit{Smith-Waterman} algorithm is tolerant of skipped or unmatched
characters. Figure~\ref{fig:stringalignment} shows a sample alignment of two
strings, ``ABCDEFG'' and ``ABCDXG''. The longest common substring of the two
would be ``ABCD'', but a local alignment also captures the matched ``G''
character.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{smithwaterman.pdf}
\caption{A local string alignment of the type generated by the \textit{Smith-Waterman Algorithm}}
\label{fig:stringalignment}
\end{figure}

The algorithm, like most vector-distance approaches, compares pairs of
inputs. Each input is placed on an axis of a 2-dimensional matrix, one
token to a row or column, as is shown in
Figure~\ref{fig:smithwatermanarray}. The array is initialized to $0$, and is
then filled top to bottom, left to right. During filling, each cell is
initially filled with the largest value of its predecessors (directly-adjacent
cells to the left, above, and to the upper-left of the cell). If the
characters on the X and Y axis match, the cell is incremented by a fixed value;
if the characters do not match, the cell is decremented (unless it is $0$, in
which case no action is taken). The largest value in the array represents the
best alignment of the two inputs \cite{smith1981identification}. Due to the
need to hold and then fill this array, Smith-Waterman is an $O(n * m)$
algorithm in time and memory, where $n$ and $m$ are the length of the two
inputs.

\begin{figure}
\centering
\includegraphics{smithwatermanarray.png}
\caption{The array built by the \textit{Smith-Waterman Algorithm}}
\label{fig:smithwatermanarray}
\end{figure}

In 2004, Robert Irving adapted the Smith-Waterman algorithm for program
similarity detection \cite{irving04}. By repeatedly applying the algorithm to
a pair of inputs, removing the detected overlay afterwards, Irving was able to
identify all the local alignments of the two inputs over a given threshold. He
presents a set of optimizations to the original Smith-Waterman algorithm that
improve its performance when applied repeatedly to the same inputs by
removing the need for recomputation of the unchanged parts of the array.


\subsubsection{Fingerprinting Algorithms}
Fingerprinting algorithms (also known as \textit{feature extraction
algorithms}) extract a number of \textit{fingerprints} (or 
	\textit{features}) which can be used to identify a document. These
\textit{fingerprints} are then added to a database (with a list of references
		back to the inputs that contained them). To identify similarities, an input's
\textit{fingerprints} are computed, added to the database, and then looked up
to see whether any submissions other than the current one have the same
\textit{fingerprints} \cite{hoad03}.

\textit{Fingerprints} are typically the hashes of one or more tokens of the
input. Typically, before being inserted into the database, a small subset of
the tokens is selected, and the rest are discarded; this is done to reduce the
number of entries required in the database. A typical solution might be to
discard all hashes except those which are congruent to $0$ modulo a certain
number $p$, but better fingerprint selectors exist \cite{schleimer03}.


\paragraph{Karp-Rabin String Matching}
The archetypal fingerprinting algorithm is \textit{Karp-Rabin} string matching.
\textit{Karp-Rabin} generates ``rolling hashes'' of the two input submissions
(the hash of characters $N$ to $M$ of the input, then the hash of characters 
 $N + 1$ to $M + 1$, and so on). The sets of hashes for the inputs are checked
against each other, and the matches are used to identify common substrings.
\cite{karp87}. \textit{Karp-Rabin} string matching is a fundamental technique
used and built on by many later string-matching papers \cite{schleimer03}
\cite{buss94} \cite{arwin2006plagiarism} \cite{burrows07} \cite{clough03}
\cite{johnson94}.

\paragraph{$n$-gram Fingerprinting}
\label{sec:ngram}
By far the most common document fingerprinting approach is called 
\textit{n-gram fingerprinting}. The document is tokenized and considered as
sequences of $n$ tokens, or \textit{n-grams}. The \textit{n-grams} overlap, so
that there an $m$ character document is considered as $m - n + 1$ 
\textit{n-grams}. The hashes of these \textit{n-grams} are stored in a
database and checked against those from other documents. The hashes are
usually stored with pointers to their occurrences to aid in detection.

TODO ILLUSTRATION

Most corpora are far too large to store the the hashes of every \textit{n-gram}
from every document in the database. Usually, a small subset of all hashes are
selected through an algorithm; these serve as the document's
\textit{fingerprints}. This does entail some information loss; even
word-for-word identical documents only match if they both contain an
\textit{n-gram} that was selected as a fingerprint. The fingerprint selection
algorithm is consequently very important. A very simple approach is to select
\textit{n-grams} whose hashes are divisible by some number $p$
\cite{schleimer03}, but this technique can leave gaps of arbitrary length
between fingerprints, so that a large chunk of similar text might slip by.

Schleimer, Wilkerson, and Aiken presented an alternative fingerprint selection
algorithm in 2003 called \textit{winnowing}. After dividing the document's
tokens into \textit{n-grams}, winnowing further divides the \textit{n-grams}
into \textit{windows} of size $t$. Winnowing always selects exactly one hash
from each window, guaranteeing that a match of $t$ or more consecutive tokens
is identified.

TODO ILLUSTRATION

Winnowing has proven to be extremely powerful; it is used by the current
industry standard source code similarity detector, \textit{Measure of Software
Similarity}, or MOSS, described in Section~\ref{sec:moss}.


\subsubsection{Feature Comparison}
Feature Comparison, also known as \textit{Attribute Counting}, attempts to
compare features of inputs not related to their structure (comparing the actual
tokens that form the inputs). These comparisons are typically based on
\textit{profiles} of the input, representing a composite of a number of
defining features --- for example, line count, word count, character count,
average word per line \cite{arwin2006plagiarism}.

Feature comparison algorithms were common in the early days of similarity
detection in the 1980s and early 1990s \cite{clough03}. However, it has become
less popular of late because of the growing effectiveness of structural
comparison algorithms such as \textit{Smith-Waterman} and fingerprinting.
Feature comparison algorithms can be just as accurate as structural 
comparison algorithms, but require a great deal of tuning as the differences 
in the profiles of documents containing unauthorized copying and those that 
do not are typically very small \cite{arwin2006plagiarism}. We found one 
reference to the creation of a similarity detection system using this 
algorithm after 2000, but no others \cite{jones01}. The paper describing this 
system provided no evidence that the aforementioned criticisms were not valid,
and did not provide any results to substantiate its effectiveness. Given 
the lack of popularity of this approach and its noted disadvantages, we did not 
pursue this line of inquiry.
% Could cite lancaster05


\iffalse
\subsubsection{Other Approaches}

Burrows, Tahaghoghi, and Zobel have developed a highly scalable approach to
similarity detection using advanced information retrieval techniques, which can
handle checking for similarity among tens of thousands of source code documents
\cite{burrows07}. The Burrows approach is sensitive to an accurate selection of
a similarity function; a number of functions have been developed, including the
Okapi BM25 function and a family of functions developed by Ciesielski, Nelson,
and Tahaghogi \cite{ciesielski08}.

Belkhouche, Nix, and Hassell have contributed an elaborate program that
compares C programs by converting them to structures representing control
flow, data tables, and other high-level concepts. Their implementation does not
compare implementations written in other languages \cite{belkhouche04}.
% Reference lancaster05
\fi


\subsection{Existing Solutions}
\label{sec:existing}
A number of programs exist for finding similarities between source code. Some
of these, like MOSS, are intended for use in detecting academic dishonesty; but
very few are publicly available. many solutions mentioned in literature were
never released, or have not been maintained for many years. The two noteworthy
products, MOSS and \textit{JPlag}, are described below.


\subsubsection{MOSS}
\label{sec:moss}
MOSS, an acronym for \textit{Measure of Software Similarity}, is a solution
developed by Professor Alex Aiken of Stanford in 1994 \cite{bowyer99}. Today,
two decades later, it is considered the gold standard in software similarity
detection \cite{beth14}. MOSS is free for non-commercial use (though it was 
previously restricted only for use in academia). It is an online service, not
a software tool that can be deployed. Its primary interface is a Perl script
that provides a command-line frontend to submit code for analysis. Results
are provided via email, and can take several hours to arrive \cite{bowyer99}.
MOSS is based on the $n$-gram Fingerprinting algorithm with winnowing
described in Section~\ref{sec:ngram}. Most of the details about the
implementation are public, but the tuning parameters of the algorithm are kept
private. 

Despite having used the same algorithm for over a 
decade, MOSS remains highly competitive. In a battery of tests run in 2014
using several similarity detection algorithms, MOSS posts detection results
comparable every other algorithm tested \cite{beth14}. In fact, when new
similarity detection systems are published, they often compare their results
with those of MOSS. 

All academic dishonesty detectors suffer from the diversity of definitions of
unauthorized copying, and MOSS is no exception. MOSS often flags behavior which
our advisor, our primary customer, does not consider to be unauthorized
copying. The secrecy of MOSS's tuning parameters and database also prevent
researchers from independently evaluating its performance or reproducing any of
its results.
% TODO Cite aiken05 - NEED TO FIND PAPER


\subsubsection{JPlag}
TODO


\subsubsection{Other Programs}
TODO
% Cite all the one-off systems that are mentioned in our lit review
% And then never appear again
