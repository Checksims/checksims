\section{Definitions}
Several terms are fundamental to the field of similarity detection which are
not in common use. We provide definitions for them here, to ensure that readers
of this report have an understanding of essential terms.

\begin{itemize}
\item The terms \textit{academic dishonesty} and \textit{unauthorized copying}
	are used throughout this paper to refer to the use of another's work
	without permission (in this context, typically by students). This is often
	referred to as ``cheating'' or ``plagiarism'' in general use, but we
	deliberately avoid these terms where possible, as they can be perceived to
	be controversial or judgmental.
\item A \textit{corpus} is a body of work. A corpus may contain work from many
	people. A typical corpus in the world of academic dishonesty detection
	might be all the student submissions for a specific assignment
\item Similarities between works contained within a single corpus are referred
	to as \textit{intra-corpal similarities}. Similarities between a work in a
	corpus and works not contained within a corpus are referred to as
	\textit{extra-corpal similarities}.
\end{itemize}


\section{Literature Review}
TODO TODO TODO TODO TODO TODO TODO

\subsection{Academic Dishonesty}
\label{sec:dishonesty}
Before proceeding with a review of techniques for detecting academic
dishonesty, it is important to have a definition of what academic dishonesty
is. There have been a number of scholarly attempts to provide a definition for
the term (and the associated term ``plagiarism'', which we will avoid). Thomas
Lancaster, in a 2005 survey of similarity detection systems, defines academic
dishonesty as ``The process by which students submit work for academic credit
which contains other people's  unacknowledged words or ideas''
\cite{lancaster05}. This provides a solid foundation, but there is still a
question of what specific acts constitute academic dishonesty.

The question of academic dishonesty has been further complicated by the
emergence of code-hosting websites such as Github and Bitbucket, and Q\&A
sites such as Stack Overflow. Students may host code they wrote for a class on
Github, where it may be copied without permission by their classmates (or
students in another class who were given the same assignment). Almost all would
agree that the student who copied the given code without permission was guilty
of unauthorized copying, but some argue that the student who originally hosted
the code is guilty of academic dishonesty for enabling copying to take place
\cite{cosma06}. In the case of Stack Overflow, students may ask questions on
the appropriate course of action to take to complete an assignment. The answers
they receive might include example source code, and most would agree that
copying this example code constitutes academic dishonesty. However, some
professors feel that the use of any information (even hints as to algorithms,
or pseudocode versions of a solution) obtained from such a source constitutes
academic dishonesty. Every instructor will likely have a different definition
of which of these offenses is a case of academic dishonesty, complicating the
creation of tools to assist in detecting them.

A 2006 survey of professors in the UK on academic dishonesty produced a large
body of results for what professors perceive is (and is not) unauthorized
copying \cite{cosma06}. The sharing of source code, comments, overall design,
documentation, and user interface were all near-universally perceived to be
unauthorized copying amongst respondents to the survey. Specifically for source
code, use of code from other sources without acknowledgment (even if the source
code was adapted to the student's specific application, or rewritten from
another language) wass again near-universally considered to be academic
dishonesty. Here, however, many respondents noted that the degree of adaptation
was an important factor, noting that while 100\% similarity was almost
certainly indicate of unauthorized copying, there was a point at which the
changes to the code would cause it to no longer be academic dishonesty (though
what this point was varied from respondent to respondent). A majority of
respondents indicated that most every offense involving the unauthorized
duplication of source code could, indeed, be considered academic dishonesty
given the right circumstances, save cases where the code copied was written by
the submitting student. In such cases (for example, the submission of an
assignment written previously for a different class, with slight
modifications), most respondents answered that it was a violation of course
policy, but not a matter of academic dishonesty.
% TODO should cite joy1999


\subsubsection{Our Working Definition}
This project was not developed to use any of the definitions listed in
\ref{sec:dishonesty}. Rather, it was created to serve the needs of one specific
professor, and his definition of academic dishonesty. Professor Lauer provided
the authors with his definition of what constitutes academic dishonesty at the
start of this project, and it is the one which was used during our development
and writing. This section provides this working definition of academic
dishonesty and unauthorized copying.

Professor Lauer considers the collaboration of students on a whiteboard to be
perfectly acceptable during assignments. Students can collaboratively develop
pseudocode-level descriptions of the appropriate solution to an assignment, and
then translate this pseudocode into a working program independently. The direct
copying of source code is, however, prohibited - each student must write their
program independently, though again they may reference a shared pseudocode
solution. In Professor Lauer's view, this ensures that the code must ``pass
through the brain'' - the students will gain more understanding by typing
independently than by copying code directly. Thus, the direct copying of code
from other sources is considered by Professor Lauer to be academic dishonesty,
while solutions being identical at the pseudocode level is not considered to be
problematic.

Using source code from the web follows much the same rules as collaborating
with other students. The use of preexisting pseudocode (or using existing code
as pseudocode) is considered acceptable, so long as the student types their own
solution, and do not copy wholesale a solution from the internet.


\subsection{Applications}
TODO
% Filler / intro paragraph to why people care about this crap

\subsubsection{Code Clone Detection}
TODO
% Cite hordijk08
% cite baker95

\subsubsection{Bioinformatics}
The comparison of similar sequences is done often in Bioinformatics. Strands of
DNA, RNA, or proteins are often sequenced and compared for a variety of
reasons. A well-known example of this is the use of DNA testing in the criminal
justice system, where DNA comparisons are used to identify the perpetrators of
crimes from trace evidence. A number of commonly-used similarity detection
algorithms were originally developed for bioinformatics use --- for example,
the Smith-Waterman algorithm \cite{smith1981identification}.
% Could use a general citation for similarity checking in bioinformatics


\subsubsection{Malware Analysis}
TODO
% Need cites

\subsubsection{Detection of Academic Dishonesty}
The detection of academic dishonesty is a prominent application of similarity
detection algorithms, and the one we are most interested in. There has been a
great deal of research focused on the detection of academic dishonesty (both in
plaintext and source code); Thomas Lancaster presents a thorough overview of
existing work in a 2005 paper, including a comparison of all existing academic
dishonesty detection solutions available at time of publication
\cite{lancaster05}. The detection of unauthorized copying draws heavily on
the fields of Natural Language Processing and Information Retrieval, to process
student submissions and efficiently store and retrieve information about
similarities \cite{beth14}.

TODO REST
% Cite lancaster05


\paragraph{Obfuscation}
\label{sec:obfuscation}
In the case of both academic dishonesty and malware, the authors who produced
the submissions being compared have an interest in preventing the detection
of any similarity that exists. Consequently, they will often take steps to
obfuscate code in an attempt to remove similarities or prevent them from
being detected. Programs and algorithms which attempt to detect malware or
academic dishonesty must be resistant to common obfuscation techniques to be
successful.

A list of 13 methods of defeating similarity detection is given by Geoffrey
Whale in a 1990 paper on program similarity, and reproduced below
\cite{whale1990identification}. This list is widely cited, and generally
considered a definitive enumeration of methods of disguising unauthorized
copying.
\begin{enumerate}
\item Changing comments or formatting (for example, adding whitespace)
\item Changing identifiers
\item Changing the order of operands in expressions (for example, $1 + 2$
	into $2 + 1$)
\item Changing data types (substituting floats for integers, or exploding
	structures into separate variables)
\item Replacing expressions with semantically identical equivalents (for
	example, \texttt{!x} with \texttt{x == false})
\item Adding redundant statements or variables
\item Changing the order of independent statements
\item Changing the structure of iteration statements
\item Changing the structure of conditional statements
\item Replacing procedure calls with the procedure body
\item Introducing non-structured statements
\item Combining original and copied program fragments
\end{enumerate}
% TODO could use another para here to smooth the transition?


\subsection{Algorithms}
Previous research into similarity detection has produced a number of algorithms
suitable for use in identifying similarities in source code. Our research
determined that there are general classes into which these algorithms fall:
Vector-Distance algorithms, Fingerprinting algorithms, and Document
Fingerprinting (similar to, but distinct from, general fingerprinting
algorithms).
% Lancaster05 good citation for here
% Cite arwin2006plagiarism

TODO TRANSITION


\subsubsection{Syntax Awareness in Comparison}
Similarity detection on source code offers a number of options that are not
present when working with plaintext. Every programming language can be
tokenized and parsed according to a grammar defined by the language. Through
this grammar, we can identify the purpose of all input tokens --- variable
name, language keyword, function name, etc. This permits the use of powerful
normalization techniques which would not normally be available.

By parsing input submissions into an abstract syntax tree (as a compiler or
interpreter normally would do to compile or execute the code), similarity
detection can be performed on a representation of the program where
similarities which might be ambiguous in plaintext become clear. For example,
the operations $1 + 2$ and $2 + 1$ are identical in purpose, but plaintext
comparison would typically not be able to identify their similarity beyond the
shared $+$ character. However, parsing into a syntax tree would create a $+$
node with the children $1$ and $2$ for both inputs, identifying that they are
functionally identical (and should be considered similar). Furthermore, it
becomes possible to apply a consistent normalization scheme to things like
identifiers. By renaming identifiers in a consistent manner, it becomes
possible to remove the effectiveness of some approaches to obfuscating
similarities (namely, renaming functions and variables)
\cite{arwin2006plagiarism}.

The use of parsing and related normalizations is a powerful tool to detect
obfuscated similarities. All 13 of the obfuscation techniques discussed in
Section~\ref{sec:obfuscation} can be identified and defeated by using
normalized abstract syntax trees produced by a parser
\cite{arwin2006plagiarism}. The obvious disadvantage of this approach is that
the parsing phase is language-sensitive. This introduces a limit on the amount
of languages a similarity detection system can process, and requires a
potentially large amount of changes to add support for new languages. Despite
this, almost every similarity detection system targeted at source code will
use a syntax-aware parsing phase to catch similarities which might otherwise
be undetectable \cite{whale1990identification} \cite{arwin2006plagiarism}
\cite{lancaster05}.


\subsubsection{Vector-Distance Algorithms}
TODO
% Can probably cite lancaster05?
% Need more cites

\paragraph{Smith-Waterman Algorithm}
\label{sec:smithwatermanlit}
The \textit{Smith-Waterman} algorithm was developed by Temple Smith and
Michael Waterman in 1981 for the comparison of DNA sequences
\cite{smith1981identification}. It is a dynamic programming algorithm that
seeks to find the optimal alignment of two strings. The string alignment
problem is similar to the longest common substring problem, in that both
attempt to find common sequences of ordered characters in input strings.
However, the longest common substring problem seeks to find identical sequences
of characters, whereas the Smith-Waterman algorithm is tolerant of skipped or
unmatched characters. Figure~\ref{fig:stringalignment} shows a sample alignment
of two strings. The longest common substring of the two would be ``ABCD'', but
a local alignment also captures the matched ``G'' character.

\begin{figure}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{smithwaterman.pdf}
\caption{A local string alignment of the type generated by the \textit{Smith-Waterman Algorithm}}
\label{fig:stringalignment}
\end{figure}

The algorithm, like most vector-distance approaches, compares pairs of
inputs. Each input is placed on an axis of a 2-dimensional matrix, one
character to a row or column, as is shown in
Figure~\ref{fig:smithwatermanarray}. The array is initially filled with $0$,
and is then filled. During filling, each cell is initially filled with the
largest value of its predecessors (directly-adjacent cells to the left, above,
and to the upper-left of the cell). If the characters on the X and Y axis
match, the cell is incremented by a value; if the characters do not match, the
cell is decremented (unless it is 0, in which case no action is taken). The
largest value in the array represents the best alignment of the two inputs
\cite{smith1981identification}. Due to the need to hold and then fill this
array, Smith-Waterman is an $O(n * m)$ algorithm in time and memory, where $n$
and $m$ are the length of the two inputs

\begin{figure}
\includegraphics{smithwatermanarray.png}
\caption{The array built by the \textit{Smith-Waterman Algorithm}}
\label{fig:smithwatermanarray}
\end{figure}

The Smith-Waterman algorithm was modified and applied to software similarity
detection (with the end goal of detection academic dishonesty) by Robert Irving
in a 2004 publication \cite{irving04}. By repeatedly applying the algorithm to
a pair of inputs, removing the detected overlay afterwards, Irving was able to
identify all the local alignments of the two inputs over a given threshold. He
presents a set of optimizations to the original Smith-Waterman algorithm which
improve its performance when applied repeatedly to the same inputs by
removing the need for recomputation of the unchanged parts of the array.


\subsubsection{Fingerprinting Algorithms}
TODO
% Should include Karp-Rabin here - citation karp87

\paragraph{n-Gram Fingerprinting}
\label{sec:ngram}
By far the most common document fingerprinting approach is called n-gram
fingerprinting. The document is tokenized, and considered in sequences of $n$
tokens, or \textit{n-grams}. The n-grams overlap, so that there an $m$
character document will be considered as $m - n + 1$ n-grams. The hashes of 
these  n-grams are stored in a database and checked against those from other 
documents. The hashes are usually stored with a pointer to their occurrence to
aid in detection.

TODO ILLUSTRATION

Most plagiarism corpora are far too large to store all n-grams from all
documents. Usually, a selection algorithm is used to select a subset of the
n-grams to serve as the document's \textit{fingerprints}, but that entails
information loss: even word-for-word identical documents will only match if
they both contain an n-gram that was selected as a fingerprint, so how the
fingerprints are selected is very important. A very simple approach is to
select n-grams whose hashes are divisible by some number $p$
\cite{schleimer03}, but this technique can leave gaps of arbitrary length
between fingerprints, so that a large chunk of similar text might slip by.

Schleimer, Wilkerson, and Aiken presented an alternative fingerprint selection
algorithm in 2003 called \textit{winnowing}. After dividing the document's
tokens into n-grams, winnowing further divides the n-grams into 
\textit{windows} of size $t$. Winnowing always selects the minimum hash from
each window, guaranteeing that a match of $t$ or more consecutive tokens will
be identified.

TODO ILLUSTRATION

Winnowing has proven to be extremely powerful; it is used by the current
industry standard source code similarity detector, MOSS, where the authors
claim that ``false positives appear to be non-existent, and the infrequent
reports of false negatives have always been tracked back either to
implementation bugs or user error'' \cite{schleimer03}. We doubt that MOSS is
quite as effective as this quote would indicate, but there can be no doubt that
it is a very successful system (indeed, it is considered the state of the art
as of 2014 \cite{beth14}).


\subsubsection{Document Fingerprinting}
TODO
% Reference is jones01
% Cite lancaster05


\subsubsection{Other Approaches}
% Reference ciesielski08
% Reference belkhouche04
% Reference lancaster05


\section{Existing Solutions}
TODO

\subsection{MOSS}
\label{sec:moss}
TODO
% Cite schleimer03
% Cite beth14
% Cite bowyer99

\subsection{JPlag}
TODO

\subsection{Other Programs}
TODO
% Cite all the one-off systems that are mentioned in our lit review
% And then never appear again
