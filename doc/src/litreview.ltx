\section{Literature Review}
TODO TODO TODO TODO TODO TODO TODO

\subsection{Definitions}
TODO
% Corpus - collection of submitted work


\subsection{Academic Dishonesty}
TODO
% includes both intra-corpal similarity (similarities within same assignment)
% and similarities outside of the corpus (previous year's assignments, Github)
% Cite joy1999
% Cite cosma06
% Cite lancaster05

\subsubsection{Our Working Definition}
TODO
% No cites just give Lauer's def

\subsection{Applications}
TODO
% Filler/intro paragraph to why people care about this crap

\subsubsection{Code Clone Detection}
TODO
% Cite hordijk08
% cite baker95

\subsubsection{Bioinformatics}
The comparison of similar sequences is done often in Bioinformatics. Strands of
DNA, RNA, or proteins are often sequenced and compared for a variety of
reasons. A well-known example of this is the use of DNA testing in the criminal
justice system, where DNA comparisons are used to identify the perpetrators of
crimes from trace evidence. A number of commonly-used similarity detection
algorithms were originally developed for bioinformatics use --- for example,
the Smith-Waterman algorithm \cite{smith1981identification}.
% Could use a general citation for similarity checking in bioinformatics


\subsubsection{Malware Analysis}
TODO
% Need cites

\subsubsection{Detection of Academic Dishonesty}
The detection of academic dishonesty is a prominent application of similarity
detection algorithms, and the one we are most interested in. There has been a
great deal of research focused on the detection of academic dishonesty (both in
plaintext and source code); Thomas Lancaster presents a thorough overview of
existing work in a 2005 paper, including a comparison of all existing academic
dishonesty detection solutions available at time of publication
\cite{lancaster05}. The detection of unauthorized copying draws heavily on
the fields of Natural Language Processing and Information Retrieval, to process
student submissions and efficiently store and retrieve information about
similarities \cite{beth14}.

TODO REST
% Cite lancaster05


\paragraph{Obfuscation}
\label{sec:obfuscation}
In the case of both academic dishonesty and malware, the authors who produced
the submissions being compared have an interest in preventing the detection
of any similarity that exists. Consequently, they will often take steps to
obfuscate code in an attempt to remove similarities or prevent them from
being detected. Programs and algorithms which attempt to detect malware or
academic dishonesty must be resistant to common obfuscation techniques to be
successful.

A list of 13 methods of defeating similarity detection is given by Geoffrey
Whale in a 1990 paper on program similarity, and reproduced below
\cite{whale1990identification}. This list is widely cited, and generally
considered a definitive enumeration of methods of disguising unauthorized
copying.
\begin{enumerate}
\item Changing comments or formatting (for example, adding whitespace)
\item Changing identifiers
\item Changing the order of operands in expressions (for example, $1 + 2$
	into $2 + 1$)
\item Changing data types (substituting floats for integers, or exploding
	structures into separate variables)
\item Replacing expressions with semantically identical equivalents (for
	example, \texttt{!x} with \texttt{x == false})
\item Adding redundant statements or variables
\item Changing the order of independent statements
\item Changing the structure of iteration statements
\item Changing the structure of conditional statements
\item Replacing procedure calls with the procedure body
\item Introducing non-structured statements
\item Combining original and copied program fragments
\end{enumerate}


\subsection{Algorithms}
Previous research into similarity detection has produced a number of algorithms
suitable for use in identifying similarities in source code. Our research
determined that there are general classes into which these algorithms fall:
Vector-Distance algorithms, Fingerprinting algorithms, and Document
Fingerprinting (similar to, but distinct from, general fingerprinting
algorithms).
% Lancaster05 good citation for here
% Cite arwin2006plagiarism

TODO TRANSITION


\subsubsection{Syntax Awareness in Comparison}
Similarity detection on source code offers a number of options that are not
present when working with plaintext. Every programming language can be
tokenized and parsed according to a grammar defined by the language. Through
this grammar, we can identify the purpose of all input tokens --- variable
name, language keyword, function name, etc. This permits the use of powerful
normalization techniques which would not normally be available.

By parsing input submissions into an abstract syntax tree (as a compiler or
interpreter normally would do to compile or execute the code), similarity
detection can be performed on a representation of the program where
similarities which might be ambiguous in plaintext become clear. For example,
the operations $1 + 2$ and $2 + 1$ are identical in purpose, but plaintext
comparison would typically not be able to identify their similarity beyond the
shared $+$ character. However, parsing into a syntax tree would create a $+$
node with the children $1$ and $2$ for both inputs, identifying that they are
functionally identical (and should be considered similar). Furthermore, it
becomes possible to apply a consistent normalization scheme to things like
identifiers. By renaming identifiers in a consistent manner, it becomes
possible to remove the effectiveness of some approaches to obfuscating
similarities (namely, renaming functions and variables)
\cite{arwin2006plagiarism}.

The use of parsing and related normalizations is a powerful tool to detect
obfuscated similarities. All 13 of the obfuscation techniques discussed in
Section~\ref{sec:obfuscation} can be identified and defeated by using
normalized abstract syntax trees produced by a parser
\cite{arwin2006plagiarism}. The obvious disadvantage of this approach is that
the parsing phase is language-sensitive. This introduces a limit on the amount
of languages a similarity detection system can process, and requires a
potentially large amount of changes to add support for new languages. Despite
this, almost every similarity detection system targeted at source code will
use a syntax-aware parsing phase to catch similarities which might otherwise
be undetectable \cite{whale1990identification} \cite{arwin2006plagiarism}
\cite{lancaster05}.


\subsubsection{Vector-Distance Algorithms}
TODO
% Can probably cite lancaster05?
% Need more cites

\paragraph{Smith-Waterman Algorithm}
\label{sec:smithwatermanlit}
The \textit{Smith-Waterman} algorithm was developed by Temple Smith and
Michael Waterman in 1981 for the comparison of DNA sequences
\cite{smith1981identification}. It is a dynamic programming algorithm that
seeks to find the optimal alignment of two strings. The string alignment
problem is similar to the longest common substring problem, in that both
attempt to find common sequences of ordered characters in input strings.
However, the longest common substring problem seeks to find identical sequences
of characters, whereas the Smith-Waterman algorithm is tolerant of skipped or
unmatched characters. TODO EXPLANATION OF STRING ALIGNMENT DIAGRAM.

INSERT STRING ALIGNMENT DIAGRAM FROM POWERPOINT HERE

The algorithm, like most vector-distance approaches, compares pairs of
inputs. Each input is placed on an axis of a 2-dimensional matrix, one
character to a row or column, as is shown in
Figure~\ref{fig:smithwatermanarray}. The array is initially filled with $0$,
and is then filled. During filling, each cell is initially filled with the
largest value of its predecessors (directly-adjacent cells to the left, above,
and to the upper-left of the cell). If the characters on the X and Y axis
match, the cell is incremented by a value; if the characters do not match, the
cell is decremented (unless it is 0, in which case no action is taken). The
largest value in the array represents the best alignment of the two inputs
\cite{smith1981identification}. Due to the need to hold and then fill this
array, Smith-Waterman is an $O(n * m)$ algorithm in time and memory, where $n$
and $m$ are the length of the two inputs

\begin{figure}
\includegraphics{smithwatermanarray.png}
\caption{The array built by the \textit{Smith-Waterman Algorithm}}
\label{fig:smithwatermanarray}
\end{figure}

The Smith-Waterman algorithm was modified and applied to software similarity
detection (with the end goal of detection academic dishonesty) by Robert Irving
in a 2004 publication \cite{irving04}. By repeatedly applying the algorithm to
a pair of inputs, removing the detected overlay afterwards, Irving was able to
identify all the local alignments of the two inputs over a given threshold. He
presents a set of optimizations to the original Smith-Waterman algorithm which
improve its performance when applied repeatedly to the same inputs by
removing the need for recomputation of the unchanged parts of the array.


\subsubsection{Fingerprinting Algorithms}
TODO
% Should include Karp-Rabin here - citation karp87

\paragraph{n-Gram Fingerprinting}
\label{sec:ngram}
By far the most common document fingerprinting approach is called n-gram
fingerprinting. The document is tokenized, and considered in sequences of $n$
tokens, or \textit{n-grams}. The n-grams overlap, so that there an $m$
character document will be considered as $m - n + 1$ n-grams. The hashes of 
these  n-grams are stored in a database and checked against those from other 
documents. The hashes are usually stored with a pointer to their occurrence to
aid in detection.

TODO ILLUSTRATION

Most plagiarism corpora are far too large to store all n-grams from all
documents. Usually, a selection algorithm is used to select a subset of the
n-grams to serve as the document's \textit{fingerprints}, but that entails
information loss: even word-for-word identical documents will only match if
they both contain an n-gram that was selected as a fingerprint, so how the
fingerprints are selected is very important. A very simple approach is to
select n-grams whose hashes are divisible by some number $p$
\cite{schleimer03}, but this technique can leave gaps of arbitrary length
between fingerprints, so that a large chunk of similar text might slip by.

Schleimer, Wilkerson, and Aiken presented an alternative fingerprint selection
algorithm in 2003 called \textit{winnowing}. After dividing the document's
tokens into n-grams, winnowing further divides the n-grams into 
\textit{windows} of size $t$. Winnowing always selects the minimum hash from
each window, guaranteeing that a match of $t$ or more consecutive tokens will
be identified.

TODO ILLUSTRATION

Winnowing has proven to be extremely powerful; it is used by the current
industry standard source code similarity detector, MOSS, where the authors
claim that ``false positives appear to be non-existent, and the infrequent
reports of false negatives have always been tracked back either to
implementation bugs or user error'' \cite{schleimer03}. We doubt that MOSS is
quite as effective as this quote would indicate, but there can be no doubt that
it is a very successful system (indeed, it is considered the state of the art
as of 2014 \cite{beth14}).


\subsubsection{Document Fingerprinting}
TODO
% Reference is jones01
% Cite lancaster05


\subsubsection{Other Approaches}
% Reference ciesielski08
% Reference belkhouche04
% Reference lancaster05


\section{Existing Solutions}
TODO

\subsection{MOSS}
TODO
% Cite schleimer03
% Cite beth14
% Cite bowyer99

\subsection{JPlag}
TODO

\subsection{Other Programs}
TODO
% Cite all the one-off systems that are mentioned in our lit review
% And then never appear again
