\section{Definitions}
Several fundamental similarity detection terms are not in common use. For
clarity, we provide their definitions here.

\begin{itemize}
\item The terms \textit{academic dishonesty} and \textit{unauthorized copying}
	are used throughout this paper to refer to the use of another's work
	without attribution and without the permission of the instructor. Such
        behavior is often referred to as ``cheating'' or ``plagiarism,'' but we
        deliberately avoid those terms where possible, as they can be perceived
        to be controversial or judgmental.
\item A \textit{corpus} (plural \textit{corpora}) is a body of work. A corpus
        may contain many documents written by many people. An example of a
        corpus in the world of academic dishonesty detection might be all the
        student	submissions for a specific assignment.
\iffalse
% Commented out as these terms are never used
\item Similarities between works contained within a single corpus are referred
	to as \textit{intra-corpal similarities}. Similarities between a work in a
	corpus and works not contained within a corpus are referred to as
	\textit{extra-corpal similarities}.
\fi
\item A \textit{Token} is a piece of a larger input. Tokens are usually
	generated by an algorithm called a \textit{Tokenizer}, which breaks an
	input string up in a consistent manner (for example, at each newline).
	\textit{Tokens} are formed from the broken-up chunks of the input.
\item A \textit{False Positive} is a result that is reported, but
        should not have been. In the context of a similarity detection system,
        it would be two or more submissions that are flagged as unusually
        similar, but are not considered to be illegally copied.
\item A \textit{False Negative} is a result that is not reported by the tool,
        but should have been. In the context of a similarity detection system,
        it would be two results that are not flagged as unusually similar in
        spite of having been created by unauthorized copying.
\end{itemize}


\section{Literature Review}
This section summarizes our review of existing literature in the area of
similarity detection. It is focused on three areas: what is academic
dishonesty (Section~\ref{sec:dishonesty}), what algorithms exist for detecting
academic dishonesty (Section~\ref{sec:algo}), and what preexisting solutions
might also solve the specific problems that inspired our solution
(Section~\ref{sec:existing}).

\subsection{Academic Dishonesty}
\label{sec:dishonesty}
There have been a number of scholarly attempts to provide a definition for the
term ``academic dishonesty.'' Thomas Lancaster, in a 2005 survey of similarity
detection systems, defines academic dishonesty as ``The process by which
students submit work for academic credit which contains other people's
unacknowledged words or ideas'' \cite{lancaster05}. Lancaster provides a
solid foundation, but there is still a question of what specific acts
constitute academic dishonesty.

The definition of academic dishonesty has been complicated by the emergence
of code-hosting websites such as \textit{GitHub} and \textit{Bitbucket}, and
Q\&A sites such as \textit{StackOverflow}. On \textit{StackOverflow}, students
may ask questions on how to complete an assignment. The answers they receive
might include example source code, and most would agree that copying this
example code without attribution constitutes academic dishonesty. However, some
professors feel that the use of any information, even hints as to algorithms
or pseudocode versions of a solution, obtained from such a source constitutes
academic dishonesty.

Students may also host code they wrote for a class on \textit{GitHub}, where
classmates or future students may copy the code without permission or
attribution. In such cases, almost anyone familiar with academia would agree
that the student who copied the given code was guilty of unauthorized copying.
However, many go further, arguing that the student who originally hosted
the code is guilty of academic dishonesty for enabling copying to take
place~--- even if that student is unaware that the copying occurred
\cite{cosma06}.

In all cases, different instructors have different definitions
of which offenses constitute unauthorized copying, complicating the creation
of tools to assist in detecting the practice. There is no consensus among
academics as to what degree of copying constitutes academic dishonesty, but
most professors insist that they ``know it when they see it.'' Mike Joy and
Michael Luck provide some example behaviors that they would consider dishonest
\cite{joy1999}:
\begin{itemize}
\item ``A weak student produces work in close collaboration with a
 colleage\textit{[sic.]} in the belief that it is acceptable.''\\
\item ``A weak student copies, then edits, a colleage's program, with or
 without the colleage's permission, hoping that this will go unnoticed.''\\
\item ``A poorly motivated (but not necessarily weak) student copies, and then
 edits, a colleague's program, with the intention of minimizing the work
 needed.''\\
\end{itemize}

A 2006 survey of UK professors produced a broad spectrum of results for what
professors perceive is (and is not) academic dishonesty \cite{cosma06}. The
sharing of source code, comments, overall design, documentation, and user
interface were all near-universally perceived to be unauthorized copying.
Specifically, use of code from other sources without acknowledgment (even if
the source code was adapted to the student's specific application, or rewritten
from another language) was considered to be academic dishonesty.

However, many respondents noted that the degree of adaptation was an important
factor, indicating that while 100\% similarity was almost certainly indicative
of unauthorized copying, sufficient changes to the code would render it
``original''; respondents disagreed as to what, exactly, constitutes
``sufficient changes.''

A majority of respondents indicated that almost every offense involving the
unauthorized duplication of source code could indeed be considered academic
dishonesty given the right circumstances, except in cases where the copied code
was written by the submitting student. In such cases (for example, the
submission of an assignment written previously for a different class, with
slight modifications), most respondents answered that it was a violation of
course policy, but not a matter of academic dishonesty.

\subsubsection{Detection of Academic Dishonesty}
There has been a great deal of research focused on the detection of academic 
dishonesty. The detection of unauthorized copying draws heavily on the fields
of Natural Language Processing and Information Retrieval to process student
submissions and efficiently store and retrieve information about similarities
\cite{beth14}. Thomas Lancaster presents a thorough overview of existing work
in his 2005 paper, including a comparison of all existing academic dishonesty
detection solutions available at time of publication \cite{lancaster05}.

Academic dishonesty detection is often broken into two broad fields: detection
of unauthorized copying in source code, and detection of unauthorized copying
in natural language \cite{lancaster05, clough03}. The source code detection
problem is generally considered the easier of the two, because
programming languages follow fixed grammars; the explicit, unambiguous syntax
of programming languages eliminates the need for advanced natural
language processing. We focus on source code similarity detection.


\paragraph{Obfuscation}
\label{sec:obfuscation}
When engaging in academic dishonesty, violators have an interest in preventing
the detection of similarities in their programs. Consequently, they often
take steps to obfuscate code in an attempt to hide or remove similarities.
Programs and algorithms that attempt to detect academic dishonesty must be
resistant to common obfuscation techniques to be successful.

Geoffrey Whale listed 12 methods of defeating similarity detection in a 1990
paper \cite{whale1990identification}. Whale's widely cited list is reproduced
below.
It is typically presented in order of sophistication, least to greatest.
\begin{enumerate}
\item Changing comments or formatting (for example, adding whitespace)
\item Changing identifiers
\item Changing the order of operands in expressions (for example, $1 + 2$
	into $2 + 1$)
\item Changing data types (substituting floats for integers, or exploding
	structures into separate variables)
\item Replacing expressions with semantically identical equivalents (for
	example, \code{!x} with \code{x == false})
\item Adding redundant statements or variables
\item Changing the order of independent statements
\item Changing the structure of iteration statements
\item Changing the structure of conditional statements
\item Replacing procedure calls with procedure bodies
\item Introducing non-structured statements such as \texttt{GOTO}s
\item Combining original and copied program fragments
\end{enumerate}

Similarities are not only useful as indicators of academic dishonesty; the next
section describes similarity detection techniques that are designed for other
applications.

\subsection{Other Applications of Similarity Detection}
There are a number of fields with an interest in detecting similarities between
input documents, most for purposes completely different than ours. These
inputs are not necessarily source code, but the fields in question may have
developed general-purpose algorithms or techniques that could be useful in the
construction of our application.


\subsubsection{Code Clone Detection}
Most professional software engineers agree that source code should not be
duplicated in a large programming project, yet it often is. Naturally, a
number of tools have been developed to search for \textit{code clones}, or
instances of duplicated source code. An early code clone detector,
\textit{dup}, was developed at AT\&T. \textit{Dup} searches C source files
line-by-line and can be used to detect parameterized matches --- files that
match when eliminating certain differences such as variable names. In 1993, 
\textit{dup} found parameterized matches for 19\% of the complete source code
of a version of the X Window System, and 23\% of a large (1.1 million line of
code) proprietary AT\&T system \cite{baker95}.


\subsubsection{Bioinformatics}
The comparison of similar sequences is common practice in Bioinformatics.
Strands of DNA, RNA, or proteins are often sequenced and compared for a
variety of reasons. A well-known example is the use of DNA testing in
the criminal justice system, where DNA comparisons are used to identify the
perpetrators of crimes from trace evidence. A number of commonly-used
similarity detection algorithms were originally developed for bioinformatics
use --- for example, the Smith-Waterman algorithm described in Section~
\ref{sec:smithwatermanlit} \cite{smith1981identification}.


\subsubsection{Copyright Infringement Detection}
Some people have tried to use similarity detection techniques to detect
copyright infringement. The earliest copyright infringement detection tool we
found was \textit{COPS} (COpyright Protection System), by Sergey Brin, James
Davis, and Hector Garcia-Molina, in 1995 \cite{brin95}. Copyright infringement
detection is a very similar problem to academic dishonesty detection, but there
is much more research explicitly focused on academic dishonesty than on
copyright infringement.


\subsection{Algorithms}
\label{sec:algo}
Previous research into similarity detection largely falls into two categories:
\textit{feature comparisons} and \textit{structural comparisons}. Feature
comparison algorithms build a profile of various attributes of the input
documents (for example, number of distinct tokens, overall word count, average
number of characters per line) and compares the profiles of submissions to
determine if they are unusually similar. Structural comparisons compare the
content of submissions --- for example, comparing the specific tokens that
form the inputs \cite{arwin2006plagiarism}. Within structural comparisons,
there are two broad subcategories: \textit{vector-distance} algorithms and
\textit{fingerprinting} algorithms.


\subsubsection{Syntax Awareness in Comparison}
\label{sec:syntaxaware}
Similarity detection on source code offers a number of advantages over working
with natural language. Every programming language can be tokenized and parsed
according to a grammar defined by the language. Through this grammar, we can
identify the purpose of all input tokens --- variable name, language keyword,
function name, etc. This permits the use of powerful normalization techniques
that are not available when dealing with natural languages.

The abstract syntax tree representation of a program highlights similarities
that may be harder to spot in plaintext. By parsing input submissions
into such syntax trees, similarity detection can be performed in a less
ambiguous manner. As compilers and interpreters normally perform this task
when preparing to compile or execute code, performing this parsing is not an
undue burden when tokenizing submissions. By parsing input submissions into an
abstract syntax tree (as a compiler or interpreter normally would do to
compile or execute the code), similarity detection can be performed on a
representation of the program where similarities that might be ambiguous in
plaintext become clear. For example, the operations $1 + 2$ and $2 + 1$ are
identical in purpose, but plaintext comparison would typically not be able to
identify their similarity beyond the shared $+$ character. However, parsing
into a syntax tree would create identical $+$ nodes with children $1$ and $2$
for both, identifying that they are identical (as addition is commutative).
Furthermore, it becomes possible to apply a consistent normalization scheme to
things like identifiers. By renaming identifiers in a consistent manner, it
becomes possible to remove the effectiveness of some approaches to obfuscating
similarities (namely, renaming functions and variables)
\cite{arwin2006plagiarism}.

The use of parsing and related normalizations can be a powerful tool to detect
obfuscated similarities. All 13 of the obfuscation techniques discussed in
Section~\ref{sec:obfuscation} can be identified and defeated by using
normalized abstract syntax trees produced by a parser
\cite{arwin2006plagiarism}. The obvious disadvantage of this approach is that
the parsing phase is language-sensitive. This limits a similarity detection
system to a few languages and imposes a significant burden to support
additional languages. In addition, language-specific parsing entails
information loss; some features that may be used indicate similarity, such as
comments or identifier misspellings, are often lost during parsing and by
similar normalizations. Despite its drawbacks, almost every similarity
detection system targeted at source code uses a syntax-aware parsing phase
to catch similarities that might otherwise be undetectable 
\cite{whale1990identification} \cite{arwin2006plagiarism} \cite{lancaster05}.


\subsubsection{Greedy String Tiling}
\label{sec:gst}
Greedy String Tiling is a popular algorithm that is most notably used by
\textit{JPlag}, described in Section~\ref{sec:jplag} \cite{prechelt00}. First,
the algorithm locates the longest string that is shared between the two
documents and designates that string as a ``tile.'' It replaces the tile with
the empty string, then tiles the next longest substring that is not already
part of a tile. It continues tiling the next largest shared string until the
largest match falls below some threshold. The algorithm is guaranteed to
terminate because the length of the maximum match decreases with each step.
Greedy string tiling runs in $O(n^3)$ worst case time, but in $O(n)$ best case
\cite{prechelt00}.

\textit{JPlag} introduced several improvements on Greedy String Tiling. First,
it searches for matching strings using Karp-Rabin string matching, as
described in Section~\ref{sec:karp-rabin}. Second, when searching for the
longest common substring between documents, it skips a number of comparisons
equal to the longest common substring found so far. If the next character
after that jump is a match, then, and only then, does the algorithm step back
to try to verify the string. Finally, \textit{JPlag} enforces that the
shorter document is always treated as the query document. \textit{Jplag}'s
optimizations reduce the average-case running time of GST to $O(n)$
\cite{prechelt00}.


\subsubsection{Vector-Distance Algorithms}
Vector-distance algorithms compare two or more inputs and identify the
number and sequence of edits that must be made to transform one of the inputs
into the other(s). The complement of this sequence of edits is the set of
things that did not change --- the similarities between the two documents.
Vector-Distance algorithms are typically the slowest of all similarity
detection algorithms, but fingerprinting and feature comparison may not
identify the best possible match.


\paragraph{Smith-Waterman Algorithm}
\label{sec:smithwatermanlit}
The \textit{Smith-Waterman} algorithm was developed by Temple Smith and
Michael Waterman in 1981 for the comparison of DNA sequences
\cite{smith1981identification}. It is a dynamic programming algorithm that
seeks to find the optimal alignment of two strings. Unlike algorithms that
solve the traditional longest common substring problem, the Smith-Waterman
algorithm is tolerant of skipped or unmatched characters.
Figure~\ref{fig:stringalignment} shows a sample alignment of two strings,
``ABCDEFG'' and ``ABCDXG.'' The longest common substring of the two would be
``ABCD,'' but a local alignment also captures the matched ``G'' character.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{smithwaterman.pdf}
\caption{A local string alignment of the type generated by the \textit{Smith-Waterman Algorithm}}
\label{fig:stringalignment}
\end{figure}

Smith-Waterman, like most vector-distance approaches, compares pairs of inputs.
Each input is placed on an axis of a 2-dimensional matrix, one token to a row
or column, as is shown in Figure~\ref{fig:smithwatermanarray}. The array is
initialized to $0$, and is then filled top to bottom, left to right. During
filling, each cell is initially filled with the largest value of its
predecessors (directly-adjacent cells to the left, above, and to the upper
left of the cell). If the characters on the X and Y axis match, the cell is
incremented by a fixed value; if the characters do not match, the cell is
decremented (unless it is $0$, in which case no action is taken). The largest
value in the array represents the best alignment of the two inputs \cite
{smith1981identification}. Due to the need to hold and then fill this array,
Smith-Waterman is an $O(n * m)$ algorithm in time and memory, where $n$ and $
$ are the length of the two inputs.

\begin{figure}
\centering
\includegraphics{smithwatermanarray.png}
\caption{The array built by the \textit{Smith-Waterman Algorithm}}
\label{fig:smithwatermanarray}
\end{figure}

In 2004, Robert Irving adapted the Smith-Waterman algorithm for program
similarity detection \cite{irving04}. By repeatedly applying the algorithm to
a pair of inputs, removing the detected overlay afterwards, Irving was able to
identify all the local alignments of the two inputs over a given threshold. He
presents a set of optimizations to the original Smith-Waterman algorithm that 
improve its performance when applied repeatedly to the same inputs by
removing the need for recomputation of the unchanged parts of the array.


\subsubsection{Fingerprinting Algorithms}
Fingerprinting algorithms (also known as \textit{feature extraction
algorithms}) extract a number of \textit{fingerprints} (or 
	\textit{features}) that can be used to identify a document. These
fingerprints are then added to a database (with a list of references
back to the inputs that contained them). To identify similarities, an input's
fingerprints are computed, added to the database, and then looked up
to see whether any submissions other than the current one have the same
fingerprints \cite{schleimer03}.

Fingerprints are typically the hashes of one or more tokens of the
input. Typically, before being inserted into the database, a small subset of
the tokens is selected, and the rest are discarded; this is done to reduce the
number of entries required in the database. A typical solution might be to
discard all hashes except those that are congruent to $0$ modulo a certain
number $p$, but better fingerprint selectors exist \cite{schleimer03}.


\paragraph{Karp-Rabin String Matching}
\label{sec:karp-rabin}
The archetypal fingerprinting algorithm is \textit{Karp-Rabin} string matching.
\textit{Karp-Rabin} generates ``rolling hashes'' of the two input submissions
(the hash of characters $N$ to $M$ of the input, then the hash of characters 
 $N + 1$ to $M + 1$, and so on). The sets of hashes for the inputs are checked
against each other, and the matches are used to identify common substrings.
\cite{karp87}. \textit{Karp-Rabin} string matching is a fundamental technique
used and built upon by many later string-matching papers \cite{schleimer03,
buss94, clough03, arwin2006plagiarism, burrows07, johnson94}.

\paragraph{$n$-gram Fingerprinting}
\label{sec:ngram}
By far the most common document fingerprinting approach is called 
\textit{n-gram fingerprinting}. Figure~\ref{fig:n-gram} presents an overview of
\textit{n-gram} fingerprinting. The document is tokenized and considered as
sequences of $n$ tokens, or \textit{n-grams}. The \textit{n-grams} overlap, so
that a document with $L$ tokens is considered as $L - n + 1$ \textit{n-grams}.
The hashes of these \textit{n-grams} are stored in a database and checked
against those from other documents. The hashes are usually stored with
pointers to their occurrences to aid in detection.

\begin{figure}
\centering
\includegraphics{ngram-diagram.png}
\caption{An overview of \textit{n-gram} fingerprinting}
\label{fig:n-gram}
\end{figure}

Most corpora are far too large to store the the hashes of every \textit{n-gram}
from every document in the database. Usually, a small subset of all hashes are
selected through an algorithm; these serve as the document's
\textit{fingerprints}. This does entail some information loss; even
word-for-word identical documents only match if they both contain an
\textit{n-gram} that was selected as a fingerprint. The fingerprint selection
algorithm is consequently very important. A very simple approach is to select
\textit{n-grams} whose hashes are divisible by some number $p$
\cite{schleimer03}, but this technique can leave gaps of arbitrary length
between fingerprints, so that a large chunk of similar text might slip by.

Schleimer, Wilkerson, and Aiken \cite{schleimer03} presented an alternative
fingerprint selection algorithm in 2003 called \textit{winnowing}. After
dividing the document's tokens into \textit{n-grams}, winnowing further
divides the \textit{n-grams} into \textit{windows} of size $t$. Winnowing
always selects exactly one hash from each window, guaranteeing that a match of
$t$ or more consecutive tokens is identified.

Winnowing has proven to be extremely powerful; it is used by the current
industry standard source code similarity detector, \textit{Measure of Software
Similarity}, or MOSS, described in Section~\ref{sec:moss}.


\subsubsection{Feature Comparison}
Feature Comparison, also known as \textit{Attribute Counting}, attempts to
compare features of inputs not related to their structure (comparing the actual
tokens that form the inputs). These comparisons are typically based on
\textit{profiles} of the input, representing a composite of a number of
defining features --- for example, line count, word count, character count,
average word per line \cite{arwin2006plagiarism}.

Feature comparison algorithms were common in the early days of similarity
detection in the 1980s and early 1990s \cite{parker89, clough03}. However, it
has become less popular of late because of the growing effectiveness of
structural comparison algorithms such as \textit{Smith-Waterman} and
fingerprinting. Feature comparison algorithms can be just as accurate as
structural comparison algorithms, but require a great deal of tuning as the
differences in the profiles of documents containing unauthorized copying and
those that do not are typically very small \cite{arwin2006plagiarism}. We
found one reference to the creation of a similarity detection system using
this algorithm after 2000, but no others \cite{jones01}. The paper describing
this system provided no evidence that the aforementioned criticisms were not
valid, and did not provide any results to substantiate its effectiveness.
Given the lack of popularity of this approach and its noted disadvantages, we
did not pursue this line of inquiry.


\subsubsection{Other Approaches}
Burrows, Tahaghoghi, and Zobel \cite{burrows07} have developed a highly scalable
approach to similarity detection using advanced information retrieval
techniques, which can handle checking for similarity among tens of thousands
of source code documents. The Burrows approach requires users to select a
function of two documents that outputs a ``similarity score'' for them.
Because the effectiveness of the Burrows approach requires using an
intelligent similarity scoring function, a number of such functions have been
developed, including the Okapi BM25 function and a family of functions
developed by Ciesielski, Nelson, and Tahaghogi using genetic programming \cite{ciesielski08}.

Belkhouche, Nix, and Hassell \cite{belkhouche04} have contributed an elaborate
academic dishonesty detection approach that compares C programs by converting
them to structures representing control flow, data tables, and other high
level concepts. Their implementation is highly language-specific.

So many other detectors of unauthorized copying have been developed that
describing all of them here would be impractical. Lancaster and Culwin have
constructed a detailed and very helpful taxonomy of commonly known copy
detection tools, and have described those tools in terms of that taxonomy
\cite{lancaster05}.


\subsection{Existing Solutions}
\label{sec:existing}
A number of programs exist for finding similarities between source code. Some
of these, like MOSS, are intended for use in detecting academic dishonesty; but
very few are publicly available. Many solutions mentioned in literature were
never released, or have not been maintained for many years. The two noteworthy
products, MOSS and \textit{JPlag}, are described below.


\subsubsection{MOSS}
\label{sec:moss}
MOSS, an acronym for \textit{Measure of Software Similarity}, is a solution
developed by Professor Alex Aiken of Stanford in 1994 \cite{bowyer99}. Today,
two decades later, it is considered the gold standard in software similarity
detection \cite{beth14}. MOSS is free for non-commercial use (though it was 
previously restricted only for use in academia). It is an online service, not
a software tool that can be deployed. Its primary interface is a Perl script
that provides a command-line frontend to submit code for analysis. Results
are provided via email, and can take several hours to arrive \cite{bowyer99}.
MOSS is based on the $n$-gram Fingerprinting algorithm with winnowing
described in Section~\ref{sec:ngram}. Most of the details about the
implementation are public, but the tuning parameters of the algorithm are kept
private. 

Despite having used the same algorithm for over a 
decade, MOSS remains highly competitive. In a battery of tests run in 2014
using several similarity detection algorithms, MOSS posts detection results
comparable every other algorithm tested \cite{beth14}. In fact, when new
similarity detection systems are published, they often compare their results
with those of MOSS. 

All academic dishonesty detectors suffer from the diversity of definitions of
unauthorized copying, and MOSS is no exception. MOSS often flags behavior that
our advisor, our primary customer, does not consider to be unauthorized
copying. The secrecy of MOSS's tuning parameters and database also prevent
researchers from independently evaluating its performance or reproducing any of
its results.


\subsubsection{JPlag}
\label{sec:jplag}
\textit{JPlag} is a web service developed by Guido Malpohl in 1996.
\textit{JPlag} compares closely with the performance of MOSS, and some academic
dishonesty detection tool developers choose to forgo testing their own tools
against MOSS in favor of \textit{JPlag} \cite{prechelt00, ciesielski08}.

\textit{JPlag} allows users to submit an archive of programming files to its
web service; the files will then be compared pairwise against each other.
\textit{Jplag} first runs the programs through a parser or scanner for the
appropriate programming language, then uses the outputs from the parser or
scanner as the input strings to its backend algorithm; only the front-end
parsing step is language-dependent \cite{prechelt00}. \textit{JPlag} is
advertised to support C, C++, Java, Scheme, C\#, and even natural language.

\textit{JPlag}'s core algorithm is Greedy String Tiling, with a number of
optimizations, as described in Section~\ref{sec:gst}.

