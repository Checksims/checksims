\documentclass{article}
\author{Dolan Murvihill \and Matthew Heon}
\title{Program Similarity Detection: Our Approach - Checksims}
\begin{document}
\maketitle

\section{Requirements}
When we first began to design our solution to discovering similarities in
software, we naturally began by developing a list of requirements which would
have to be met. Some of these were solicited from our advisor (the intended
audience of the program), while others came from our literature review. These
requirements are summarized below, and drove the decisions which created our
solution.

From Professor Lauer, we obtained the requirement that whatever we produce must
be easy to use. The program should be able to be used by course staff with
minimal to no training, and should produce output which can be easily
interpreted. The output itself did not need to be a definitive determination of
academic wrongdoing; instead, it simply needed to flag suspicious submissions
for further review by course staff. The detector needed to be complete and
usable inside of seven weeks, placing a severe time limit on implementation and
encouraging the implementation of a relatively small set of features. Given
this time constrant and the potential that the detector would be used for a
number of classes taught using varying languages, it was also desirable that
the detector not attempt to perform language-specific analysis of the source
code, but instead only interpret submissions as plaintext. Finally, the
detector should be made to match Professor Lauer's specific definition of
similarity and academic dishonesty, which is very permissive of relatively
similar code so long as it was typed in separately. This meant that some types
of similarity detection (for example, detecting semantically-identical code
segments) are far less useful, as they match activities which are considered
perfectly acceptable under this definition of academic dishonesty.

From our literature review, we determined that source code similarity detection
tools almost universally will parse input submissions into a syntax tree, as it
is very easy to disguise (intentially or unintentionally) similar code through
a number of small tweaks (swapping argument or operand order, for example).
This is obviously in conflict with the aforementioned desire for a plaintext
only system. We determined that the the plaintext requirement was the
overriding concern. Given this, we were forced to discard many of the
algorithms and innovations of existing software similarity detection research.
Instead, we sought to identify methods used for similarity detection in
plaintext submissions which could be sufficiently accurate and run in a
reasonable amount of time.

These requirements seem to lead to a product which is small in scope,
implementing only a subset of functionality which might eventually be
desirable. With such a severe time constraint on primary implementation, we
decided to require that our solution be modular and easily extensible. While we
might not be able to implement all features we desired, the work could be
passed on to future MQP teams to drive implementation of other features.
Proper documentation and unit testing of all produced source code, and emphasis
on extendability when designing the architecture, were decided to be
priorities.

% Reference to MOSS in lit review
We decided to implement a client-based solution (as opposed to a hosted
solution, similar to MOSS). This offers a number of benefits, most notably as a
time saving measure. A hosted solution would require the creation of a backend
server, a client to interact with this server, and a stable API for the two to
communicate. In comparison, a client-based solution only requires one piece of
software, the client itself. Furthermore, an open-source, client-based solution
offers the ability for technically capable users to easily modify our solution
to meet their own needs with ease. Finally, a client-based solution is
transparent in its operation - there is no magic occurring ``behind the
curtain'' on server-side, and any results can be independently verified.

Finally, we decided to create a require that our similarity detection tool be
capable of running on all common operating systems. Though we do not believe
that data on operating system preferences among WPI course staff exists, we
have anecdotal evidence to suggest that Windows, Mac OS X, and Linux are all
used regularly. Given this, we did not want to require that our software run on
a specific operating system.

We decided not to require a graphical interface, despite its positive
implications for usability. Given our time constraints, implementation of a
full GUI was hard to justify. Furthermore, given the limited complexity of the
features we intended to implement, we felt that a command-line interface would
be more than sufficient to provide an easy-to-use interface for our solution.


\section{Our Approach}
Our approach to similarity detection is named \textit{Checksims}, and was
built to fulfill all requirements we derived. \textit{Checksims} is a
client-based Java application implementing a pair of similarity detection
algorithms and several output formats. It performs similarity detection on an
arbitrary number of student submissions within a single assignment, and
produces output which can be used to easily determine which submissions were
unusually similar.

\textit{Checksims} uses a simple, modular design designed for easy
extensibility. It is designed to be trivially easy to create a add a new
similarity detection algorithm or output strategy without significant changes
to the program. This will enable future projects to expand on our work and
offer further features without needing to make significant changes to the core
program, increasing productivity and decreasing the likelyhood of introducing
bugs.

\subsection{Submissions and Tokenization}
\textit{Checksims} accepts as input a directory containing a number of student
submissions, and a pattern to match files to include. It is assumed that each
submission is contained within a single subdirectory of the input directory,
and all files matching the given pattern will be recursively found in each
submission directory and concatenated to form a submission.

Submissions are then split into tokens for comparison by a similarity detection
algorithm. Several algorithms are included to perform this task, all resulting
in a linked list of tokens representing the original input. All current
tokenizing algorithms do not affect the order of the input submission, though
some may alter the original submission by removing whitespace. All tokenizing
algorithms operate on the plaintext of the submission only, and no attempt to
parse the input into a syntax tree (or, indeed, use the grammar of the input
language at all), persuant to our stated goal of performing plaintext only
comparisons. Parsing using a language-specific grammar should be able to be
added to \textit{Checksims}, but as no provision was made for it initially, it
is expected it will be much more difficult than adding a new similarity
detection algorithm or 

Tokenization type has no effect on the function of similarity detection
algorithms. Tokens are presented as a simple interface, which enables equality
checking without care for what type of token is being accepted. Because of
this, all similarity detection algorithms are tokenization agnostic and can be
run with any token type. To ensure internal consistency, however, only one
tokenization strategy is used for all submissions, and thus all tokens that
are handled in a single run of the program are of the same type.

Internally, tokens are represented as integers, with a distinct integer ID for
each distinct input token (and repeated input tokens reusing the previously
assigned ID). For example, the input sequence ``ABCDA'' would be tokenized into
the sequence ``0 1 2 3 0'' (note the repeated identifier for the repeated
character). This choice is less efficient when tokens represent individual
characters of the input submission (32-bit integers representing UTF-8
characters which will, in typical case, be 8 bits), it offers a substantial
space and performance advantage for tokenization algorithms which represent
substrings of the input as tokens (for example, line tokenization, which splits
the input submission at newline characters to form tokens). Given that we
expect most uses of this program to use string-backed tokens (for reasons given
subsequently), using integers as tokens was included as an optimization
measure.

\subsection{Preprocessing and Common Code Removal}
Once submissions have been accepted and tokenized, \textit{Checksims} offers
the option of running them through a set of preprocessors before performing
similarity detection. Preprocessors manipulate the tokens of submissions to
normalize them prior to running detection algorithms. The only normalization
currently included transforms all tokens by lowercasing any characters within,
ensuring that case is not a factor in comparisons. Preprocessors are
implemented modularly, so more can be easily added in the future.

Common code removal can also be performed at this stage. Common code removal
accepts a submission which contains given code which is expected to be present
in all submissions. This code is tokenized using the same method used for all
other submissions, and similarity detection is performed between it and each
submission. Any code matching the common code is removed prior to the main
similarity detection algorithm being run. Common code removal is performed
prior to any preprocessors, as common code should match exactly without them.

\subsection{Similarity Detection}
After submissions have been tokenized and normalized, \texit{Checksims} will
apply a pairwise similarity detection algorithm to all unique unordered pairs
of submissions. This will obtain the similarity of every submission in the
input with every other input submission, representing a complete picture of how
similar all submissions within the group are. At present, two pairwise
detection algorithms are included: Line Comparison and Smith-Waterman.

Pairwise detection was chosen to simplify the construction of
\texit{Checksims}, and allow easy multithreading. The alternative would be to
create a database of features (token sequences, for example) for all
submissions encountered, and compare new submissions against this database to
check for matches (while adding their features to it, to ensure they will be
matched in the future). 

\subsubsection{Line Comparison}


\subsubsection{Smith-Waterman Algorithm}

\subsection{User-Friendly Output}

\subsubsecton{Similarity Matrix}

\end{document}