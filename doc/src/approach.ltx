\section{Requirements}
\label{sec:requirements}
Our advisor, Professor Lauer, commissioned the \textit{Checksims} similarity
detection tool with the following requirements:
\begin{itemize}
\item The program should be usable by course staff with very little
 or no training, and should produce output in a form that can be easily
 interpreted.
\item The output itself should not be a definitive accusation of academic
 wrongdoing; instead, it should simply flag suspicious submissions for further
 review by course staff.
\item The detector should to be complete and usable within seven weeks; this
 requirement placed a severe time limit on implementation and encouraged the
 implementation of a relatively small set of features.
\item The detector should not attempt to perform language-specific analysis of
 the source code, but instead only interpret submissions as plaintext. The
 language-agnosticism requirement came from the time constraint and the
 potential that the detector would be used for a number of classes using
 various languages.
\item The algorithm should be run locally and preferably be easy to invoke once
 student submissions are closed.
\item Finally, the detector should be made to match Professor Lauer's specific
 definition of similarity and academic dishonesty, which is very permissive of
 relatively similar code so long as it was typed separately.
\end{itemize}

Source code similarity detection tools almost always parse input submissions
into syntax trees, as it is very easy to disguise (intentionally or 
unintentionally) similar code through a number of small tweaks (swapping 
argument or operand order, for example). It was apparent to us that 
performing any kind of syntax tree analysis was incompatible with the 
requirement that the system produced should operate on plaintext only. The
plaintext requirement was the overriding concern --- syntax-based parsing 
limits the languages that can be used with a detector, and it would also be
difficult to implement given our time constraints.

The requirements seem to lead to a tool that is small in scope, implementing
only a subset of the functionality that might eventually be desirable. The
solution should be modular and easily extensible, so that new features can be
added to the very basic initial feature set; the intention is to pass the tool
on to future project teams, or to the open source community. The requirements
emphasize an extensible architecture, proper documentation and
unit testing, and a useful, extensible test suite.

% Reference to MOSS in lit review (is this necessary?)
The requirements lead naturally to a client-based solution (as opposed to a
hosted solution such as MOSS). The short implementation time led to very simple
frontend and user-interface code, ensured that as much implementation time as
possible was spent working on the actual similarity detection code. A client
based solution does have the notable disadvantage of complicating access to
larger corpora of assignments, which have to be downloaded and run locally.
While a larger data set is certainly desirable, implementation simplicity was
more important. Furthermore, an open-source, client-based solution offers the
ability for technically capable users to easily modify our solution to meet
their own needs. Finally, a client-based solution is transparent in its
required operation --- there is no magic occurring ``behind the curtain'' on
the server. The transparency of the design also makes it very easy to
independently verify results.

Instructors and teaching assistants at WPI use Windows, Mac OS X, and Linux.
The tool should be platform-independent and as easy to use as possible on all
three major operating systems. A graphical interface would improve usability,
but due to implementation time constraints it was not added as a requirement.


\section{Approach}
\label{sec:approach}
Our similarity detection tool, \textit{Checksims}, was built to fulfill all
requirements outlined above. \textit{Checksims} is a client-based Java
application implementing a pair of similarity detection algorithms and
several output formats. It performs similarity detection on an arbitrary
number of student submissions within a single assignment, and it produces
output that can easily identify submissions that need to be checked by hand for
unauthorized copying.

\textit{Checksims} uses a simple, modular architecture designed for easy
extensibility. It is designed to be trivial to add new similarity
detection algorithms or output strategies without significant changes to the
program, thus enabling future projects to expand on our work and offer further
features without requiring significant changes to the core program,
increasing productivity and decreasing the likelihood of introducing bugs.

\subsection{Architecture}
\textit{Checksims} has a roughly linear architecture, composed of a number of
discrete components, most with one input and one output. The overall service
accepts a set of student submissions as input, and returns usable output.
The overall architecture is shown in Figure~\ref{fig:architecture} and is
described below.

\begin{enumerate}
 \item Student submissions are first processed by the \textit{tokenizer}. The
    tokenizer identifies all files within the submission, applies a tokenizing
    algorithm on them, and yields the resulting tokenized submissions.
 \item The tokenized submissions are then passed into a
    \textit{common code remover}, which removes code designated as ``common''
    from all tokenized submissions to ensure it is not matched.
 \item One or more \textit{preprocessors} are applied to the tokenized
    submissions, transforming the tokens to improve accuracy.
 \item Submissions are then grouped into pairs. A user-selectable
    \textit{similarity detector}, which implements a \textit{similarity
    detection algorithm}, is then run on all possible pairs of submissions, and
    the results are recorded in a \textit{similarity matrix}.
 \item The similarity matrix is then passed to a user-selected \textit{output
    strategy}, which produces a human-readable form of the output for parsing.
\end{enumerate}

% Architecture Diagram
\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{../src/architecture.pdf}
\caption{The Architecture of \textit{Checksims}}
\label{fig:architecture}
\end{sidewaysfigure}


\subsection{Submissions and Tokenization}
\textit{Checksims} accepts an input directory containing a number of student
submissions and a pattern to match files to be composed into a submission.
It is assumed that each submission is contained within a single subdirectory
of the input directory --- that is, all student code is located in
subdirectories of the input directory, and each subdirectory contains the code
of exactly one student. All files within a single subdirectory are considered
to belong to a single student, and are compared with a \textit{match pattern}
to determine whether they are checked. This allows \textit{Checksims} to only
include source files; for example, a match pattern of \texttt{*.\{c,h\}} would
only match C source files, and would ignore README files.

Submissions are then split into tokens for comparison by a similarity detection
algorithm. Several tokenizers are included to perform this task, all resulting
in a linked list of tokens representing the original input. The same
tokenization algorithm is used for all submissions to ensure internal
consistency. None of the provided tokenizing algorithms affect the order of
the input submission, though some may alter the original submission by
removing whitespace. The provided tokenizing algorithms operate on the
plaintext of the submission only. No attempt is made to parse the input into
a syntax tree (or even use the grammar of the input language at all),
pursuant to our stated goal of performing plaintext only comparisons. Parsing
using a language-specific grammar could be added to \textit{Checksims}, but
as no provision was made for it initially, would probably be more difficult
than simply plugging in a new similarity detection
algorithm.

% This section may not be desirable to include
% Commenting out for now
\iffalse
Tokenization type has no effect on the function of similarity detection
algorithms. Tokens are presented as a simple interface, which enables equality
checking without care for what type of token is being accepted. Because of
this, all similarity detection algorithms are tokenization agnostic and can be
run with any token type. To ensure internal consistency, however, only one
tokenization strategy is used for all submissions, and thus all tokens that
are handled in a single run of the program are of the same type.

Internally, tokens are represented as integers, with a distinct integer ID for
each distinct input token (and repeated input tokens reusing the previously
assigned ID). For example, the input sequence ``ABCDA'' would be tokenized into
the sequence ``0 1 2 3 0'' (note the repeated identifier for the repeated
character). This choice is less efficient when tokens represent individual
characters of the input submission (32-bit integers representing UTF-8
characters which are usually 8 bits), but it offers a
substantial space and performance advantage for tokenization algorithms that
represent substrings of the input as tokens (for example, line tokenization,
which splits the input submission at newline characters to form tokens). Given
that we expect most uses of this program to use string-backed tokens (for
reasons given subsequently), using integers as tokens was included as an
optimization measure.
\fi


\subsection{Preprocessing and Common Code Removal}
Once submissions have been accepted and tokenized, \textit{Checksims} performs
common code removal. Common code removal accepts a submission that contains
code that is expected to be present in all submissions --- for instance,
templates, copyright notices or helper functions provided by the instructor.
This code is tokenized using the same method used for all other submissions,
and similarity detection is performed between it and each submission. Any
code matching the common code is removed before the main similarity detection
algorithm is run. Common code removal can also improve the performance of more
expensive similarity detection algorithms by making submissions smaller. 

\textit{Checksims} offers the option of modifying submissions with one or more
preprocessors after performing common code removal but before performing
similarity detection. Preprocessors manipulate the token representations of
submissions to normalize them prior to running detection algorithms. The only
preprocessor in the current release converts all letters to lower case.
Preprocessors are implemented modularly, and more are planned in the future. 


\subsection{Similarity Detection}
After submissions have been tokenized and normalized, \textit{Checksims}
applies a pairwise similarity detection algorithm to all unique unordered pairs
of submissions, obtaining the similarity of every submission in the input with
every other input submission --- a complete picture of the similarities within
the group. At present, two pairwise detection algorithms are included with 
\textit{Checksims}: \textit{Line Comparison}, and \textit{Smith-Waterman}.

Comparisons are carried out pairwise in order to simplify the construction of
\textit{Checksims} and enable easy multithreading. An alternative would be to
create a database of features (token sequences, for example) for all
submissions encountered, and compare new submissions against this database to
check for matches (while adding their own unique features to it, so future
checks will include them). Pairwise detection is easy to run in parallel
because of its lack of shared state, and easy to represent in a similarity
matrix.

Many similarity detectors, including MOSS, check submissions against a database
instead of running a pairwise comparison. Using a central database reduces the
space complexity of the framework from $O(n^2)$ to $O(n)$ when running
detection algorithms that can take advantage of it. \textit{Checksims} does not
at present support this architecture, because it would complicate
implementation of multithreading by introducing a shared resource (the
database). It would probably be a good idea to add support for this
architecture in the future.

The following subsections describe the two similarity detectors provided with
the current release of \textit{Checksims}.


\subsubsection{Line Comparison}
\label{sec:linecompare}
The \textit{Line Comparison} algorithm is a special case of \textit{n-gram
fingerprinting}, as described in Section~\ref{sec:ngram}. \textit{Line
Comparison} is meant to work with line-sized tokens. It hashes each input token
and creates a map of each hash to each occurrence (the position and
submission where the token was found). Hash collisions are identified as
hashes that map to more than one occurrence, and collisions involving both
submissions are tokens shared between the two. The percentage of tokens
involved in such collisions is tallied and reported as the final
result.

It is worth noting that line comparison is actually a \textit{feature
extraction} algorithm, and could be run with a central database of
submissions if desired. However, since \textit{Checksims} only implements
pairwise comparisons, the ``database'' used is a hash table that must be
rebuilt every time a submission is compared. The current architecture is not
particularly efficient, but because of the high speed of modern hashing
algorithms, the loss of performance does not have a noticeable impact on
execution speed.

\textit{Line Comparison} is a simple algorithm that was implemented as a
proof of concept. It runs extremely fast in linear time with the size of both
submissions, but it misses a number of similarities due to the nature of hash
collisions. Even a trivial change (a single letter added or removed, for
example) results in a different hash, causing the changed token to not be
matched. Indeed, all of the obfuscation techniques in
Section~\ref{sec:obfuscation} can defeat the \textit{Line Comparison}
algorithm, unless preprocessors --- which cannot easily thwart sophisticated
obfuscations such as arithmetic operand reordering --- are applied to
combat them. Furthermore, almost all hashes enforce the property that any
change in the input results in major changes to the output. Therefore, it is
impossible to tell the degree of similarity between two tokens simply by
comparing their hashes. Therefore, it is impossible to identify similar lines
just from their hashes, preventing  \textit{Line Comparison} from being used
to identify very similar hashes. Line Comparison remains in \textit{Checksims}
both as a proof of concept and example of a simple algorithm, and to
quickly identify extremely similar or identical submissions (an initial check
that can be run prior to a slower but more accurate algorithm such as
\textit{Smith-Waterman}).

\textit{Line Comparison} is designed to work with tokens
that represent lines; its usefulness with significantly coarser or finer
tokenizers is questionable. In the character tokenization case, for example,
almost every submission is presumably written using the same subset of
characters (capital and lowercase letters, numbers, and punctuation used as
language-specific syntax). These characters are shared between almost every
submission; it is unusual to see a similarity result of less than 99\% when
using \textit{Line Comparison} with character tokenization.


\subsubsection{Smith-Waterman Algorithm}
\label{sec:smithwaterman}
The \textit{Smith-Waterman} algorithm is the primary similarity detection
algorithm included in \textit{Checksims}. It is described in
Section~\ref{sec:smithwatermanlit}. \textit{Smith-Waterman}'s first published
use in academic dishonesty detection was by Robert Irving \cite{irving04}. When
run against our test corpus, Smith-Waterman defeated multiple methods of
obfuscation described in Section~\ref{sec:obfuscation}, including changed
identifier names. We believe \textit{Smith-Waterman} caught most of the
instances of academic dishonesty in our test corpus. Smith-Waterman is
described in Section~\ref{sec:smithwatermanlit}. Section~\ref{sec:results}.
discusses its performance.

\textit{Smith-Waterman} scales poorly. Its running time and memory usage both
scale as $O(m * n)$ where $m$ and $n$ are the size of the two submissions
being compared (after tokenizing). If both submissions are the same size,
scaling is $O(n^{2})$. Initially, the algorithm ran too slowly to be useful,
even on classes of 40-50 students. We made several optimization passes to
improve performance, including changing the default tokenization algorithm
for \textit{Smith-Waterman} from character tokens to whitespace-separated
tokens, reducing the number of tokens by an estimated factor of four. After
these optimizations, \textit{Smith-Waterman} is fast enough for typical use
cases at WPI (classes of 60-70 students and submissions of 500 to 1000 tokens),
though substantially larger class sizes or an increase in average size of the
submissions themselves greatly increases the time required for the algorithm
to complete. Larger class sizes are more manageable than larger submission
sizes. Based on the results described in Section~\ref{sec:runtime}, we
estimate that a class of 100 to 110 students might finish in around two hours
while a large increase in token count could cause individual comparisons to
take days to run and require dozens, if not hundreds, of gigabytes of memory. 


\subsection{User-Friendly Output}
After all possible similarities have been computed, \textit{Checksims} formats
the results into a ``similarity matrix'' as described in
Section~\ref{sec:simmatrix}, and then uses an output strategy to format and
print the resulting matrix. A variety of output strategies are available. The
specific strategy used is user-specified. All output strategies focus on
presenting information in a usable fashion, with an emphasis on identifying
unusually large similarities easily.

Like preprocessors and algorithms, output strategies are pluggable modules, 
allowing new output strategies to be written and inserted with ease, with the
restriction that they can only display information contained in the
similarity matrix. Some information that might be desirable to display (for
example, the specific matching tokens) is not present in the similarity matrix,
placing limits on what output formats are possible. It may be desirable to make
additional information available to output strategies in the future.


\subsubsection{Similarity Matrix}
\label{sec:simmatrix}
Output strategies work from a ``similarity matrix''. These matrices are built
from the complete results of the similarity detection algorithm, and contain
the similarity of every submission to every other. As the name would imply, a
similarity matrix is a $N$x$N$ matrix (with $N$ being the number of
submissions), with each cell representing the similarity of one submission with
another.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{matrix.pdf}
\caption{A sample similarity matrix}
\label{fig:matrix}
\end{figure}

In a similarity matrix, the submissions used in similarity detection are
counted, and a square matrix of that dimension is created. Submissions are each
assigned a row and column. Every cell is initialized as the similarity of the
submissions that define its intersection (specifically, column submission's
similarity to row submission). If the row and column submissions are the same,
the cell is ignored (declared as empty). An example is shown in
Figure~\ref{fig:matrix}. Each cell shows the similarity of the submission on
the X axis with the submission on the Y axis. In Figure~\ref{fig:matrix}, the
bottom-right corner cell shows the percentage similarity of submission A to
submission C --- that is, the percent of submission A's tokens that are shared
with submission C.

\begin{figure}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{htmloutput.png}
\caption{Sample output from the HTML output strategy}
\label{fig:html}
\end{figure}

Some output strategies print the similarity matrix itself, possibly with
visual aids to ``call out'' unusually large similarities. For example, the
\texttt{HTML} output strategy produces a web page containing a similarity
matrix with cells color-coded to allow the eye to pick out the most similar
submissions by hand. A screenshot of sample output from this output strategy
is shown in Figure~\ref{fig:html}.

