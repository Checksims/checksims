\documentclass{article}
\title{Software Similarity Detection: Proposal}
\author{Matthew Heon \and Dolan Murvihill}

\begin{document}
\maketitle
Academic dishonesty is a common phenomenon in any education system. It is most
common in introductory classes, where struggling students duplicate the work of
other, more talented students and claim it as their own. Plagiarism severely
damages the integrity of a class, and is considered one of the most severe
transgressions in academia. The severity of unauthorized copying, coupled with
its frequency, means that instructors and teaching assistants must spend a
significant amount of time checking for academic dishonesty. Even so, cheating
is not always caught.

To help save instructors' time and reduce the frequency
of uncaught duplication, we propose automatically applying heuristics to
identify similar work in the specific discipline of computer programming. No
current technology can replace a human in determining whether a work was
copied, but we believe appropriate algorithms can focus the attention of the
course staff on similar work submissions, allowing them to catch more with the
time they have.

\section{Deliverables}
This is a two-term MQP, with 2/3 credits being applied in A term of 2014 and
1/3 credit being applied in B term of 2014. Over the course of the project,
we will produce the following:
\subsection{Literature Review}
We will conduct a thorough review of prior work in the field of plagiarism
detection, and the domain-specific field of programming languages, to identify
the most common techniques in use today and inform our direction. We will
produce a summary of our findings, discussing the relevant algorithms,
tradeoffs, and evaluation techniques currently used in the field. Our findings
will be thoroughly cited, and we will recommend the most important papers to
read. The finished literature review will be ready at the end of A term, but
the bulk of the research will be done early in the term, to inform our
engineering process.

\subsection{Evaluation Framework}
We will design a software system which we can use to receive measurable,
reproducible, actionable data to evaluate any algorithms we design. The
framework is designed to give us a quantitative understanding of our product's
characteristics, which will inform our engineering process. The framework will
be designed to conform as closely as possible to the industry standard methods
for evaluating similar software, as identified in our literature review. The
evaluation framework will be in a usable state early in the term, and we will
improve it continuously as needed.

\subsection{Server Framework}
We will create a server software, designed to run on UNIX-like operating
systems, that provides an interface for a software similarity checking
algorithm. The server will provide a REST API for requesting similarity checks
on assignments, which will allow the client to specify the algorithm(s) to use
for the checks, as well as other useful parameters, such as alert thresholds.
The server will also be able to store a large number of previous work to check
against. The server code will be as compact as possible, and refactored for
generality only when needed.

\subsection{Example Plugin}
We will implement a similarity checking algorithm for use in our advisor's
sophomore level computer science class during B term. The example plugin will
be focused on catching obvious copies and defeat only the simplest evasion
tactics. The example plugin is our first priority; we will provide a working
program in time for use in B term.

\subsection{Example Client}
We will develop a command line tool, available on UNIX and Windows, that makes
appropriate REST calls to the server and reports the responses in a
human-friendly manner. The tool will be available for use by the start of B
term.

\subsection{Final Paper}
We will write a final paper according to WPI's project guidelines. It will be
available by the project deadline.

\end{document}
