\documentclass{article}
\usepackage{url}
\title{Checksims: A User's Guide}
\author{Dolan Murvihill \and Matthew Heon}
\begin{document}
\maketitle

\section{Introduction}
\textit{Checksims} is a tool for detecting source code similarities in an
arbitrary number of user-provided programming projects. Its primary purpose is
to detect academic dishonesty in programming assignments.

Checksims accepts a number of submissions (programming assignments) as input,
applies a tokenizer to transform each submission into a series of tokens, and
then applies a pairwise similarity detection algorithm to all possible pairs
of submissions. The results of the algorithm are then printed via an output
strategy.

\section{Installing Checksims}
\textit{Checksims} is distributed as an executable Java package (.jar file).
As a Java application, Checksims is cross-platform and should run on any system
capable of running a Java virtual machine (JVM). The provided Jar file is
completely self-contained and requires no installation, and should be named
as follows:
\\
\indent
\texttt{checksims-1.0-jar-with-dependencies.jar}
\\

Note that 1.0 represents the current version of \textit{Checksims} at the time
of this writing, and may be different for the version you receive.

Please note that Checksims requires a Java 8 virtual machine. The latest
version of the Oracle JVM is recommended, and can be found at the following
URL:
\\
\indent
\url{https://www.java.com/en/download/index.jsp}
\\

\section{Running Checksims}

\textit{Checksims} is a command-line application, and is typically invoked
from your operating system's shell. The .jar file given can be run using Java
as follows:
\\
\indent
\texttt{java -jar PATH/TO/CHECKSIMS\_JAR.jar <ARGUMENTS>}
\\

It may be desirable to rename the provided .jar file or write a wrapping shell
script to reduce the amount of typing required for this basic invocation.

Checksims has two mandatory arguments: a glob match pattern, and a directory
to scan for submissions.

The glob match pattern is a shell-style match pattern used to identify files to
include in submissions. Wildcard characters accepted by a shell are permitted;
for example, providing a \texttt{*} will include every file in a submission,
while \texttt{*.c} will include all C files. To ensure that these are not
parsed by your shell, it is recommended to escape this pattern (for example,
with quotes in a Linux bash shell).

After the glob match pattern, one or more directories to search for submissions
must be provided. \textit{Checksims} assumes each subdirectory of these
search directories is a submission. It will recursively identify any files
matching the given glob pattern in a submission directory, append all matching
files together, and tokenize the collection. File naming information is lost
during this process, but the contents of all matching files will be present.
Each submission is named for the directory containing it; if a directory
containing two subdirectories named ``A'' and ``B'' is provided as a search
directory for submissions, two submissions named ``A'' and ``B'' will be
created.

After creation, any empty submissions (no files found matching given pattern,
or only empty files found) will be removed prior to running the detection algorithm.

At present, there is no way of differentiating submissions beyond placing them
within separate directories.

Before the glob matcher, you may place a number of arguments to control
Checksims's functionality. These are detailed below:

\begin{itemize}
\item \texttt{-a}, \texttt{--algorithm}: Specify algorithm to use for
similarity detection. Available options are listed with \texttt{-h}. If no
algorithm is given, the default is used.
\item \texttt{-c}, \texttt{--common}: Perform common code removal. Specify a
directory containing common code (files within this directory will be
identified using the same glob matcher as normal submissions).
\item \texttt{-f}, \texttt{--file}: Output to a file. Must provide filename of
output file as argument.
\item \texttt{-h}, \texttt{--help}: Print usage information and available
algorithms, preprocessors, and output strategies.
\item \texttt{-j}, \texttt{--jobs}: Specify number of threads to use. Defaults
to number of CPUs available on your system.
\item \texttt{-o}, \texttt{--output}: Specify output format. Available options
are listed with \texttt{-h}. If no output format is given, the default is used.
\item \texttt{-p}, \texttt{--preprocess}: Specify preprocessors to apply. More
than one can be provided; if so, separate them with commons. Available options
are listed with \texttt{-h}. If this argument is not provided, no preprocessors
are applied.
\item \texttt{-t}, \texttt{--token}: Specify tokenization to use. Available
options are listed with \texttt{-h}. If no tokenization is given, the default
is used.
\item \texttt{-v}, \texttt{--verbose}: Use verbose debugging output.
\end{itemize}

Checksims contains built-in usage information and descriptions of its arguments
which can be printed by supplying the \texttt{-h} or \texttt{--help} flag.

\section{Appendix A: Description of Tokenizations}
\textit{Checksims} breaks submissions into a series of tokens as they are read
in. Several options are provided, each providing a tradeoff of speed for
performance. All similarity detection algorithms provide a default tokenization
which has been chosen to optimize their performance for typical usage, but this
default can be overridden at runtime if desired. This may be desirable, as
tokenization has strong implications for algorithm accuracy and performance.

Only one tokenization is supported at any given time; it is impossible to
request that Checksims tokenize one submission using character tokenization,
and another using whitespace tokenizations. This is done to ensure a uniform
basis for token comparison.

Three tokenization options are provided by default: Character, Whitespace, and
Line. Their advantages and disadvantages are listed below.

\subsection{Character Tokenization}
The simplest tokenization method, character tokenization breaks a submission
into the characters which compose it and builds a token for each character.
Whitespace characters (spaces and newlines) are included as tokens.

Character tokenization has the slowest performance of all the tokenization
schemes as it generates far more tokens for the algorithm to process. However,
for most algorithms, character tokenization will be the most conducive to
accuracy, as it can identify largely similar words and lines which would
otherwise be ignored. Character tokenization also runs a slight memory overhead
compared to the other tokenization schemes.

\subsection{Whitespace Tokenization}
Whitespace tokenization breaks a submission apart at whitespace characters
(spaces, tabs, newlines) to create tokens. Whitespace characters are removed as
part of the splitting process, and are not included as tokens.

Whitespace tokenization represents a balance between performance and accuracy.
With preprocessing (lowercasing to remove case ambiguity, etc), it can retain
much of the accuracy of character tokenization while substantially improving
performance (assuming whitespace tokens are on average 4 characters, a fourfold
reduction in token count can be expected, even ignoring the deletion of
whitespace).

\subsection{Line Tokenization}
Line tokenization splits the submission on newlines, creating a token from each
line in the original. Non-newline whitespace characters (spaces and tabs) are
retained.

Line tokenization represents the fastest but least precise tokenization option.
It is capable of identifying exact duplication, but even trivial attempts to
obfuscate similarities will prevent their being identified.

\section{Appendix B: Description of Preprocessors}
After a submission is converted into tokens, these tokens can then be
manipulated to improve detection accuracy. This is accomplished by the use of
predefined preprocessors. At present, only one of these is available. The
lowercase preprocessor lowercases all letters in all tokens, effectively
ensuring that algorithms are case-insensitive.

\section{Appendix C: Description of Algorithms}
\textit{Checksims} provides two primary detection algorithms at present. The
first, Smith-Waterman, offers accurate detection but slow performance. The
second, Line Comparison, is very fast, but not very accurate.

\subsection{Smith-Waterman}
The Smith-Waterman algorithm for string overlaying was originally developed to
find optimal local alignments between DNA sequences for bioinformatics
problems. Adapted to handle arbitrary alphabets, it proves a valuable tool for
identifying similar token sequences. As a local alignment algorithm, it is
capable of detecting sequences even when they are not completely identical. A
small number of missing or unmatched tokens are tolerated, identifying more
similarities than simply finding the longest common sequence. Furthermore,
Smith-Waterman is guaranteed to identify the optimal local alignment - if
common sequences exist, they will be found.

However, Smith-Waterman's accuracy comes at a substantial performance cost. The
algorithm itself is $O(nm)$ where $n$ and $m$ are the lengths of the two
sequences being compared; assuming equal and even growth of both sequences, the
algorithm scales exponentially (both for runtime and memory). For smaller
submissions, Smith-Waterman can complete an entire class in a few minutes; for
larger submissions, however, hours (or even days) may be required.

Because of the performance penalty of Smith-Waterman, it is recommended to use
it with the whitespace tokenization scheme, which it defaults to. This
minimizes the amount of tokens present, greatly improving performance.

\subsection{Line Comparison}
The Line Comparison algorithm identifies identical tokens in both submissions.
It is a trivial algorithm unique to Checksims, and notable for its speed. Line
comparison hashes each input token, and identifies hash collisions (identical
tokens). Similarity is reported on the number of collisions detected between
the two submissions.

Line comparison makes one pass through each submission, and thus is $O(m + n)$.
It is thus far faster than Smith-Waterman.

As the name of the algorithm indicates, it is only intended to be used with
(and defaults to) the line tokenization scheme. Whitespace tokenization results
in a percent of shared words contained in submissions, which is almost always
very high and does not mean much about the actual similarity of two
submissions. Character tokenization tends to result in greater than 99\%
similarity for all submissions, given that most all will be using the same
basic alphabet (capital and lowercase letters, numbers, and 
language-appropriate syntax such as \{ or [).

Given the restriction to the use of line tokenization, even small changes (for
example, a single missing character) can result in otherwise extremely similar
lines not being recorded as similar. Line comparison is thus very limited in
terms of accuracy.

\section{Appendix D: Description of Output Strategies}
Once an algorithm has been applied to the submissions, the results must be
printed in a usable format so they may be used and interpreted. Output
strategies determine how this is done.

Results will often be presented as a 2x2 matrix, henceforth referred to as a
\textit{Similarity Matrix}. A similarity matrix is square, with each submission
compared appearing once on both the X axis and Y axis. Each cell represents the
similarity of the submission given on the X axis with the submission given on
the Y axis (please note that this is one-way similarity - comparing Y to X may
have different results, as the submissions could be of different size). Where
the submissions on the X axis and Y axis are identical (will happen once per
row and column, given that each submission occurs once per axis), the
similarity between the submissions is ignored (they would be 100\% identical,
given that they are the same submission!).

\subsection{CSV}
The CSV output strategy records output as a similarity matrix in
comma-separated value format. This output format is computer-readable, not
human-readable. It can be imported into Microsoft Excel or a number of other
software statistics packages to generate statistics about detected similarities

\subsection{HTML}
The HTML output strategy produces a web page which can be opened in a typical
web browser, presenting a colorized version of a similarity matrix. A color
range (yellow to red) shows how similar each cell is, allowing easy visual
identification of similar students and clusters of similarities.

\subsection{Threshold}
The threshold output strategy produces an ordered list of submission pairs
which are sufficiently similar (by default, 60\% or greater). The list
is ordered from most to least similar, and omits all information about
similarities lower than the threshold. This output strategy produces
quickly actionable information about the most-similar submissions.

\end{document}
